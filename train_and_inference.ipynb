{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d489b8d-a987-4e05-a660-cd29520c1378",
   "metadata": {},
   "source": [
    "# Import Library and Helper Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "62535c54-444d-4a37-8d6a-7a61258fa21c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gc\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "import wave\n",
    "import os\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "d98a81bf-3127-444c-93ee-c122a2831903",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.metrics import confusion_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "f7cdd362-d151-4c65-8acc-730f00295cdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported - ready to use PyTorch 2.2.0+cu121\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchsummary import summary\n",
    "from torchview import draw_graph\n",
    "from pathlib import Path\n",
    "import torchaudio\n",
    "print(\"Libraries imported - ready to use PyTorch\", torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "738fb7fa-b895-49a2-99b7-cbc94a7010c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import trange,tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "09444f04-a7c8-48dc-b383-9732f71cc682",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "8669c583-e31b-4855-b372-f880df773da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random \n",
    "SEED = 42\n",
    "def seed_everything(seed=42): \n",
    "    random.seed(seed) \n",
    "    os.environ['PYTHONHASHSEED'] = str(seed) \n",
    "    np.random.seed(seed) \n",
    "seed_everything(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b89e900d-e0b3-40ee-90ab-d586a4507823",
   "metadata": {},
   "source": [
    "# Define Workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "efe77a13-fe9f-49cb-86b0-16c878d66c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd = os.getcwd()\n",
    "workspace = os.path.join(cwd,'Project','gtzan_32k')\n",
    "dataset_dir = os.path.join(workspace,'dataset')    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ea7e4d-61e2-4a54-886b-471c1b13e743",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "67296f65-d9ff-4059-a633-3d72411611ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    sample_rate = 0\n",
    "    clip_samples = 0\n",
    "    center = False\n",
    "    labels = []\n",
    "    idx_to_lb = {}\n",
    "    lb_to_idx = {}\n",
    "    classes_num = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "b850c4f1-d7af-43b3-a13d-807c484c7ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "\n",
    "Config.sample_rate = 32000\n",
    "Config.clip_samples = sample_rate * 10 #(s)\n",
    "Config.center = False\n",
    "\n",
    "Config.labels = os.listdir(dataset_dir)    \n",
    "Config.lb_to_idx = {lb: idx for idx, lb in enumerate(labels)}\n",
    "Config.idx_to_lb = {idx: lb for idx, lb in enumerate(labels)}\n",
    "Config.classes_num = len(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a37cee5-7274-425a-8a0c-35787a227460",
   "metadata": {},
   "source": [
    "# Pytorch Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "ce1a3795-ed05-4f5b-9dcd-fb05cc1bbd6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "def move_data_to_device(x, device):\n",
    "    if 'float' in str(x.dtype):\n",
    "        x = torch.Tensor(x)\n",
    "    elif 'int' in str(x.dtype):\n",
    "        x = torch.LongTensor(x)\n",
    "    else:\n",
    "        return x\n",
    "\n",
    "    return x.to(device)\n",
    "\n",
    "\n",
    "def do_mixup(x, mixup_lambda):\n",
    "    \"\"\"Mixup x of even indexes (0, 2, 4, ...) with x of odd indexes \n",
    "    (1, 3, 5, ...).\n",
    "\n",
    "    Args:\n",
    "      x: (batch_size * 2, ...)\n",
    "      mixup_lambda: (batch_size * 2,)\n",
    "\n",
    "    Returns:\n",
    "      out: (batch_size, ...)\n",
    "    \"\"\"\n",
    "    out = (x[0 :: 2].transpose(0, -1) * mixup_lambda[0 :: 2] + \\\n",
    "        x[1 :: 2].transpose(0, -1) * mixup_lambda[1 :: 2]).transpose(0, -1)\n",
    "    return out\n",
    "    \n",
    "\n",
    "def append_to_dict(dict, key, value):\n",
    "    if key in dict.keys():\n",
    "        dict[key].append(value)\n",
    "    else:\n",
    "        dict[key] = [value]\n",
    "\n",
    "\n",
    "def forward(model, generator, return_input=False, \n",
    "    return_target=False):\n",
    "    \"\"\"Forward data to a model.\n",
    "    \n",
    "    Args: \n",
    "      model: object\n",
    "      generator: object\n",
    "      return_input: bool\n",
    "      return_target: bool\n",
    "\n",
    "    Returns:\n",
    "      audio_name: (audios_num,)\n",
    "      clipwise_output: (audios_num, classes_num)\n",
    "      (ifexist) segmentwise_output: (audios_num, segments_num, classes_num)\n",
    "      (ifexist) framewise_output: (audios_num, frames_num, classes_num)\n",
    "      (optional) return_input: (audios_num, segment_samples)\n",
    "      (optional) return_target: (audios_num, classes_num)\n",
    "    \"\"\"\n",
    "    output_dict = {}\n",
    "    device = next(model.parameters()).device\n",
    "    time1 = time.time()\n",
    "\n",
    "    # Forward data to a model in mini-batches\n",
    "    for n, batch_data_dict in enumerate(generator):\n",
    "        print(n)\n",
    "        batch_waveform = move_data_to_device(batch_data_dict['waveform'], device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            batch_output = model(batch_waveform)\n",
    "\n",
    "        append_to_dict(output_dict, 'audio_name', batch_data_dict['audio_name'])\n",
    "\n",
    "        append_to_dict(output_dict, 'clipwise_output', \n",
    "            batch_output['clipwise_output'].data.cpu().numpy())\n",
    "\n",
    "        if 'segmentwise_output' in batch_output.keys():\n",
    "            append_to_dict(output_dict, 'segmentwise_output', \n",
    "                batch_output['segmentwise_output'].data.cpu().numpy())\n",
    "\n",
    "        if 'framewise_output' in batch_output.keys():\n",
    "            append_to_dict(output_dict, 'framewise_output', \n",
    "                batch_output['framewise_output'].data.cpu().numpy())\n",
    "            \n",
    "        if return_input:\n",
    "            append_to_dict(output_dict, 'waveform', batch_data_dict['waveform'])\n",
    "            \n",
    "        if return_target:\n",
    "            if 'target' in batch_data_dict.keys():\n",
    "                append_to_dict(output_dict, 'target', batch_data_dict['target'])\n",
    "\n",
    "        if n % 10 == 0:\n",
    "            print(' --- Inference time: {:.3f} s / 10 iterations ---'.format(\n",
    "                time.time() - time1))\n",
    "            time1 = time.time()\n",
    "\n",
    "    for key in output_dict.keys():\n",
    "        output_dict[key] = np.concatenate(output_dict[key], axis=0)\n",
    "\n",
    "    return output_dict\n",
    "\n",
    "\n",
    "def interpolate(x, ratio):\n",
    "    \"\"\"Interpolate data in time domain. This is used to compensate the \n",
    "    resolution reduction in downsampling of a CNN.\n",
    "    \n",
    "    Args:\n",
    "      x: (batch_size, time_steps, classes_num)\n",
    "      ratio: int, ratio to interpolate\n",
    "\n",
    "    Returns:\n",
    "      upsampled: (batch_size, time_steps * ratio, classes_num)\n",
    "    \"\"\"\n",
    "    (batch_size, time_steps, classes_num) = x.shape\n",
    "    upsampled = x[:, :, None, :].repeat(1, 1, ratio, 1)\n",
    "    upsampled = upsampled.reshape(batch_size, time_steps * ratio, classes_num)\n",
    "    return upsampled\n",
    "\n",
    "\n",
    "def pad_framewise_output(framewise_output, frames_num):\n",
    "    \"\"\"Pad framewise_output to the same length as input frames. The pad value \n",
    "    is the same as the value of the last frame.\n",
    "\n",
    "    Args:\n",
    "      framewise_output: (batch_size, frames_num, classes_num)\n",
    "      frames_num: int, number of frames to pad\n",
    "\n",
    "    Outputs:\n",
    "      output: (batch_size, frames_num, classes_num)\n",
    "    \"\"\"\n",
    "    pad = framewise_output[:, -1 :, :].repeat(1, frames_num - framewise_output.shape[1], 1)\n",
    "    \"\"\"tensor for padding\"\"\"\n",
    "\n",
    "    output = torch.cat((framewise_output, pad), dim=1)\n",
    "    \"\"\"(batch_size, frames_num, classes_num)\"\"\"\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "def count_flops(model, audio_length):\n",
    "    \"\"\"Count flops. Code modified from others' implementation.\n",
    "    \"\"\"\n",
    "    multiply_adds = True\n",
    "    list_conv2d=[]\n",
    "    def conv2d_hook(self, input, output):\n",
    "        batch_size, input_channels, input_height, input_width = input[0].size()\n",
    "        output_channels, output_height, output_width = output[0].size()\n",
    " \n",
    "        kernel_ops = self.kernel_size[0] * self.kernel_size[1] * (self.in_channels / self.groups) * (2 if multiply_adds else 1)\n",
    "        bias_ops = 1 if self.bias is not None else 0\n",
    " \n",
    "        params = output_channels * (kernel_ops + bias_ops)\n",
    "        flops = batch_size * params * output_height * output_width\n",
    " \n",
    "        list_conv2d.append(flops)\n",
    "\n",
    "    list_conv1d=[]\n",
    "    def conv1d_hook(self, input, output):\n",
    "        batch_size, input_channels, input_length = input[0].size()\n",
    "        output_channels, output_length = output[0].size()\n",
    " \n",
    "        kernel_ops = self.kernel_size[0] * (self.in_channels / self.groups) * (2 if multiply_adds else 1)\n",
    "        bias_ops = 1 if self.bias is not None else 0\n",
    " \n",
    "        params = output_channels * (kernel_ops + bias_ops)\n",
    "        flops = batch_size * params * output_length\n",
    " \n",
    "        list_conv1d.append(flops)\n",
    " \n",
    "    list_linear=[] \n",
    "    def linear_hook(self, input, output):\n",
    "        batch_size = input[0].size(0) if input[0].dim() == 2 else 1\n",
    " \n",
    "        weight_ops = self.weight.nelement() * (2 if multiply_adds else 1)\n",
    "        bias_ops = self.bias.nelement()\n",
    " \n",
    "        flops = batch_size * (weight_ops + bias_ops)\n",
    "        list_linear.append(flops)\n",
    " \n",
    "    list_bn=[] \n",
    "    def bn_hook(self, input, output):\n",
    "        list_bn.append(input[0].nelement() * 2)\n",
    " \n",
    "    list_relu=[] \n",
    "    def relu_hook(self, input, output):\n",
    "        list_relu.append(input[0].nelement() * 2)\n",
    " \n",
    "    list_pooling2d=[]\n",
    "    def pooling2d_hook(self, input, output):\n",
    "        batch_size, input_channels, input_height, input_width = input[0].size()\n",
    "        output_channels, output_height, output_width = output[0].size()\n",
    " \n",
    "        kernel_ops = self.kernel_size * self.kernel_size\n",
    "        bias_ops = 0\n",
    "        params = output_channels * (kernel_ops + bias_ops)\n",
    "        flops = batch_size * params * output_height * output_width\n",
    " \n",
    "        list_pooling2d.append(flops)\n",
    "\n",
    "    list_pooling1d=[]\n",
    "    def pooling1d_hook(self, input, output):\n",
    "        batch_size, input_channels, input_length = input[0].size()\n",
    "        output_channels, output_length = output[0].size()\n",
    " \n",
    "        kernel_ops = self.kernel_size[0]\n",
    "        bias_ops = 0\n",
    "        \n",
    "        params = output_channels * (kernel_ops + bias_ops)\n",
    "        flops = batch_size * params * output_length\n",
    " \n",
    "        list_pooling2d.append(flops)\n",
    " \n",
    "    def foo(net):\n",
    "        childrens = list(net.children())\n",
    "        if not childrens:\n",
    "            if isinstance(net, nn.Conv2d):\n",
    "                net.register_forward_hook(conv2d_hook)\n",
    "            elif isinstance(net, nn.Conv1d):\n",
    "                net.register_forward_hook(conv1d_hook)\n",
    "            elif isinstance(net, nn.Linear):\n",
    "                net.register_forward_hook(linear_hook)\n",
    "            elif isinstance(net, nn.BatchNorm2d) or isinstance(net, nn.BatchNorm1d):\n",
    "                net.register_forward_hook(bn_hook)\n",
    "            elif isinstance(net, nn.ReLU):\n",
    "                net.register_forward_hook(relu_hook)\n",
    "            elif isinstance(net, nn.AvgPool2d) or isinstance(net, nn.MaxPool2d):\n",
    "                net.register_forward_hook(pooling2d_hook)\n",
    "            elif isinstance(net, nn.AvgPool1d) or isinstance(net, nn.MaxPool1d):\n",
    "                net.register_forward_hook(pooling1d_hook)\n",
    "            else:\n",
    "                print('Warning: flop of module {} is not counted!'.format(net))\n",
    "            return\n",
    "        for c in childrens:\n",
    "            foo(c)\n",
    "\n",
    "    # Register hook\n",
    "    foo(model)\n",
    "    \n",
    "    device = device = next(model.parameters()).device\n",
    "    input = torch.rand(1, audio_length).to(device)\n",
    "\n",
    "    out = model(input)\n",
    " \n",
    "    total_flops = sum(list_conv2d) + sum(list_conv1d) + sum(list_linear) + \\\n",
    "        sum(list_bn) + sum(list_relu) + sum(list_pooling2d) + sum(list_pooling1d)\n",
    "    \n",
    "    return total_flops"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69434973-f0b4-4b7e-b291-a9a9b47e838a",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "285926ec-4a55-4f9a-b8d8-7b3556a6f82e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchlibrosa.stft import Spectrogram, LogmelFilterBank\n",
    "from torchlibrosa.augmentation import SpecAugmentation\n",
    "\n",
    "# from pytorch_utils import do_mixup, interpolate, pad_framewise_output\n",
    " \n",
    "\n",
    "def init_layer(layer):\n",
    "    \"\"\"Initialize a Linear or Convolutional layer. \"\"\"\n",
    "    nn.init.xavier_uniform_(layer.weight)\n",
    " \n",
    "    if hasattr(layer, 'bias'):\n",
    "        if layer.bias is not None:\n",
    "            layer.bias.data.fill_(0.)\n",
    "            \n",
    "    \n",
    "def init_bn(bn):\n",
    "    \"\"\"Initialize a Batchnorm layer. \"\"\"\n",
    "    bn.bias.data.fill_(0.)\n",
    "    bn.weight.data.fill_(1.)\n",
    "\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        \n",
    "        super(ConvBlock, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_channels=in_channels, \n",
    "                              out_channels=out_channels,\n",
    "                              kernel_size=(3, 3), stride=(1, 1),\n",
    "                              padding=(1, 1), bias=False)\n",
    "                              \n",
    "        self.conv2 = nn.Conv2d(in_channels=out_channels, \n",
    "                              out_channels=out_channels,\n",
    "                              kernel_size=(3, 3), stride=(1, 1),\n",
    "                              padding=(1, 1), bias=False)\n",
    "                              \n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "        self.init_weight()\n",
    "        \n",
    "    def init_weight(self):\n",
    "        init_layer(self.conv1)\n",
    "        init_layer(self.conv2)\n",
    "        init_bn(self.bn1)\n",
    "        init_bn(self.bn2)\n",
    "\n",
    "        \n",
    "    def forward(self, input, pool_size=(2, 2), pool_type='avg'):\n",
    "        \n",
    "        x = input\n",
    "        x = F.relu_(self.bn1(self.conv1(x)))\n",
    "        x = F.relu_(self.bn2(self.conv2(x)))\n",
    "        if pool_type == 'max':\n",
    "            x = F.max_pool2d(x, kernel_size=pool_size)\n",
    "        elif pool_type == 'avg':\n",
    "            x = F.avg_pool2d(x, kernel_size=pool_size)\n",
    "        elif pool_type == 'avg+max':\n",
    "            x1 = F.avg_pool2d(x, kernel_size=pool_size)\n",
    "            x2 = F.max_pool2d(x, kernel_size=pool_size)\n",
    "            x = x1 + x2\n",
    "        else:\n",
    "            raise Exception('Incorrect argument!')\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "class ConvBlock5x5(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        \n",
    "        super(ConvBlock5x5, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_channels=in_channels, \n",
    "                              out_channels=out_channels,\n",
    "                              kernel_size=(5, 5), stride=(1, 1),\n",
    "                              padding=(2, 2), bias=False)\n",
    "                              \n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "        self.init_weight()\n",
    "        \n",
    "    def init_weight(self):\n",
    "        init_layer(self.conv1)\n",
    "        init_bn(self.bn1)\n",
    "\n",
    "        \n",
    "    def forward(self, input, pool_size=(2, 2), pool_type='avg'):\n",
    "        \n",
    "        x = input\n",
    "        x = F.relu_(self.bn1(self.conv1(x)))\n",
    "        if pool_type == 'max':\n",
    "            x = F.max_pool2d(x, kernel_size=pool_size)\n",
    "        elif pool_type == 'avg':\n",
    "            x = F.avg_pool2d(x, kernel_size=pool_size)\n",
    "        elif pool_type == 'avg+max':\n",
    "            x1 = F.avg_pool2d(x, kernel_size=pool_size)\n",
    "            x2 = F.max_pool2d(x, kernel_size=pool_size)\n",
    "            x = x1 + x2\n",
    "        else:\n",
    "            raise Exception('Incorrect argument!')\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "class AttBlock(nn.Module):\n",
    "    def __init__(self, n_in, n_out, activation='linear', temperature=1.):\n",
    "        super(AttBlock, self).__init__()\n",
    "        \n",
    "        self.activation = activation\n",
    "        self.temperature = temperature\n",
    "        self.att = nn.Conv1d(in_channels=n_in, out_channels=n_out, kernel_size=1, stride=1, padding=0, bias=True)\n",
    "        self.cla = nn.Conv1d(in_channels=n_in, out_channels=n_out, kernel_size=1, stride=1, padding=0, bias=True)\n",
    "        \n",
    "        self.bn_att = nn.BatchNorm1d(n_out)\n",
    "        self.init_weights()\n",
    "        \n",
    "    def init_weights(self):\n",
    "        init_layer(self.att)\n",
    "        init_layer(self.cla)\n",
    "        init_bn(self.bn_att)\n",
    "         \n",
    "    def forward(self, x):\n",
    "        # x: (n_samples, n_in, n_time)\n",
    "        norm_att = torch.softmax(torch.clamp(self.att(x), -10, 10), dim=-1)\n",
    "        cla = self.nonlinear_transform(self.cla(x))\n",
    "        x = torch.sum(norm_att * cla, dim=2)\n",
    "        return x, norm_att, cla\n",
    "\n",
    "    def nonlinear_transform(self, x):\n",
    "        if self.activation == 'linear':\n",
    "            return x\n",
    "        elif self.activation == 'sigmoid':\n",
    "            return torch.sigmoid(x)\n",
    "\n",
    "\n",
    "class Cnn14(nn.Module):\n",
    "    def __init__(self, sample_rate, window_size, hop_size, mel_bins, fmin, \n",
    "        fmax, classes_num):\n",
    "        \n",
    "        super(Cnn14, self).__init__()\n",
    "\n",
    "        window = 'hann'\n",
    "        center = True\n",
    "        pad_mode = 'reflect'\n",
    "        ref = 1.0\n",
    "        amin = 1e-10\n",
    "        top_db = None\n",
    "\n",
    "        # Spectrogram extractor\n",
    "        self.spectrogram_extractor = Spectrogram(n_fft=window_size, hop_length=hop_size, \n",
    "            win_length=window_size, window=window, center=center, pad_mode=pad_mode, \n",
    "            freeze_parameters=True)\n",
    "\n",
    "        # Logmel feature extractor\n",
    "        self.logmel_extractor = LogmelFilterBank(sr=sample_rate, n_fft=window_size, \n",
    "            n_mels=mel_bins, fmin=fmin, fmax=fmax, ref=ref, amin=amin, top_db=top_db, \n",
    "            freeze_parameters=True)\n",
    "\n",
    "        # Spec augmenter\n",
    "        self.spec_augmenter = SpecAugmentation(time_drop_width=64, time_stripes_num=2, \n",
    "            freq_drop_width=8, freq_stripes_num=2)\n",
    "\n",
    "        self.bn0 = nn.BatchNorm2d(64)\n",
    "\n",
    "        self.conv_block1 = ConvBlock(in_channels=1, out_channels=64)\n",
    "        self.conv_block2 = ConvBlock(in_channels=64, out_channels=128)\n",
    "        self.conv_block3 = ConvBlock(in_channels=128, out_channels=256)\n",
    "        self.conv_block4 = ConvBlock(in_channels=256, out_channels=512)\n",
    "        self.conv_block5 = ConvBlock(in_channels=512, out_channels=1024)\n",
    "        self.conv_block6 = ConvBlock(in_channels=1024, out_channels=2048)\n",
    "\n",
    "        self.fc1 = nn.Linear(2048, 2048, bias=True)\n",
    "        self.fc_audioset = nn.Linear(2048, classes_num, bias=True)\n",
    "        \n",
    "        self.init_weight()\n",
    "\n",
    "    def init_weight(self):\n",
    "        init_bn(self.bn0)\n",
    "        init_layer(self.fc1)\n",
    "        init_layer(self.fc_audioset)\n",
    " \n",
    "    def forward(self, input, mixup_lambda=None):\n",
    "        \"\"\"\n",
    "        Input: (batch_size, data_length)\"\"\"\n",
    "\n",
    "        x = self.spectrogram_extractor(input)   # (batch_size, 1, time_steps, freq_bins)\n",
    "        x = self.logmel_extractor(x)    # (batch_size, 1, time_steps, mel_bins)\n",
    "\n",
    "        x = x.transpose(1, 3)\n",
    "        x = self.bn0(x)\n",
    "        x = x.transpose(1, 3)\n",
    "        \n",
    "        if self.training:\n",
    "            x = self.spec_augmenter(x)\n",
    "\n",
    "        # Mixup on spectrogram\n",
    "        if self.training and mixup_lambda is not None:\n",
    "            x = do_mixup(x, mixup_lambda)\n",
    "\n",
    "        x = self.conv_block1(x, pool_size=(2, 2), pool_type='avg')\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = self.conv_block2(x, pool_size=(2, 2), pool_type='avg')\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = self.conv_block3(x, pool_size=(2, 2), pool_type='avg')\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = self.conv_block4(x, pool_size=(2, 2), pool_type='avg')\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = self.conv_block5(x, pool_size=(2, 2), pool_type='avg')\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = self.conv_block6(x, pool_size=(1, 1), pool_type='avg')\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = torch.mean(x, dim=3)\n",
    "        \n",
    "        (x1, _) = torch.max(x, dim=2)\n",
    "        x2 = torch.mean(x, dim=2)\n",
    "        x = x1 + x2\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = F.relu_(self.fc1(x))\n",
    "        embedding = F.dropout(x, p=0.5, training=self.training)\n",
    "        clipwise_output = torch.sigmoid(self.fc_audioset(x))\n",
    "        \n",
    "        output_dict = {'clipwise_output': clipwise_output, 'embedding': embedding}\n",
    "\n",
    "        return output_dict\n",
    "\n",
    "\n",
    "class Cnn14_no_specaug(nn.Module):\n",
    "    def __init__(self, sample_rate, window_size, hop_size, mel_bins, fmin, \n",
    "        fmax, classes_num):\n",
    "        \n",
    "        super(Cnn14_no_specaug, self).__init__()\n",
    "\n",
    "        window = 'hann'\n",
    "        center = True\n",
    "        pad_mode = 'reflect'\n",
    "        ref = 1.0\n",
    "        amin = 1e-10\n",
    "        top_db = None\n",
    "\n",
    "        # Spectrogram extractor\n",
    "        self.spectrogram_extractor = Spectrogram(n_fft=window_size, hop_length=hop_size, \n",
    "            win_length=window_size, window=window, center=center, pad_mode=pad_mode, \n",
    "            freeze_parameters=True)\n",
    "\n",
    "        # Logmel feature extractor\n",
    "        self.logmel_extractor = LogmelFilterBank(sr=sample_rate, n_fft=window_size, \n",
    "            n_mels=mel_bins, fmin=fmin, fmax=fmax, ref=ref, amin=amin, top_db=top_db, \n",
    "            freeze_parameters=True)\n",
    "\n",
    "        self.bn0 = nn.BatchNorm2d(64)\n",
    "\n",
    "        self.conv_block1 = ConvBlock(in_channels=1, out_channels=64)\n",
    "        self.conv_block2 = ConvBlock(in_channels=64, out_channels=128)\n",
    "        self.conv_block3 = ConvBlock(in_channels=128, out_channels=256)\n",
    "        self.conv_block4 = ConvBlock(in_channels=256, out_channels=512)\n",
    "        self.conv_block5 = ConvBlock(in_channels=512, out_channels=1024)\n",
    "        self.conv_block6 = ConvBlock(in_channels=1024, out_channels=2048)\n",
    "\n",
    "        self.fc1 = nn.Linear(2048, 2048, bias=True)\n",
    "        self.fc_audioset = nn.Linear(2048, classes_num, bias=True)\n",
    "        \n",
    "        self.init_weight()\n",
    "\n",
    "    def init_weight(self):\n",
    "        init_bn(self.bn0)\n",
    "        init_layer(self.fc1)\n",
    "        init_layer(self.fc_audioset)\n",
    " \n",
    "    def forward(self, input, mixup_lambda=None):\n",
    "        \"\"\"\n",
    "        Input: (batch_size, data_length)\"\"\"\n",
    "\n",
    "        x = self.spectrogram_extractor(input)   # (batch_size, 1, time_steps, freq_bins)\n",
    "        x = self.logmel_extractor(x)    # (batch_size, 1, time_steps, mel_bins)\n",
    "        \n",
    "        x = x.transpose(1, 3)\n",
    "        x = self.bn0(x)\n",
    "        x = x.transpose(1, 3)\n",
    "        \n",
    "        # Mixup on spectrogram\n",
    "        if self.training and mixup_lambda is not None:\n",
    "            x = do_mixup(x, mixup_lambda)\n",
    "        \n",
    "        x = self.conv_block1(x, pool_size=(2, 2), pool_type='avg')\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = self.conv_block2(x, pool_size=(2, 2), pool_type='avg')\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = self.conv_block3(x, pool_size=(2, 2), pool_type='avg')\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = self.conv_block4(x, pool_size=(2, 2), pool_type='avg')\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = self.conv_block5(x, pool_size=(2, 2), pool_type='avg')\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = self.conv_block6(x, pool_size=(1, 1), pool_type='avg')\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = torch.mean(x, dim=3)\n",
    "        \n",
    "        (x1, _) = torch.max(x, dim=2)\n",
    "        x2 = torch.mean(x, dim=2)\n",
    "        x = x1 + x2\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = F.relu_(self.fc1(x))\n",
    "        embedding = F.dropout(x, p=0.5, training=self.training)\n",
    "        clipwise_output = torch.sigmoid(self.fc_audioset(x))\n",
    "        \n",
    "        output_dict = {'clipwise_output': clipwise_output, 'embedding': embedding}\n",
    "\n",
    "        return output_dict\n",
    "\n",
    "\n",
    "class Cnn14_no_dropout(nn.Module):\n",
    "    def __init__(self, sample_rate, window_size, hop_size, mel_bins, fmin, \n",
    "        fmax, classes_num):\n",
    "        \n",
    "        super(Cnn14_no_dropout, self).__init__()\n",
    "\n",
    "        window = 'hann'\n",
    "        center = True\n",
    "        pad_mode = 'reflect'\n",
    "        ref = 1.0\n",
    "        amin = 1e-10\n",
    "        top_db = None\n",
    "\n",
    "        # Spectrogram extractor\n",
    "        self.spectrogram_extractor = Spectrogram(n_fft=window_size, hop_length=hop_size, \n",
    "            win_length=window_size, window=window, center=center, pad_mode=pad_mode, \n",
    "            freeze_parameters=True)\n",
    "\n",
    "        # Logmel feature extractor\n",
    "        self.logmel_extractor = LogmelFilterBank(sr=sample_rate, n_fft=window_size, \n",
    "            n_mels=mel_bins, fmin=fmin, fmax=fmax, ref=ref, amin=amin, top_db=top_db, \n",
    "            freeze_parameters=True)\n",
    "\n",
    "        # Spec augmenter\n",
    "        self.spec_augmenter = SpecAugmentation(time_drop_width=64, time_stripes_num=2, \n",
    "            freq_drop_width=8, freq_stripes_num=2)\n",
    "\n",
    "        self.bn0 = nn.BatchNorm2d(64)\n",
    "\n",
    "        self.conv_block1 = ConvBlock(in_channels=1, out_channels=64)\n",
    "        self.conv_block2 = ConvBlock(in_channels=64, out_channels=128)\n",
    "        self.conv_block3 = ConvBlock(in_channels=128, out_channels=256)\n",
    "        self.conv_block4 = ConvBlock(in_channels=256, out_channels=512)\n",
    "        self.conv_block5 = ConvBlock(in_channels=512, out_channels=1024)\n",
    "        self.conv_block6 = ConvBlock(in_channels=1024, out_channels=2048)\n",
    "\n",
    "        self.fc1 = nn.Linear(2048, 2048, bias=True)\n",
    "        self.fc_audioset = nn.Linear(2048, classes_num, bias=True)\n",
    "        \n",
    "        self.init_weight()\n",
    "\n",
    "    def init_weight(self):\n",
    "        init_bn(self.bn0)\n",
    "        init_layer(self.fc1)\n",
    "        init_layer(self.fc_audioset)\n",
    " \n",
    "    def forward(self, input, mixup_lambda=None):\n",
    "        \"\"\"\n",
    "        Input: (batch_size, data_length)\"\"\"\n",
    "\n",
    "        x = self.spectrogram_extractor(input)   # (batch_size, 1, time_steps, freq_bins)\n",
    "        x = self.logmel_extractor(x)    # (batch_size, 1, time_steps, mel_bins)\n",
    "        \n",
    "        x = x.transpose(1, 3)\n",
    "        x = self.bn0(x)\n",
    "        x = x.transpose(1, 3)\n",
    "        \n",
    "        if self.training:\n",
    "            x = self.spec_augmenter(x)\n",
    "\n",
    "        # Mixup on spectrogram\n",
    "        if self.training and mixup_lambda is not None:\n",
    "            x = do_mixup(x, mixup_lambda)\n",
    "        \n",
    "        x = self.conv_block1(x, pool_size=(2, 2), pool_type='avg')\n",
    "        x = self.conv_block2(x, pool_size=(2, 2), pool_type='avg')\n",
    "        x = self.conv_block3(x, pool_size=(2, 2), pool_type='avg')\n",
    "        x = self.conv_block4(x, pool_size=(2, 2), pool_type='avg')\n",
    "        x = self.conv_block5(x, pool_size=(2, 2), pool_type='avg')\n",
    "        x = self.conv_block6(x, pool_size=(1, 1), pool_type='avg')\n",
    "        x = torch.mean(x, dim=3)\n",
    "        \n",
    "        (x1, _) = torch.max(x, dim=2)\n",
    "        x2 = torch.mean(x, dim=2)\n",
    "        x = x1 + x2\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = F.relu_(self.fc1(x))\n",
    "        embedding = F.dropout(x, p=0.5, training=self.training)\n",
    "        clipwise_output = torch.sigmoid(self.fc_audioset(x))\n",
    "        \n",
    "        output_dict = {'clipwise_output': clipwise_output, 'embedding': embedding}\n",
    "\n",
    "        return output_dict\n",
    "\n",
    "\n",
    "class Cnn6(nn.Module):\n",
    "    def __init__(self, sample_rate, window_size, hop_size, mel_bins, fmin, \n",
    "        fmax, classes_num):\n",
    "        \n",
    "        super(Cnn6, self).__init__()\n",
    "\n",
    "        window = 'hann'\n",
    "        center = True\n",
    "        pad_mode = 'reflect'\n",
    "        ref = 1.0\n",
    "        amin = 1e-10\n",
    "        top_db = None\n",
    "\n",
    "        # Spectrogram extractor\n",
    "        self.spectrogram_extractor = Spectrogram(n_fft=window_size, hop_length=hop_size, \n",
    "            win_length=window_size, window=window, center=center, pad_mode=pad_mode, \n",
    "            freeze_parameters=True)\n",
    "\n",
    "        # Logmel feature extractor\n",
    "        self.logmel_extractor = LogmelFilterBank(sr=sample_rate, n_fft=window_size, \n",
    "            n_mels=mel_bins, fmin=fmin, fmax=fmax, ref=ref, amin=amin, top_db=top_db, \n",
    "            freeze_parameters=True)\n",
    "\n",
    "        # Spec augmenter\n",
    "        self.spec_augmenter = SpecAugmentation(time_drop_width=64, time_stripes_num=2, \n",
    "            freq_drop_width=8, freq_stripes_num=2)\n",
    "\n",
    "        self.bn0 = nn.BatchNorm2d(64)\n",
    "\n",
    "        self.conv_block1 = ConvBlock5x5(in_channels=1, out_channels=64)\n",
    "        self.conv_block2 = ConvBlock5x5(in_channels=64, out_channels=128)\n",
    "        self.conv_block3 = ConvBlock5x5(in_channels=128, out_channels=256)\n",
    "        self.conv_block4 = ConvBlock5x5(in_channels=256, out_channels=512)\n",
    "\n",
    "        self.fc1 = nn.Linear(512, 512, bias=True)\n",
    "        self.fc_audioset = nn.Linear(512, classes_num, bias=True)\n",
    "        \n",
    "        self.init_weight()\n",
    "\n",
    "    def init_weight(self):\n",
    "        init_bn(self.bn0)\n",
    "        init_layer(self.fc1)\n",
    "        init_layer(self.fc_audioset)\n",
    " \n",
    "    def forward(self, input, mixup_lambda=None):\n",
    "        \"\"\"\n",
    "        Input: (batch_size, data_length)\"\"\"\n",
    "\n",
    "        x = self.spectrogram_extractor(input)   # (batch_size, 1, time_steps, freq_bins)\n",
    "        x = self.logmel_extractor(x)    # (batch_size, 1, time_steps, mel_bins)\n",
    "        \n",
    "        x = x.transpose(1, 3)\n",
    "        x = self.bn0(x)\n",
    "        x = x.transpose(1, 3)\n",
    "        \n",
    "        if self.training:\n",
    "            x = self.spec_augmenter(x)\n",
    "\n",
    "        # Mixup on spectrogram\n",
    "        if self.training and mixup_lambda is not None:\n",
    "            x = do_mixup(x, mixup_lambda)\n",
    "        \n",
    "        x = self.conv_block1(x, pool_size=(2, 2), pool_type='avg')\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = self.conv_block2(x, pool_size=(2, 2), pool_type='avg')\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = self.conv_block3(x, pool_size=(2, 2), pool_type='avg')\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = self.conv_block4(x, pool_size=(2, 2), pool_type='avg')\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = torch.mean(x, dim=3)\n",
    "        \n",
    "        (x1, _) = torch.max(x, dim=2)\n",
    "        x2 = torch.mean(x, dim=2)\n",
    "        x = x1 + x2\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = F.relu_(self.fc1(x))\n",
    "        embedding = F.dropout(x, p=0.5, training=self.training)\n",
    "        clipwise_output = torch.sigmoid(self.fc_audioset(x))\n",
    "        \n",
    "        output_dict = {'clipwise_output': clipwise_output, 'embedding': embedding}\n",
    "\n",
    "        return output_dict\n",
    "\n",
    "\n",
    "class Cnn10(nn.Module):\n",
    "    def __init__(self, sample_rate, window_size, hop_size, mel_bins, fmin, \n",
    "        fmax, classes_num):\n",
    "        \n",
    "        super(Cnn10, self).__init__()\n",
    "\n",
    "        window = 'hann'\n",
    "        center = True\n",
    "        pad_mode = 'reflect'\n",
    "        ref = 1.0\n",
    "        amin = 1e-10\n",
    "        top_db = None\n",
    "\n",
    "        # Spectrogram extractor\n",
    "        self.spectrogram_extractor = Spectrogram(n_fft=window_size, hop_length=hop_size, \n",
    "            win_length=window_size, window=window, center=center, pad_mode=pad_mode, \n",
    "            freeze_parameters=True)\n",
    "\n",
    "        # Logmel feature extractor\n",
    "        self.logmel_extractor = LogmelFilterBank(sr=sample_rate, n_fft=window_size, \n",
    "            n_mels=mel_bins, fmin=fmin, fmax=fmax, ref=ref, amin=amin, top_db=top_db, \n",
    "            freeze_parameters=True)\n",
    "\n",
    "        # Spec augmenter\n",
    "        self.spec_augmenter = SpecAugmentation(time_drop_width=64, time_stripes_num=2, \n",
    "            freq_drop_width=8, freq_stripes_num=2)\n",
    "\n",
    "        self.bn0 = nn.BatchNorm2d(64)\n",
    "\n",
    "        self.conv_block1 = ConvBlock(in_channels=1, out_channels=64)\n",
    "        self.conv_block2 = ConvBlock(in_channels=64, out_channels=128)\n",
    "        self.conv_block3 = ConvBlock(in_channels=128, out_channels=256)\n",
    "        self.conv_block4 = ConvBlock(in_channels=256, out_channels=512)\n",
    "\n",
    "        self.fc1 = nn.Linear(512, 512, bias=True)\n",
    "        self.fc_audioset = nn.Linear(512, classes_num, bias=True)\n",
    "        \n",
    "        self.init_weight()\n",
    "\n",
    "    def init_weight(self):\n",
    "        init_bn(self.bn0)\n",
    "        init_layer(self.fc1)\n",
    "        init_layer(self.fc_audioset)\n",
    " \n",
    "    def forward(self, input, mixup_lambda=None):\n",
    "        \"\"\"\n",
    "        Input: (batch_size, data_length)\"\"\"\n",
    "\n",
    "        x = self.spectrogram_extractor(input)   # (batch_size, 1, time_steps, freq_bins)\n",
    "        x = self.logmel_extractor(x)    # (batch_size, 1, time_steps, mel_bins)\n",
    "        \n",
    "        x = x.transpose(1, 3)\n",
    "        x = self.bn0(x)\n",
    "        x = x.transpose(1, 3)\n",
    "        \n",
    "        if self.training:\n",
    "            x = self.spec_augmenter(x)\n",
    "\n",
    "        # Mixup on spectrogram\n",
    "        if self.training and mixup_lambda is not None:\n",
    "            x = do_mixup(x, mixup_lambda)\n",
    "        \n",
    "        x = self.conv_block1(x, pool_size=(2, 2), pool_type='avg')\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = self.conv_block2(x, pool_size=(2, 2), pool_type='avg')\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = self.conv_block3(x, pool_size=(2, 2), pool_type='avg')\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = self.conv_block4(x, pool_size=(2, 2), pool_type='avg')\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = torch.mean(x, dim=3)\n",
    "        \n",
    "        (x1, _) = torch.max(x, dim=2)\n",
    "        x2 = torch.mean(x, dim=2)\n",
    "        x = x1 + x2\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = F.relu_(self.fc1(x))\n",
    "        embedding = F.dropout(x, p=0.5, training=self.training)\n",
    "        clipwise_output = torch.sigmoid(self.fc_audioset(x))\n",
    "        \n",
    "        output_dict = {'clipwise_output': clipwise_output, 'embedding': embedding}\n",
    "\n",
    "        return output_dict\n",
    "\n",
    "\n",
    "def _resnet_conv3x3(in_planes, out_planes):\n",
    "    #3x3 convolution with padding\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=1,\n",
    "                     padding=1, groups=1, bias=False, dilation=1)\n",
    "\n",
    "\n",
    "def _resnet_conv1x1(in_planes, out_planes):\n",
    "    #1x1 convolution\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=1, bias=False)\n",
    "\n",
    "\n",
    "class _ResnetBasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,\n",
    "                 base_width=64, dilation=1, norm_layer=None):\n",
    "        super(_ResnetBasicBlock, self).__init__()\n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.BatchNorm2d\n",
    "        if groups != 1 or base_width != 64:\n",
    "            raise ValueError('_ResnetBasicBlock only supports groups=1 and base_width=64')\n",
    "        if dilation > 1:\n",
    "            raise NotImplementedError(\"Dilation > 1 not supported in _ResnetBasicBlock\")\n",
    "        # Both self.conv1 and self.downsample layers downsample the input when stride != 1\n",
    "\n",
    "        self.stride = stride\n",
    "\n",
    "        self.conv1 = _resnet_conv3x3(inplanes, planes)\n",
    "        self.bn1 = norm_layer(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = _resnet_conv3x3(planes, planes)\n",
    "        self.bn2 = norm_layer(planes)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        init_layer(self.conv1)\n",
    "        init_bn(self.bn1)\n",
    "        init_layer(self.conv2)\n",
    "        init_bn(self.bn2)\n",
    "        nn.init.constant_(self.bn2.weight, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "\n",
    "        if self.stride == 2:\n",
    "            out = F.avg_pool2d(x, kernel_size=(2, 2))\n",
    "        else:\n",
    "            out = x\n",
    "\n",
    "        out = self.conv1(out)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = F.dropout(out, p=0.1, training=self.training)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        \n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(identity)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class _ResnetBottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,\n",
    "                 base_width=64, dilation=1, norm_layer=None):\n",
    "        super(_ResnetBottleneck, self).__init__()\n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.BatchNorm2d\n",
    "        width = int(planes * (base_width / 64.)) * groups\n",
    "        self.stride = stride\n",
    "        # Both self.conv2 and self.downsample layers downsample the input when stride != 1\n",
    "        self.conv1 = _resnet_conv1x1(inplanes, width)\n",
    "        self.bn1 = norm_layer(width)\n",
    "        self.conv2 = _resnet_conv3x3(width, width)\n",
    "        self.bn2 = norm_layer(width)\n",
    "        self.conv3 = _resnet_conv1x1(width, planes * self.expansion)\n",
    "        self.bn3 = norm_layer(planes * self.expansion)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        init_layer(self.conv1)\n",
    "        init_bn(self.bn1)\n",
    "        init_layer(self.conv2)\n",
    "        init_bn(self.bn2)\n",
    "        init_layer(self.conv3)\n",
    "        init_bn(self.bn3)\n",
    "        nn.init.constant_(self.bn3.weight, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "\n",
    "        if self.stride == 2:\n",
    "            x = F.avg_pool2d(x, kernel_size=(2, 2))\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "        out = F.dropout(out, p=0.1, training=self.training)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(identity)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class _ResNet(nn.Module):\n",
    "    def __init__(self, block, layers, zero_init_residual=False,\n",
    "                 groups=1, width_per_group=64, replace_stride_with_dilation=None,\n",
    "                 norm_layer=None):\n",
    "        super(_ResNet, self).__init__()\n",
    "\n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.BatchNorm2d\n",
    "        self._norm_layer = norm_layer\n",
    "\n",
    "        self.inplanes = 64\n",
    "        self.dilation = 1\n",
    "        if replace_stride_with_dilation is None:\n",
    "            # each element in the tuple indicates if we should replace\n",
    "            # the 2x2 stride with a dilated convolution instead\n",
    "            replace_stride_with_dilation = [False, False, False]\n",
    "        if len(replace_stride_with_dilation) != 3:\n",
    "            raise ValueError(\"replace_stride_with_dilation should be None \"\n",
    "                             \"or a 3-element tuple, got {}\".format(replace_stride_with_dilation))\n",
    "        self.groups = groups\n",
    "        self.base_width = width_per_group\n",
    "\n",
    "        self.layer1 = self._make_layer(block, 64, layers[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2,\n",
    "                                       dilate=replace_stride_with_dilation[0])\n",
    "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2,\n",
    "                                       dilate=replace_stride_with_dilation[1])\n",
    "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2,\n",
    "                                       dilate=replace_stride_with_dilation[2])\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, stride=1, dilate=False):\n",
    "        norm_layer = self._norm_layer\n",
    "        downsample = None\n",
    "        previous_dilation = self.dilation\n",
    "        if dilate:\n",
    "            self.dilation *= stride\n",
    "            stride = 1\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            if stride == 1:\n",
    "                downsample = nn.Sequential(\n",
    "                    _resnet_conv1x1(self.inplanes, planes * block.expansion),\n",
    "                    norm_layer(planes * block.expansion),\n",
    "                )\n",
    "                init_layer(downsample[0])\n",
    "                init_bn(downsample[1])\n",
    "            elif stride == 2:\n",
    "                downsample = nn.Sequential(\n",
    "                    nn.AvgPool2d(kernel_size=2), \n",
    "                    _resnet_conv1x1(self.inplanes, planes * block.expansion),\n",
    "                    norm_layer(planes * block.expansion),\n",
    "                )\n",
    "                init_layer(downsample[1])\n",
    "                init_bn(downsample[2])\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, stride, downsample, self.groups,\n",
    "                            self.base_width, previous_dilation, norm_layer))\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes, groups=self.groups,\n",
    "                                base_width=self.base_width, dilation=self.dilation,\n",
    "                                norm_layer=norm_layer))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class ResNet22(nn.Module):\n",
    "    def __init__(self, sample_rate, window_size, hop_size, mel_bins, fmin, \n",
    "        fmax, classes_num):\n",
    "        \n",
    "        super(ResNet22, self).__init__()\n",
    "\n",
    "        window = 'hann'\n",
    "        center = True\n",
    "        pad_mode = 'reflect'\n",
    "        ref = 1.0\n",
    "        amin = 1e-10\n",
    "        top_db = None\n",
    "\n",
    "        # Spectrogram extractor\n",
    "        self.spectrogram_extractor = Spectrogram(n_fft=window_size, hop_length=hop_size, \n",
    "            win_length=window_size, window=window, center=center, pad_mode=pad_mode, \n",
    "            freeze_parameters=True)\n",
    "\n",
    "        # Logmel feature extractor\n",
    "        self.logmel_extractor = LogmelFilterBank(sr=sample_rate, n_fft=window_size, \n",
    "            n_mels=mel_bins, fmin=fmin, fmax=fmax, ref=ref, amin=amin, top_db=top_db, \n",
    "            freeze_parameters=True)\n",
    "\n",
    "        # Spec augmenter\n",
    "        self.spec_augmenter = SpecAugmentation(time_drop_width=64, time_stripes_num=2, \n",
    "            freq_drop_width=8, freq_stripes_num=2)\n",
    "\n",
    "        self.bn0 = nn.BatchNorm2d(64)\n",
    "\n",
    "        self.conv_block1 = ConvBlock(in_channels=1, out_channels=64)\n",
    "        # self.conv_block2 = ConvBlock(in_channels=64, out_channels=64)\n",
    "\n",
    "        self.resnet = _ResNet(block=_ResnetBasicBlock, layers=[2, 2, 2, 2], zero_init_residual=True)\n",
    "\n",
    "        self.conv_block_after1 = ConvBlock(in_channels=512, out_channels=2048)\n",
    "\n",
    "        self.fc1 = nn.Linear(2048, 2048)\n",
    "        self.fc_audioset = nn.Linear(2048, classes_num, bias=True)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        init_bn(self.bn0)\n",
    "        init_layer(self.fc1)\n",
    "        init_layer(self.fc_audioset)\n",
    "\n",
    "\n",
    "    def forward(self, input, mixup_lambda=None):\n",
    "        \"\"\"\n",
    "        Input: (batch_size, data_length)\"\"\"\n",
    "\n",
    "        x = self.spectrogram_extractor(input)   # (batch_size, 1, time_steps, freq_bins)\n",
    "        x = self.logmel_extractor(x)    # (batch_size, 1, time_steps, mel_bins)\n",
    "        \n",
    "        x = x.transpose(1, 3)\n",
    "        x = self.bn0(x)\n",
    "        x = x.transpose(1, 3)\n",
    "        \n",
    "        if self.training:\n",
    "            x = self.spec_augmenter(x)\n",
    "\n",
    "        # Mixup on spectrogram\n",
    "        if self.training and mixup_lambda is not None:\n",
    "            x = do_mixup(x, mixup_lambda)\n",
    "        \n",
    "        x = self.conv_block1(x, pool_size=(2, 2), pool_type='avg')\n",
    "        x = F.dropout(x, p=0.2, training=self.training, inplace=True)\n",
    "        x = self.resnet(x)\n",
    "        x = F.avg_pool2d(x, kernel_size=(2, 2))\n",
    "        x = F.dropout(x, p=0.2, training=self.training, inplace=True)\n",
    "        x = self.conv_block_after1(x, pool_size=(1, 1), pool_type='avg')\n",
    "        x = F.dropout(x, p=0.2, training=self.training, inplace=True)\n",
    "        x = torch.mean(x, dim=3)\n",
    "        \n",
    "        (x1, _) = torch.max(x, dim=2)\n",
    "        x2 = torch.mean(x, dim=2)\n",
    "        x = x1 + x2\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = F.relu_(self.fc1(x))\n",
    "        embedding = F.dropout(x, p=0.5, training=self.training)\n",
    "        clipwise_output = torch.sigmoid(self.fc_audioset(x))\n",
    "        \n",
    "        output_dict = {'clipwise_output': clipwise_output, 'embedding': embedding}\n",
    "\n",
    "        return output_dict\n",
    "\n",
    "\n",
    "class ResNet38(nn.Module):\n",
    "    def __init__(self, sample_rate, window_size, hop_size, mel_bins, fmin, \n",
    "        fmax, classes_num):\n",
    "        \n",
    "        super(ResNet38, self).__init__()\n",
    "\n",
    "        window = 'hann'\n",
    "        center = True\n",
    "        pad_mode = 'reflect'\n",
    "        ref = 1.0\n",
    "        amin = 1e-10\n",
    "        top_db = None\n",
    "\n",
    "        # Spectrogram extractor\n",
    "        self.spectrogram_extractor = Spectrogram(n_fft=window_size, hop_length=hop_size, \n",
    "            win_length=window_size, window=window, center=center, pad_mode=pad_mode, \n",
    "            freeze_parameters=True)\n",
    "\n",
    "        # Logmel feature extractor\n",
    "        self.logmel_extractor = LogmelFilterBank(sr=sample_rate, n_fft=window_size, \n",
    "            n_mels=mel_bins, fmin=fmin, fmax=fmax, ref=ref, amin=amin, top_db=top_db, \n",
    "            freeze_parameters=True)\n",
    "\n",
    "        # Spec augmenter\n",
    "        self.spec_augmenter = SpecAugmentation(time_drop_width=64, time_stripes_num=2, \n",
    "            freq_drop_width=8, freq_stripes_num=2)\n",
    "\n",
    "        self.bn0 = nn.BatchNorm2d(64)\n",
    "\n",
    "        self.conv_block1 = ConvBlock(in_channels=1, out_channels=64)\n",
    "        # self.conv_block2 = ConvBlock(in_channels=64, out_channels=64)\n",
    "\n",
    "        self.resnet = _ResNet(block=_ResnetBasicBlock, layers=[3, 4, 6, 3], zero_init_residual=True)\n",
    "\n",
    "        self.conv_block_after1 = ConvBlock(in_channels=512, out_channels=2048)\n",
    "\n",
    "        self.fc1 = nn.Linear(2048, 2048)\n",
    "        self.fc_audioset = nn.Linear(2048, classes_num, bias=True)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        init_bn(self.bn0)\n",
    "        init_layer(self.fc1)\n",
    "        init_layer(self.fc_audioset)\n",
    "\n",
    "\n",
    "    def forward(self, input, mixup_lambda=None):\n",
    "        \"\"\"\n",
    "        Input: (batch_size, data_length)\"\"\"\n",
    "\n",
    "        x = self.spectrogram_extractor(input)   # (batch_size, 1, time_steps, freq_bins)\n",
    "        x = self.logmel_extractor(x)    # (batch_size, 1, time_steps, mel_bins)\n",
    "        \n",
    "        x = x.transpose(1, 3)\n",
    "        x = self.bn0(x)\n",
    "        x = x.transpose(1, 3)\n",
    "        \n",
    "        if self.training:\n",
    "            x = self.spec_augmenter(x)\n",
    "\n",
    "        # Mixup on spectrogram\n",
    "        if self.training and mixup_lambda is not None:\n",
    "            x = do_mixup(x, mixup_lambda)\n",
    "        \n",
    "        x = self.conv_block1(x, pool_size=(2, 2), pool_type='avg')\n",
    "        x = F.dropout(x, p=0.2, training=self.training, inplace=True)\n",
    "        x = self.resnet(x)\n",
    "        x = F.avg_pool2d(x, kernel_size=(2, 2))\n",
    "        x = F.dropout(x, p=0.2, training=self.training, inplace=True)\n",
    "        x = self.conv_block_after1(x, pool_size=(1, 1), pool_type='avg')\n",
    "        x = F.dropout(x, p=0.2, training=self.training, inplace=True)\n",
    "        x = torch.mean(x, dim=3)\n",
    "        \n",
    "        (x1, _) = torch.max(x, dim=2)\n",
    "        x2 = torch.mean(x, dim=2)\n",
    "        x = x1 + x2\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = F.relu_(self.fc1(x))\n",
    "        embedding = F.dropout(x, p=0.5, training=self.training)\n",
    "        clipwise_output = torch.sigmoid(self.fc_audioset(x))\n",
    "        \n",
    "        output_dict = {'clipwise_output': clipwise_output, 'embedding': embedding}\n",
    "\n",
    "        return output_dict\n",
    "\n",
    "\n",
    "class ResNet54(nn.Module):\n",
    "    def __init__(self, sample_rate, window_size, hop_size, mel_bins, fmin, \n",
    "        fmax, classes_num):\n",
    "        \n",
    "        super(ResNet54, self).__init__()\n",
    "\n",
    "        window = 'hann'\n",
    "        center = True\n",
    "        pad_mode = 'reflect'\n",
    "        ref = 1.0\n",
    "        amin = 1e-10\n",
    "        top_db = None\n",
    "\n",
    "        # Spectrogram extractor\n",
    "        self.spectrogram_extractor = Spectrogram(n_fft=window_size, hop_length=hop_size, \n",
    "            win_length=window_size, window=window, center=center, pad_mode=pad_mode, \n",
    "            freeze_parameters=True)\n",
    "\n",
    "        # Logmel feature extractor\n",
    "        self.logmel_extractor = LogmelFilterBank(sr=sample_rate, n_fft=window_size, \n",
    "            n_mels=mel_bins, fmin=fmin, fmax=fmax, ref=ref, amin=amin, top_db=top_db, \n",
    "            freeze_parameters=True)\n",
    "\n",
    "        # Spec augmenter\n",
    "        self.spec_augmenter = SpecAugmentation(time_drop_width=64, time_stripes_num=2, \n",
    "            freq_drop_width=8, freq_stripes_num=2)\n",
    "\n",
    "        self.bn0 = nn.BatchNorm2d(64)\n",
    "\n",
    "        self.conv_block1 = ConvBlock(in_channels=1, out_channels=64)\n",
    "        # self.conv_block2 = ConvBlock(in_channels=64, out_channels=64)\n",
    "\n",
    "        self.resnet = _ResNet(block=_ResnetBottleneck, layers=[3, 4, 6, 3], zero_init_residual=True)\n",
    "\n",
    "        self.conv_block_after1 = ConvBlock(in_channels=2048, out_channels=2048)\n",
    "\n",
    "        self.fc1 = nn.Linear(2048, 2048)\n",
    "        self.fc_audioset = nn.Linear(2048, classes_num, bias=True)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        init_bn(self.bn0)\n",
    "        init_layer(self.fc1)\n",
    "        init_layer(self.fc_audioset)\n",
    "\n",
    "\n",
    "    def forward(self, input, mixup_lambda=None):\n",
    "        \"\"\"\n",
    "        Input: (batch_size, data_length)\"\"\"\n",
    "\n",
    "        x = self.spectrogram_extractor(input)   # (batch_size, 1, time_steps, freq_bins)\n",
    "        x = self.logmel_extractor(x)    # (batch_size, 1, time_steps, mel_bins)\n",
    "        \n",
    "        x = x.transpose(1, 3)\n",
    "        x = self.bn0(x)\n",
    "        x = x.transpose(1, 3)\n",
    "        \n",
    "        if self.training:\n",
    "            x = self.spec_augmenter(x)\n",
    "\n",
    "        # Mixup on spectrogram\n",
    "        if self.training and mixup_lambda is not None:\n",
    "            x = do_mixup(x, mixup_lambda)\n",
    "        \n",
    "        x = self.conv_block1(x, pool_size=(2, 2), pool_type='avg')\n",
    "        x = F.dropout(x, p=0.2, training=self.training, inplace=True)\n",
    "        x = self.resnet(x)\n",
    "        x = F.avg_pool2d(x, kernel_size=(2, 2))\n",
    "        x = F.dropout(x, p=0.2, training=self.training, inplace=True)\n",
    "        x = self.conv_block_after1(x, pool_size=(1, 1), pool_type='avg')\n",
    "        x = F.dropout(x, p=0.2, training=self.training, inplace=True)\n",
    "        x = torch.mean(x, dim=3)\n",
    "        \n",
    "        (x1, _) = torch.max(x, dim=2)\n",
    "        x2 = torch.mean(x, dim=2)\n",
    "        x = x1 + x2\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = F.relu_(self.fc1(x))\n",
    "        embedding = F.dropout(x, p=0.5, training=self.training)\n",
    "        clipwise_output = torch.sigmoid(self.fc_audioset(x))\n",
    "        \n",
    "        output_dict = {'clipwise_output': clipwise_output, 'embedding': embedding}\n",
    "\n",
    "        return output_dict\n",
    "\n",
    "\n",
    "class Cnn14_emb512(nn.Module):\n",
    "    def __init__(self, sample_rate, window_size, hop_size, mel_bins, fmin, \n",
    "        fmax, classes_num):\n",
    "        \n",
    "        super(Cnn14_emb512, self).__init__()\n",
    "\n",
    "        window = 'hann'\n",
    "        center = True\n",
    "        pad_mode = 'reflect'\n",
    "        ref = 1.0\n",
    "        amin = 1e-10\n",
    "        top_db = None\n",
    "\n",
    "        # Spectrogram extractor\n",
    "        self.spectrogram_extractor = Spectrogram(n_fft=window_size, hop_length=hop_size, \n",
    "            win_length=window_size, window=window, center=center, pad_mode=pad_mode, \n",
    "            freeze_parameters=True)\n",
    "\n",
    "        # Logmel feature extractor\n",
    "        self.logmel_extractor = LogmelFilterBank(sr=sample_rate, n_fft=window_size, \n",
    "            n_mels=mel_bins, fmin=fmin, fmax=fmax, ref=ref, amin=amin, top_db=top_db, \n",
    "            freeze_parameters=True)\n",
    "\n",
    "        # Spec augmenter\n",
    "        self.spec_augmenter = SpecAugmentation(time_drop_width=64, time_stripes_num=2, \n",
    "            freq_drop_width=8, freq_stripes_num=2)\n",
    "\n",
    "        self.bn0 = nn.BatchNorm2d(64)\n",
    "\n",
    "        self.conv_block1 = ConvBlock(in_channels=1, out_channels=64)\n",
    "        self.conv_block2 = ConvBlock(in_channels=64, out_channels=128)\n",
    "        self.conv_block3 = ConvBlock(in_channels=128, out_channels=256)\n",
    "        self.conv_block4 = ConvBlock(in_channels=256, out_channels=512)\n",
    "        self.conv_block5 = ConvBlock(in_channels=512, out_channels=1024)\n",
    "        self.conv_block6 = ConvBlock(in_channels=1024, out_channels=2048)\n",
    "\n",
    "        self.fc1 = nn.Linear(2048, 512, bias=True)\n",
    "        self.fc_audioset = nn.Linear(512, classes_num, bias=True)\n",
    "        \n",
    "        self.init_weight()\n",
    "\n",
    "    def init_weight(self):\n",
    "        init_bn(self.bn0)\n",
    "        init_layer(self.fc1)\n",
    "        init_layer(self.fc_audioset)\n",
    " \n",
    "    def forward(self, input, mixup_lambda=None):\n",
    "        \"\"\"\n",
    "        Input: (batch_size, data_length)\"\"\"\n",
    "\n",
    "        x = self.spectrogram_extractor(input)   # (batch_size, 1, time_steps, freq_bins)\n",
    "        x = self.logmel_extractor(x)    # (batch_size, 1, time_steps, mel_bins)\n",
    "        \n",
    "        x = x.transpose(1, 3)\n",
    "        x = self.bn0(x)\n",
    "        x = x.transpose(1, 3)\n",
    "        \n",
    "        if self.training:\n",
    "            x = self.spec_augmenter(x)\n",
    "\n",
    "        # Mixup on spectrogram\n",
    "        if self.training and mixup_lambda is not None:\n",
    "            x = do_mixup(x, mixup_lambda)\n",
    "        \n",
    "        x = self.conv_block1(x, pool_size=(2, 2), pool_type='avg')\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = self.conv_block2(x, pool_size=(2, 2), pool_type='avg')\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = self.conv_block3(x, pool_size=(2, 2), pool_type='avg')\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = self.conv_block4(x, pool_size=(2, 2), pool_type='avg')\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = self.conv_block5(x, pool_size=(2, 2), pool_type='avg')\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = self.conv_block6(x, pool_size=(1, 1), pool_type='avg')\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = torch.mean(x, dim=3)\n",
    "        \n",
    "        (x1, _) = torch.max(x, dim=2)\n",
    "        x2 = torch.mean(x, dim=2)\n",
    "        x = x1 + x2\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = F.relu_(self.fc1(x))\n",
    "        embedding = F.dropout(x, p=0.5, training=self.training)\n",
    "        clipwise_output = torch.sigmoid(self.fc_audioset(x))\n",
    "        \n",
    "        output_dict = {'clipwise_output': clipwise_output, 'embedding': embedding}\n",
    "\n",
    "        return output_dict\n",
    "\n",
    "\n",
    "class Cnn14_emb128(nn.Module):\n",
    "    def __init__(self, sample_rate, window_size, hop_size, mel_bins, fmin, \n",
    "        fmax, classes_num):\n",
    "        \n",
    "        super(Cnn14_emb128, self).__init__()\n",
    "\n",
    "        window = 'hann'\n",
    "        center = True\n",
    "        pad_mode = 'reflect'\n",
    "        ref = 1.0\n",
    "        amin = 1e-10\n",
    "        top_db = None\n",
    "\n",
    "        # Spectrogram extractor\n",
    "        self.spectrogram_extractor = Spectrogram(n_fft=window_size, hop_length=hop_size, \n",
    "            win_length=window_size, window=window, center=center, pad_mode=pad_mode, \n",
    "            freeze_parameters=True)\n",
    "\n",
    "        # Logmel feature extractor\n",
    "        self.logmel_extractor = LogmelFilterBank(sr=sample_rate, n_fft=window_size, \n",
    "            n_mels=mel_bins, fmin=fmin, fmax=fmax, ref=ref, amin=amin, top_db=top_db, \n",
    "            freeze_parameters=True)\n",
    "\n",
    "        # Spec augmenter\n",
    "        self.spec_augmenter = SpecAugmentation(time_drop_width=64, time_stripes_num=2, \n",
    "            freq_drop_width=8, freq_stripes_num=2)\n",
    "\n",
    "        self.bn0 = nn.BatchNorm2d(64)\n",
    "\n",
    "        self.conv_block1 = ConvBlock(in_channels=1, out_channels=64)\n",
    "        self.conv_block2 = ConvBlock(in_channels=64, out_channels=128)\n",
    "        self.conv_block3 = ConvBlock(in_channels=128, out_channels=256)\n",
    "        self.conv_block4 = ConvBlock(in_channels=256, out_channels=512)\n",
    "        self.conv_block5 = ConvBlock(in_channels=512, out_channels=1024)\n",
    "        self.conv_block6 = ConvBlock(in_channels=1024, out_channels=2048)\n",
    "\n",
    "        self.fc1 = nn.Linear(2048, 128, bias=True)\n",
    "        self.fc_audioset = nn.Linear(128, classes_num, bias=True)\n",
    "        \n",
    "        self.init_weight()\n",
    "\n",
    "    def init_weight(self):\n",
    "        init_bn(self.bn0)\n",
    "        init_layer(self.fc1)\n",
    "        init_layer(self.fc_audioset)\n",
    " \n",
    "    def forward(self, input, mixup_lambda=None):\n",
    "        \"\"\"\n",
    "        Input: (batch_size, data_length)\"\"\"\n",
    "\n",
    "        x = self.spectrogram_extractor(input)   # (batch_size, 1, time_steps, freq_bins)\n",
    "        x = self.logmel_extractor(x)    # (batch_size, 1, time_steps, mel_bins)\n",
    "        \n",
    "        x = x.transpose(1, 3)\n",
    "        x = self.bn0(x)\n",
    "        x = x.transpose(1, 3)\n",
    "        \n",
    "        if self.training:\n",
    "            x = self.spec_augmenter(x)\n",
    "\n",
    "        # Mixup on spectrogram\n",
    "        if self.training and mixup_lambda is not None:\n",
    "            x = do_mixup(x, mixup_lambda)\n",
    "        \n",
    "        x = self.conv_block1(x, pool_size=(2, 2), pool_type='avg')\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = self.conv_block2(x, pool_size=(2, 2), pool_type='avg')\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = self.conv_block3(x, pool_size=(2, 2), pool_type='avg')\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = self.conv_block4(x, pool_size=(2, 2), pool_type='avg')\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = self.conv_block5(x, pool_size=(2, 2), pool_type='avg')\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = self.conv_block6(x, pool_size=(1, 1), pool_type='avg')\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = torch.mean(x, dim=3)\n",
    "        \n",
    "        (x1, _) = torch.max(x, dim=2)\n",
    "        x2 = torch.mean(x, dim=2)\n",
    "        x = x1 + x2\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = F.relu_(self.fc1(x))\n",
    "        embedding = F.dropout(x, p=0.5, training=self.training)\n",
    "        clipwise_output = torch.sigmoid(self.fc_audioset(x))\n",
    "        \n",
    "        output_dict = {'clipwise_output': clipwise_output, 'embedding': embedding}\n",
    "\n",
    "        return output_dict\n",
    "\n",
    "\n",
    "class Cnn14_emb32(nn.Module):\n",
    "    def __init__(self, sample_rate, window_size, hop_size, mel_bins, fmin, \n",
    "        fmax, classes_num):\n",
    "        \n",
    "        super(Cnn14_emb32, self).__init__()\n",
    "\n",
    "        window = 'hann'\n",
    "        center = True\n",
    "        pad_mode = 'reflect'\n",
    "        ref = 1.0\n",
    "        amin = 1e-10\n",
    "        top_db = None\n",
    "\n",
    "        # Spectrogram extractor\n",
    "        self.spectrogram_extractor = Spectrogram(n_fft=window_size, hop_length=hop_size, \n",
    "            win_length=window_size, window=window, center=center, pad_mode=pad_mode, \n",
    "            freeze_parameters=True)\n",
    "\n",
    "        # Logmel feature extractor\n",
    "        self.logmel_extractor = LogmelFilterBank(sr=sample_rate, n_fft=window_size, \n",
    "            n_mels=mel_bins, fmin=fmin, fmax=fmax, ref=ref, amin=amin, top_db=top_db, \n",
    "            freeze_parameters=True)\n",
    "\n",
    "        # Spec augmenter\n",
    "        self.spec_augmenter = SpecAugmentation(time_drop_width=64, time_stripes_num=2, \n",
    "            freq_drop_width=8, freq_stripes_num=2)\n",
    "\n",
    "        self.bn0 = nn.BatchNorm2d(64)\n",
    "\n",
    "        self.conv_block1 = ConvBlock(in_channels=1, out_channels=64)\n",
    "        self.conv_block2 = ConvBlock(in_channels=64, out_channels=128)\n",
    "        self.conv_block3 = ConvBlock(in_channels=128, out_channels=256)\n",
    "        self.conv_block4 = ConvBlock(in_channels=256, out_channels=512)\n",
    "        self.conv_block5 = ConvBlock(in_channels=512, out_channels=1024)\n",
    "        self.conv_block6 = ConvBlock(in_channels=1024, out_channels=2048)\n",
    "\n",
    "        self.fc1 = nn.Linear(2048, 32, bias=True)\n",
    "        self.fc_audioset = nn.Linear(32, classes_num, bias=True)\n",
    "        \n",
    "        self.init_weight()\n",
    "\n",
    "    def init_weight(self):\n",
    "        init_bn(self.bn0)\n",
    "        init_layer(self.fc1)\n",
    "        init_layer(self.fc_audioset)\n",
    " \n",
    "    def forward(self, input, mixup_lambda=None):\n",
    "        \"\"\"\n",
    "        Input: (batch_size, data_length)\"\"\"\n",
    "\n",
    "        x = self.spectrogram_extractor(input)   # (batch_size, 1, time_steps, freq_bins)\n",
    "        x = self.logmel_extractor(x)    # (batch_size, 1, time_steps, mel_bins)\n",
    "        \n",
    "        x = x.transpose(1, 3)\n",
    "        x = self.bn0(x)\n",
    "        x = x.transpose(1, 3)\n",
    "        \n",
    "        if self.training:\n",
    "            x = self.spec_augmenter(x)\n",
    "\n",
    "        # Mixup on spectrogram\n",
    "        if self.training and mixup_lambda is not None:\n",
    "            x = do_mixup(x, mixup_lambda)\n",
    "        \n",
    "        x = self.conv_block1(x, pool_size=(2, 2), pool_type='avg')\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = self.conv_block2(x, pool_size=(2, 2), pool_type='avg')\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = self.conv_block3(x, pool_size=(2, 2), pool_type='avg')\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = self.conv_block4(x, pool_size=(2, 2), pool_type='avg')\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = self.conv_block5(x, pool_size=(2, 2), pool_type='avg')\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = self.conv_block6(x, pool_size=(1, 1), pool_type='avg')\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = torch.mean(x, dim=3)\n",
    "        \n",
    "        (x1, _) = torch.max(x, dim=2)\n",
    "        x2 = torch.mean(x, dim=2)\n",
    "        x = x1 + x2\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = F.relu_(self.fc1(x))\n",
    "        embedding = F.dropout(x, p=0.5, training=self.training)\n",
    "        clipwise_output = torch.sigmoid(self.fc_audioset(x))\n",
    "        \n",
    "        output_dict = {'clipwise_output': clipwise_output, 'embedding': embedding}\n",
    "\n",
    "        return output_dict\n",
    "\n",
    "\n",
    "class MobileNetV1(nn.Module):\n",
    "    def __init__(self, sample_rate, window_size, hop_size, mel_bins, fmin, \n",
    "        fmax, classes_num):\n",
    "        \n",
    "        super(MobileNetV1, self).__init__()\n",
    "\n",
    "        window = 'hann'\n",
    "        center = True\n",
    "        pad_mode = 'reflect'\n",
    "        ref = 1.0\n",
    "        amin = 1e-10\n",
    "        top_db = None\n",
    "\n",
    "        # Spectrogram extractor\n",
    "        self.spectrogram_extractor = Spectrogram(n_fft=window_size, hop_length=hop_size, \n",
    "            win_length=window_size, window=window, center=center, pad_mode=pad_mode, \n",
    "            freeze_parameters=True)\n",
    "\n",
    "        # Logmel feature extractor\n",
    "        self.logmel_extractor = LogmelFilterBank(sr=sample_rate, n_fft=window_size, \n",
    "            n_mels=mel_bins, fmin=fmin, fmax=fmax, ref=ref, amin=amin, top_db=top_db, \n",
    "            freeze_parameters=True)\n",
    "\n",
    "        # Spec augmenter\n",
    "        self.spec_augmenter = SpecAugmentation(time_drop_width=64, time_stripes_num=2, \n",
    "            freq_drop_width=8, freq_stripes_num=2)\n",
    "\n",
    "        self.bn0 = nn.BatchNorm2d(64)\n",
    "\n",
    "        def conv_bn(inp, oup, stride):\n",
    "            _layers = [\n",
    "                nn.Conv2d(inp, oup, 3, 1, 1, bias=False), \n",
    "                nn.AvgPool2d(stride), \n",
    "                nn.BatchNorm2d(oup), \n",
    "                nn.ReLU(inplace=True)\n",
    "                ]\n",
    "            _layers = nn.Sequential(*_layers)\n",
    "            init_layer(_layers[0])\n",
    "            init_bn(_layers[2])\n",
    "            return _layers\n",
    "\n",
    "        def conv_dw(inp, oup, stride):\n",
    "            _layers = [\n",
    "                nn.Conv2d(inp, inp, 3, 1, 1, groups=inp, bias=False), \n",
    "                nn.AvgPool2d(stride), \n",
    "                nn.BatchNorm2d(inp), \n",
    "                nn.ReLU(inplace=True), \n",
    "                nn.Conv2d(inp, oup, 1, 1, 0, bias=False), \n",
    "                nn.BatchNorm2d(oup), \n",
    "                nn.ReLU(inplace=True)\n",
    "                ]\n",
    "            _layers = nn.Sequential(*_layers)\n",
    "            init_layer(_layers[0])\n",
    "            init_bn(_layers[2])\n",
    "            init_layer(_layers[4])\n",
    "            init_bn(_layers[5])\n",
    "            return _layers\n",
    "\n",
    "        self.features = nn.Sequential(\n",
    "            conv_bn(  1,  32, 2), \n",
    "            conv_dw( 32,  64, 1),\n",
    "            conv_dw( 64, 128, 2),\n",
    "            conv_dw(128, 128, 1),\n",
    "            conv_dw(128, 256, 2),\n",
    "            conv_dw(256, 256, 1),\n",
    "            conv_dw(256, 512, 2),\n",
    "            conv_dw(512, 512, 1),\n",
    "            conv_dw(512, 512, 1),\n",
    "            conv_dw(512, 512, 1),\n",
    "            conv_dw(512, 512, 1),\n",
    "            conv_dw(512, 512, 1),\n",
    "            conv_dw(512, 1024, 2),\n",
    "            conv_dw(1024, 1024, 1))\n",
    "\n",
    "        self.fc1 = nn.Linear(1024, 1024, bias=True)\n",
    "        self.fc_audioset = nn.Linear(1024, classes_num, bias=True)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        init_bn(self.bn0)\n",
    "        init_layer(self.fc1)\n",
    "        init_layer(self.fc_audioset)\n",
    " \n",
    "    def forward(self, input, mixup_lambda=None):\n",
    "        \"\"\"\n",
    "        Input: (batch_size, data_length)\"\"\"\n",
    "\n",
    "        x = self.spectrogram_extractor(input)   # (batch_size, 1, time_steps, freq_bins)\n",
    "        x = self.logmel_extractor(x)    # (batch_size, 1, time_steps, mel_bins)\n",
    "        \n",
    "        x = x.transpose(1, 3)\n",
    "        x = self.bn0(x)\n",
    "        x = x.transpose(1, 3)\n",
    "        \n",
    "        if self.training:\n",
    "            x = self.spec_augmenter(x)\n",
    "\n",
    "        # Mixup on spectrogram\n",
    "        if self.training and mixup_lambda is not None:\n",
    "            x = do_mixup(x, mixup_lambda)\n",
    "        \n",
    "        x = self.features(x)\n",
    "        x = torch.mean(x, dim=3)\n",
    "        \n",
    "        (x1, _) = torch.max(x, dim=2)\n",
    "        x2 = torch.mean(x, dim=2)\n",
    "        x = x1 + x2\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = F.relu_(self.fc1(x))\n",
    "        embedding = F.dropout(x, p=0.5, training=self.training)\n",
    "        clipwise_output = torch.sigmoid(self.fc_audioset(x))\n",
    "        \n",
    "        output_dict = {'clipwise_output': clipwise_output, 'embedding': embedding}\n",
    "\n",
    "        return output_dict\n",
    "\n",
    "\n",
    "class InvertedResidual(nn.Module):\n",
    "    def __init__(self, inp, oup, stride, expand_ratio):\n",
    "        super(InvertedResidual, self).__init__()\n",
    "        self.stride = stride\n",
    "        assert stride in [1, 2]\n",
    "\n",
    "        hidden_dim = round(inp * expand_ratio)\n",
    "        self.use_res_connect = self.stride == 1 and inp == oup\n",
    "\n",
    "        if expand_ratio == 1:\n",
    "            _layers = [\n",
    "                nn.Conv2d(hidden_dim, hidden_dim, 3, 1, 1, groups=hidden_dim, bias=False), \n",
    "                nn.AvgPool2d(stride), \n",
    "                nn.BatchNorm2d(hidden_dim), \n",
    "                nn.ReLU6(inplace=True), \n",
    "                nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False), \n",
    "                nn.BatchNorm2d(oup)\n",
    "                ]\n",
    "            _layers = nn.Sequential(*_layers)\n",
    "            init_layer(_layers[0])\n",
    "            init_bn(_layers[2])\n",
    "            init_layer(_layers[4])\n",
    "            init_bn(_layers[5])\n",
    "            self.conv = _layers\n",
    "        else:\n",
    "            _layers = [\n",
    "                nn.Conv2d(inp, hidden_dim, 1, 1, 0, bias=False), \n",
    "                nn.BatchNorm2d(hidden_dim), \n",
    "                nn.ReLU6(inplace=True), \n",
    "                nn.Conv2d(hidden_dim, hidden_dim, 3, 1, 1, groups=hidden_dim, bias=False), \n",
    "                nn.AvgPool2d(stride), \n",
    "                nn.BatchNorm2d(hidden_dim), \n",
    "                nn.ReLU6(inplace=True), \n",
    "                nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False), \n",
    "                nn.BatchNorm2d(oup)\n",
    "                ]\n",
    "            _layers = nn.Sequential(*_layers)\n",
    "            init_layer(_layers[0])\n",
    "            init_bn(_layers[1])\n",
    "            init_layer(_layers[3])\n",
    "            init_bn(_layers[5])\n",
    "            init_layer(_layers[7])\n",
    "            init_bn(_layers[8])\n",
    "            self.conv = _layers\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.use_res_connect:\n",
    "            return x + self.conv(x)\n",
    "        else:\n",
    "            return self.conv(x)\n",
    "\n",
    "\n",
    "class MobileNetV2(nn.Module):\n",
    "    def __init__(self, sample_rate, window_size, hop_size, mel_bins, fmin, \n",
    "        fmax, classes_num):\n",
    "        \n",
    "        super(MobileNetV2, self).__init__()\n",
    "\n",
    "        window = 'hann'\n",
    "        center = True\n",
    "        pad_mode = 'reflect'\n",
    "        ref = 1.0\n",
    "        amin = 1e-10\n",
    "        top_db = None\n",
    "\n",
    "        # Spectrogram extractor\n",
    "        self.spectrogram_extractor = Spectrogram(n_fft=window_size, hop_length=hop_size, \n",
    "            win_length=window_size, window=window, center=center, pad_mode=pad_mode, \n",
    "            freeze_parameters=True)\n",
    "\n",
    "        # Logmel feature extractor\n",
    "        self.logmel_extractor = LogmelFilterBank(sr=sample_rate, n_fft=window_size, \n",
    "            n_mels=mel_bins, fmin=fmin, fmax=fmax, ref=ref, amin=amin, top_db=top_db, \n",
    "            freeze_parameters=True)\n",
    "\n",
    "        # Spec augmenter\n",
    "        self.spec_augmenter = SpecAugmentation(time_drop_width=64, time_stripes_num=2, \n",
    "            freq_drop_width=8, freq_stripes_num=2)\n",
    "\n",
    "        self.bn0 = nn.BatchNorm2d(64)\n",
    " \n",
    "        width_mult=1.\n",
    "        block = InvertedResidual\n",
    "        input_channel = 32\n",
    "        last_channel = 1280\n",
    "        interverted_residual_setting = [\n",
    "            # t, c, n, s\n",
    "            [1, 16, 1, 1],\n",
    "            [6, 24, 2, 2],\n",
    "            [6, 32, 3, 2],\n",
    "            [6, 64, 4, 2],\n",
    "            [6, 96, 3, 2],\n",
    "            [6, 160, 3, 1],\n",
    "            [6, 320, 1, 1],\n",
    "        ]\n",
    "\n",
    "        def conv_bn(inp, oup, stride):\n",
    "            _layers = [\n",
    "                nn.Conv2d(inp, oup, 3, 1, 1, bias=False), \n",
    "                nn.AvgPool2d(stride), \n",
    "                nn.BatchNorm2d(oup), \n",
    "                nn.ReLU6(inplace=True)\n",
    "                ]\n",
    "            _layers = nn.Sequential(*_layers)\n",
    "            init_layer(_layers[0])\n",
    "            init_bn(_layers[2])\n",
    "            return _layers\n",
    "\n",
    "\n",
    "        def conv_1x1_bn(inp, oup):\n",
    "            _layers = nn.Sequential(\n",
    "                nn.Conv2d(inp, oup, 1, 1, 0, bias=False),\n",
    "                nn.BatchNorm2d(oup),\n",
    "                nn.ReLU6(inplace=True)\n",
    "            )\n",
    "            init_layer(_layers[0])\n",
    "            init_bn(_layers[1])\n",
    "            return _layers\n",
    "\n",
    "        # building first layer\n",
    "        input_channel = int(input_channel * width_mult)\n",
    "        self.last_channel = int(last_channel * width_mult) if width_mult > 1.0 else last_channel\n",
    "        self.features = [conv_bn(1, input_channel, 2)]\n",
    "        # building inverted residual blocks\n",
    "        for t, c, n, s in interverted_residual_setting:\n",
    "            output_channel = int(c * width_mult)\n",
    "            for i in range(n):\n",
    "                if i == 0:\n",
    "                    self.features.append(block(input_channel, output_channel, s, expand_ratio=t))\n",
    "                else:\n",
    "                    self.features.append(block(input_channel, output_channel, 1, expand_ratio=t))\n",
    "                input_channel = output_channel\n",
    "        # building last several layers\n",
    "        self.features.append(conv_1x1_bn(input_channel, self.last_channel))\n",
    "        # make it nn.Sequential\n",
    "        self.features = nn.Sequential(*self.features)\n",
    "\n",
    "        self.fc1 = nn.Linear(1280, 1024, bias=True)\n",
    "        self.fc_audioset = nn.Linear(1024, classes_num, bias=True)\n",
    "        \n",
    "        self.init_weight()\n",
    "\n",
    "    def init_weight(self):\n",
    "        init_bn(self.bn0)\n",
    "        init_layer(self.fc1)\n",
    "        init_layer(self.fc_audioset)\n",
    " \n",
    "    def forward(self, input, mixup_lambda=None):\n",
    "        \"\"\"\n",
    "        Input: (batch_size, data_length)\"\"\"\n",
    "\n",
    "        x = self.spectrogram_extractor(input)   # (batch_size, 1, time_steps, freq_bins)\n",
    "        x = self.logmel_extractor(x)    # (batch_size, 1, time_steps, mel_bins)\n",
    "        \n",
    "        x = x.transpose(1, 3)\n",
    "        x = self.bn0(x)\n",
    "        x = x.transpose(1, 3)\n",
    "        \n",
    "        if self.training:\n",
    "            x = self.spec_augmenter(x)\n",
    "\n",
    "        # Mixup on spectrogram\n",
    "        if self.training and mixup_lambda is not None:\n",
    "            x = do_mixup(x, mixup_lambda)\n",
    "        \n",
    "        x = self.features(x)\n",
    "        \n",
    "        x = torch.mean(x, dim=3)\n",
    "        \n",
    "        (x1, _) = torch.max(x, dim=2)\n",
    "        x2 = torch.mean(x, dim=2)\n",
    "        x = x1 + x2\n",
    "        # x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = F.relu_(self.fc1(x))\n",
    "        embedding = F.dropout(x, p=0.5, training=self.training)\n",
    "        clipwise_output = torch.sigmoid(self.fc_audioset(x))\n",
    "        \n",
    "        output_dict = {'clipwise_output': clipwise_output, 'embedding': embedding}\n",
    "\n",
    "        return output_dict\n",
    "\n",
    "\n",
    "class LeeNetConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride):\n",
    "        \n",
    "        super(LeeNetConvBlock, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv1d(in_channels=in_channels, \n",
    "                              out_channels=out_channels,\n",
    "                              kernel_size=kernel_size, stride=stride,\n",
    "                              padding=kernel_size // 2, bias=False)\n",
    "                              \n",
    "        self.bn1 = nn.BatchNorm1d(out_channels)\n",
    "\n",
    "        self.init_weight()\n",
    "        \n",
    "    def init_weight(self):\n",
    "        init_layer(self.conv1)\n",
    "        init_bn(self.bn1)\n",
    "\n",
    "    def forward(self, x, pool_size=1):\n",
    "        x = F.relu_(self.bn1(self.conv1(x)))\n",
    "        if pool_size != 1:\n",
    "            x = F.max_pool1d(x, kernel_size=pool_size, padding=pool_size // 2)\n",
    "        return x\n",
    "\n",
    "\n",
    "class LeeNet11(nn.Module):\n",
    "    def __init__(self, sample_rate, window_size, hop_size, mel_bins, fmin, \n",
    "        fmax, classes_num):\n",
    "        \n",
    "        super(LeeNet11, self).__init__()\n",
    "\n",
    "        window = 'hann'\n",
    "        center = True\n",
    "        pad_mode = 'reflect'\n",
    "        ref = 1.0\n",
    "        amin = 1e-10\n",
    "        top_db = None\n",
    "\n",
    "        self.conv_block1 = LeeNetConvBlock(1, 64, 3, 3)\n",
    "        self.conv_block2 = LeeNetConvBlock(64, 64, 3, 1)\n",
    "        self.conv_block3 = LeeNetConvBlock(64, 64, 3, 1)\n",
    "        self.conv_block4 = LeeNetConvBlock(64, 128, 3, 1)\n",
    "        self.conv_block5 = LeeNetConvBlock(128, 128, 3, 1)\n",
    "        self.conv_block6 = LeeNetConvBlock(128, 128, 3, 1)\n",
    "        self.conv_block7 = LeeNetConvBlock(128, 128, 3, 1)\n",
    "        self.conv_block8 = LeeNetConvBlock(128, 128, 3, 1)\n",
    "        self.conv_block9 = LeeNetConvBlock(128, 256, 3, 1)\n",
    "        \n",
    "\n",
    "        self.fc1 = nn.Linear(256, 512, bias=True)\n",
    "        self.fc_audioset = nn.Linear(512, classes_num, bias=True)\n",
    "        \n",
    "        self.init_weight()\n",
    "\n",
    "    def init_weight(self):\n",
    "        init_layer(self.fc1)\n",
    "        init_layer(self.fc_audioset)\n",
    " \n",
    "    def forward(self, input, mixup_lambda=None):\n",
    "        \"\"\"\n",
    "        Input: (batch_size, data_length)\"\"\"\n",
    "\n",
    "        x = input[:, None, :]\n",
    "\n",
    "        # Mixup on spectrogram\n",
    "        if self.training and mixup_lambda is not None:\n",
    "            x = do_mixup(x, mixup_lambda)\n",
    "        \n",
    "        x = self.conv_block1(x)\n",
    "        x = self.conv_block2(x, pool_size=3)\n",
    "        x = self.conv_block3(x, pool_size=3)\n",
    "        x = self.conv_block4(x, pool_size=3)\n",
    "        x = self.conv_block5(x, pool_size=3)\n",
    "        x = self.conv_block6(x, pool_size=3)\n",
    "        x = self.conv_block7(x, pool_size=3)\n",
    "        x = self.conv_block8(x, pool_size=3)\n",
    "        x = self.conv_block9(x, pool_size=3)\n",
    "        \n",
    "        (x1, _) = torch.max(x, dim=2)\n",
    "        x2 = torch.mean(x, dim=2)\n",
    "        x = x1 + x2\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = F.relu_(self.fc1(x))\n",
    "        embedding = F.dropout(x, p=0.5, training=self.training)\n",
    "        clipwise_output = torch.sigmoid(self.fc_audioset(x))\n",
    "        \n",
    "        output_dict = {'clipwise_output': clipwise_output, 'embedding': embedding}\n",
    "\n",
    "        return output_dict\n",
    "\n",
    "\n",
    "class LeeNetConvBlock2(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride):\n",
    "        \n",
    "        super(LeeNetConvBlock2, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv1d(in_channels=in_channels, \n",
    "                              out_channels=out_channels,\n",
    "                              kernel_size=kernel_size, stride=stride,\n",
    "                              padding=kernel_size // 2, bias=False)\n",
    "                              \n",
    "        self.conv2 = nn.Conv1d(in_channels=out_channels, \n",
    "                              out_channels=out_channels,\n",
    "                              kernel_size=kernel_size, stride=1,\n",
    "                              padding=kernel_size // 2, bias=False)\n",
    "\n",
    "        self.bn1 = nn.BatchNorm1d(out_channels)\n",
    "        self.bn2 = nn.BatchNorm1d(out_channels)\n",
    "\n",
    "        self.init_weight()\n",
    "        \n",
    "    def init_weight(self):\n",
    "        init_layer(self.conv1)\n",
    "        init_layer(self.conv2)\n",
    "        init_bn(self.bn1)\n",
    "        init_bn(self.bn2)\n",
    "\n",
    "    def forward(self, x, pool_size=1):\n",
    "        x = F.relu_(self.bn1(self.conv1(x)))\n",
    "        x = F.relu_(self.bn2(self.conv2(x)))\n",
    "        if pool_size != 1:\n",
    "            x = F.max_pool1d(x, kernel_size=pool_size, padding=pool_size // 2)\n",
    "        return x\n",
    "\n",
    "\n",
    "class LeeNet24(nn.Module):\n",
    "    def __init__(self, sample_rate, window_size, hop_size, mel_bins, fmin, \n",
    "        fmax, classes_num):\n",
    "        \n",
    "        super(LeeNet24, self).__init__()\n",
    "\n",
    "        window = 'hann'\n",
    "        center = True\n",
    "        pad_mode = 'reflect'\n",
    "        ref = 1.0\n",
    "        amin = 1e-10\n",
    "        top_db = None\n",
    "\n",
    "        self.conv_block1 = LeeNetConvBlock2(1, 64, 3, 3)\n",
    "        self.conv_block2 = LeeNetConvBlock2(64, 96, 3, 1)\n",
    "        self.conv_block3 = LeeNetConvBlock2(96, 128, 3, 1)\n",
    "        self.conv_block4 = LeeNetConvBlock2(128, 128, 3, 1)\n",
    "        self.conv_block5 = LeeNetConvBlock2(128, 256, 3, 1)\n",
    "        self.conv_block6 = LeeNetConvBlock2(256, 256, 3, 1)\n",
    "        self.conv_block7 = LeeNetConvBlock2(256, 512, 3, 1)\n",
    "        self.conv_block8 = LeeNetConvBlock2(512, 512, 3, 1)\n",
    "        self.conv_block9 = LeeNetConvBlock2(512, 1024, 3, 1)\n",
    "\n",
    "        self.fc1 = nn.Linear(1024, 1024, bias=True)\n",
    "        self.fc_audioset = nn.Linear(1024, classes_num, bias=True)\n",
    "        \n",
    "        self.init_weight()\n",
    "\n",
    "    def init_weight(self):\n",
    "        init_layer(self.fc1)\n",
    "        init_layer(self.fc_audioset)\n",
    " \n",
    "    def forward(self, input, mixup_lambda=None):\n",
    "        \"\"\"\n",
    "        Input: (batch_size, data_length)\"\"\"\n",
    "\n",
    "        x = input[:, None, :]\n",
    "\n",
    "        # Mixup on spectrogram\n",
    "        if self.training and mixup_lambda is not None:\n",
    "            x = do_mixup(x, mixup_lambda)\n",
    "        \n",
    "        x = self.conv_block1(x)\n",
    "        x = F.dropout(x, p=0.1, training=self.training)\n",
    "        x = self.conv_block2(x, pool_size=3)\n",
    "        x = F.dropout(x, p=0.1, training=self.training)\n",
    "        x = self.conv_block3(x, pool_size=3)\n",
    "        x = F.dropout(x, p=0.1, training=self.training)\n",
    "        x = self.conv_block4(x, pool_size=3)\n",
    "        x = F.dropout(x, p=0.1, training=self.training)\n",
    "        x = self.conv_block5(x, pool_size=3)\n",
    "        x = F.dropout(x, p=0.1, training=self.training)\n",
    "        x = self.conv_block6(x, pool_size=3)\n",
    "        x = F.dropout(x, p=0.1, training=self.training)\n",
    "        x = self.conv_block7(x, pool_size=3)\n",
    "        x = F.dropout(x, p=0.1, training=self.training)\n",
    "        x = self.conv_block8(x, pool_size=3)\n",
    "        x = F.dropout(x, p=0.1, training=self.training)\n",
    "        x = self.conv_block9(x, pool_size=1)\n",
    "\n",
    "        (x1, _) = torch.max(x, dim=2)\n",
    "        x2 = torch.mean(x, dim=2)\n",
    "        x = x1 + x2\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = F.relu_(self.fc1(x))\n",
    "        embedding = F.dropout(x, p=0.5, training=self.training)\n",
    "        clipwise_output = torch.sigmoid(self.fc_audioset(x))\n",
    "        \n",
    "        output_dict = {'clipwise_output': clipwise_output, 'embedding': embedding}\n",
    "\n",
    "        return output_dict\n",
    "\n",
    "\n",
    "class DaiNetResBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size):\n",
    "        \n",
    "        super(DaiNetResBlock, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv1d(in_channels=in_channels, \n",
    "                              out_channels=out_channels,\n",
    "                              kernel_size=kernel_size, stride=1,\n",
    "                              padding=kernel_size // 2, bias=False)\n",
    "\n",
    "        self.conv2 = nn.Conv1d(in_channels=out_channels, \n",
    "                              out_channels=out_channels,\n",
    "                              kernel_size=kernel_size, stride=1,\n",
    "                              padding=kernel_size // 2, bias=False)\n",
    "\n",
    "        self.conv3 = nn.Conv1d(in_channels=out_channels, \n",
    "                              out_channels=out_channels,\n",
    "                              kernel_size=kernel_size, stride=1,\n",
    "                              padding=kernel_size // 2, bias=False)\n",
    "\n",
    "        self.conv4 = nn.Conv1d(in_channels=out_channels, \n",
    "                              out_channels=out_channels,\n",
    "                              kernel_size=kernel_size, stride=1,\n",
    "                              padding=kernel_size // 2, bias=False)\n",
    "\n",
    "        self.downsample = nn.Conv1d(in_channels=in_channels, \n",
    "                              out_channels=out_channels,\n",
    "                              kernel_size=1, stride=1,\n",
    "                              padding=0, bias=False)\n",
    "\n",
    "        self.bn1 = nn.BatchNorm1d(out_channels)\n",
    "        self.bn2 = nn.BatchNorm1d(out_channels)\n",
    "        self.bn3 = nn.BatchNorm1d(out_channels)\n",
    "        self.bn4 = nn.BatchNorm1d(out_channels)\n",
    "        self.bn_downsample = nn.BatchNorm1d(out_channels)\n",
    "\n",
    "        self.init_weight()\n",
    "        \n",
    "    def init_weight(self):\n",
    "        init_layer(self.conv1)\n",
    "        init_layer(self.conv2)\n",
    "        init_layer(self.conv3)\n",
    "        init_layer(self.conv4)\n",
    "        init_layer(self.downsample)\n",
    "        init_bn(self.bn1)\n",
    "        init_bn(self.bn2)\n",
    "        init_bn(self.bn3)\n",
    "        init_bn(self.bn4)\n",
    "        nn.init.constant_(self.bn4.weight, 0)\n",
    "        init_bn(self.bn_downsample)\n",
    "\n",
    "    def forward(self, input, pool_size=1):\n",
    "        x = F.relu_(self.bn1(self.conv1(input)))\n",
    "        x = F.relu_(self.bn2(self.conv2(x)))\n",
    "        x = F.relu_(self.bn3(self.conv3(x)))\n",
    "        x = self.bn4(self.conv4(x))\n",
    "        if input.shape == x.shape:\n",
    "            x = F.relu_(x + input)\n",
    "        else:\n",
    "            x = F.relu(x + self.bn_downsample(self.downsample(input)))\n",
    "\n",
    "        if pool_size != 1:\n",
    "            x = F.max_pool1d(x, kernel_size=pool_size, padding=pool_size // 2)\n",
    "        return x\n",
    "\n",
    "\n",
    "class DaiNet19(nn.Module):\n",
    "    def __init__(self, sample_rate, window_size, hop_size, mel_bins, fmin, \n",
    "        fmax, classes_num):\n",
    "        \n",
    "        super(DaiNet19, self).__init__()\n",
    "\n",
    "        window = 'hann'\n",
    "        center = True\n",
    "        pad_mode = 'reflect'\n",
    "        ref = 1.0\n",
    "        amin = 1e-10\n",
    "        top_db = None\n",
    "\n",
    "        self.conv0 = nn.Conv1d(in_channels=1, out_channels=64, kernel_size=80, stride=4, padding=0, bias=False)\n",
    "        self.bn0 = nn.BatchNorm1d(64)\n",
    "        self.conv_block1 = DaiNetResBlock(64, 64, 3)\n",
    "        self.conv_block2 = DaiNetResBlock(64, 128, 3)\n",
    "        self.conv_block3 = DaiNetResBlock(128, 256, 3)\n",
    "        self.conv_block4 = DaiNetResBlock(256, 512, 3)\n",
    "\n",
    "        self.fc1 = nn.Linear(512, 512, bias=True)\n",
    "        self.fc_audioset = nn.Linear(512, classes_num, bias=True)\n",
    "        \n",
    "        self.init_weight()\n",
    "\n",
    "    def init_weight(self):\n",
    "        init_layer(self.conv0)\n",
    "        init_bn(self.bn0)\n",
    "        init_layer(self.fc1)\n",
    "        init_layer(self.fc_audioset)\n",
    " \n",
    "    def forward(self, input, mixup_lambda=None):\n",
    "        \"\"\"\n",
    "        Input: (batch_size, data_length)\"\"\"\n",
    "\n",
    "        x = input[:, None, :]\n",
    "\n",
    "        # Mixup on spectrogram\n",
    "        if self.training and mixup_lambda is not None:\n",
    "            x = do_mixup(x, mixup_lambda)\n",
    "\n",
    "        x = self.bn0(self.conv0(x))\n",
    "        x = self.conv_block1(x)\n",
    "        x = F.max_pool1d(x, kernel_size=4)\n",
    "        x = self.conv_block2(x)\n",
    "        x = F.max_pool1d(x, kernel_size=4)\n",
    "        x = self.conv_block3(x)\n",
    "        x = F.max_pool1d(x, kernel_size=4)\n",
    "        x = self.conv_block4(x)\n",
    "\n",
    "        (x1, _) = torch.max(x, dim=2)\n",
    "        x2 = torch.mean(x, dim=2)\n",
    "        x = x1 + x2\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = F.relu_(self.fc1(x))\n",
    "        embedding = F.dropout(x, p=0.5, training=self.training)\n",
    "        clipwise_output = torch.sigmoid(self.fc_audioset(x))\n",
    "        \n",
    "        output_dict = {'clipwise_output': clipwise_output, 'embedding': embedding}\n",
    "\n",
    "        return output_dict\n",
    "\n",
    "\n",
    "def _resnet_conv3x1_wav1d(in_planes, out_planes, dilation):\n",
    "    #3x3 convolution with padding\n",
    "    return nn.Conv1d(in_planes, out_planes, kernel_size=3, stride=1,\n",
    "                     padding=dilation, groups=1, bias=False, dilation=dilation)\n",
    "\n",
    "\n",
    "def _resnet_conv1x1_wav1d(in_planes, out_planes):\n",
    "    #1x1 convolution\n",
    "    return nn.Conv1d(in_planes, out_planes, kernel_size=1, stride=1, bias=False)\n",
    "\n",
    "\n",
    "class _ResnetBasicBlockWav1d(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,\n",
    "                 base_width=64, dilation=1, norm_layer=None):\n",
    "        super(_ResnetBasicBlockWav1d, self).__init__()\n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.BatchNorm1d\n",
    "        if groups != 1 or base_width != 64:\n",
    "            raise ValueError('_ResnetBasicBlock only supports groups=1 and base_width=64')\n",
    "        if dilation > 1:\n",
    "            raise NotImplementedError(\"Dilation > 1 not supported in _ResnetBasicBlock\")\n",
    "        # Both self.conv1 and self.downsample layers downsample the input when stride != 1\n",
    "\n",
    "        self.stride = stride\n",
    "\n",
    "        self.conv1 = _resnet_conv3x1_wav1d(inplanes, planes, dilation=1)\n",
    "        self.bn1 = norm_layer(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = _resnet_conv3x1_wav1d(planes, planes, dilation=2)\n",
    "        self.bn2 = norm_layer(planes)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        init_layer(self.conv1)\n",
    "        init_bn(self.bn1)\n",
    "        init_layer(self.conv2)\n",
    "        init_bn(self.bn2)\n",
    "        nn.init.constant_(self.bn2.weight, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "\n",
    "        if self.stride != 1:\n",
    "            out = F.max_pool1d(x, kernel_size=self.stride)\n",
    "        else:\n",
    "            out = x\n",
    "\n",
    "        out = self.conv1(out)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = F.dropout(out, p=0.1, training=self.training)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        \n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(identity)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class _ResNetWav1d(nn.Module):\n",
    "    def __init__(self, block, layers, zero_init_residual=False,\n",
    "                 groups=1, width_per_group=64, replace_stride_with_dilation=None,\n",
    "                 norm_layer=None):\n",
    "        super(_ResNetWav1d, self).__init__()\n",
    "\n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.BatchNorm1d\n",
    "        self._norm_layer = norm_layer\n",
    "\n",
    "        self.inplanes = 64\n",
    "        self.dilation = 1\n",
    "        if replace_stride_with_dilation is None:\n",
    "            # each element in the tuple indicates if we should replace\n",
    "            # the 2x2 stride with a dilated convolution instead\n",
    "            replace_stride_with_dilation = [False, False, False]\n",
    "        if len(replace_stride_with_dilation) != 3:\n",
    "            raise ValueError(\"replace_stride_with_dilation should be None \"\n",
    "                             \"or a 3-element tuple, got {}\".format(replace_stride_with_dilation))\n",
    "        self.groups = groups\n",
    "        self.base_width = width_per_group\n",
    "\n",
    "        self.layer1 = self._make_layer(block, 64, layers[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1], stride=4)\n",
    "        self.layer3 = self._make_layer(block, 256, layers[2], stride=4)\n",
    "        self.layer4 = self._make_layer(block, 512, layers[3], stride=4)\n",
    "        self.layer5 = self._make_layer(block, 1024, layers[4], stride=4)\n",
    "        self.layer6 = self._make_layer(block, 1024, layers[5], stride=4)\n",
    "        self.layer7 = self._make_layer(block, 2048, layers[6], stride=4)\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, stride=1, dilate=False):\n",
    "        norm_layer = self._norm_layer\n",
    "        downsample = None\n",
    "        previous_dilation = self.dilation\n",
    "        if dilate:\n",
    "            self.dilation *= stride\n",
    "            stride = 1\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            if stride == 1:\n",
    "                downsample = nn.Sequential(\n",
    "                    _resnet_conv1x1_wav1d(self.inplanes, planes * block.expansion),\n",
    "                    norm_layer(planes * block.expansion),\n",
    "                )\n",
    "                init_layer(downsample[0])\n",
    "                init_bn(downsample[1])\n",
    "            else:\n",
    "                downsample = nn.Sequential(\n",
    "                    nn.AvgPool1d(kernel_size=stride), \n",
    "                    _resnet_conv1x1_wav1d(self.inplanes, planes * block.expansion),\n",
    "                    norm_layer(planes * block.expansion),\n",
    "                )\n",
    "                init_layer(downsample[1])\n",
    "                init_bn(downsample[2])\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, stride, downsample, self.groups,\n",
    "                            self.base_width, previous_dilation, norm_layer))\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes, groups=self.groups,\n",
    "                                base_width=self.base_width, dilation=self.dilation,\n",
    "                                norm_layer=norm_layer))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        x = self.layer5(x)\n",
    "        x = self.layer6(x)\n",
    "        x = self.layer7(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class Res1dNet31(nn.Module):\n",
    "    def __init__(self, sample_rate, window_size, hop_size, mel_bins, fmin, \n",
    "        fmax, classes_num):\n",
    "        \n",
    "        super(Res1dNet31, self).__init__()\n",
    "\n",
    "        window = 'hann'\n",
    "        center = True\n",
    "        pad_mode = 'reflect'\n",
    "        ref = 1.0\n",
    "        amin = 1e-10\n",
    "        top_db = None\n",
    "\n",
    "        self.conv0 = nn.Conv1d(in_channels=1, out_channels=64, kernel_size=11, stride=5, padding=5, bias=False)\n",
    "        self.bn0 = nn.BatchNorm1d(64)\n",
    "\n",
    "        self.resnet = _ResNetWav1d(_ResnetBasicBlockWav1d, [2, 2, 2, 2, 2, 2, 2])\n",
    "\n",
    "        self.fc1 = nn.Linear(2048, 2048, bias=True)\n",
    "        self.fc_audioset = nn.Linear(2048, classes_num, bias=True)\n",
    "         \n",
    "        self.init_weight()\n",
    "\n",
    "    def init_weight(self):\n",
    "        init_layer(self.conv0)\n",
    "        init_bn(self.bn0)\n",
    "        init_layer(self.fc1)\n",
    "        init_layer(self.fc_audioset)\n",
    " \n",
    "    def forward(self, input, mixup_lambda=None):\n",
    "        \"\"\"\n",
    "        Input: (batch_size, data_length)\"\"\"\n",
    "\n",
    "        x = input[:, None, :]\n",
    "\n",
    "        # Mixup on spectrogram\n",
    "        if self.training and mixup_lambda is not None:\n",
    "            x = do_mixup(x, mixup_lambda)\n",
    "\n",
    "        x = self.bn0(self.conv0(x))\n",
    "        x = self.resnet(x)\n",
    "\n",
    "        (x1, _) = torch.max(x, dim=2)\n",
    "        x2 = torch.mean(x, dim=2)\n",
    "        x = x1 + x2\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = F.relu_(self.fc1(x))\n",
    "        embedding = F.dropout(x, p=0.5, training=self.training)\n",
    "        clipwise_output = torch.sigmoid(self.fc_audioset(x))\n",
    "        \n",
    "        output_dict = {'clipwise_output': clipwise_output, 'embedding': embedding}\n",
    "\n",
    "        return output_dict\n",
    "\n",
    "\n",
    "class Res1dNet51(nn.Module):\n",
    "    def __init__(self, sample_rate, window_size, hop_size, mel_bins, fmin, \n",
    "        fmax, classes_num):\n",
    "        \n",
    "        super(Res1dNet51, self).__init__()\n",
    "\n",
    "        window = 'hann'\n",
    "        center = True\n",
    "        pad_mode = 'reflect'\n",
    "        ref = 1.0\n",
    "        amin = 1e-10\n",
    "        top_db = None\n",
    "\n",
    "        self.conv0 = nn.Conv1d(in_channels=1, out_channels=64, kernel_size=11, stride=5, padding=5, bias=False)\n",
    "        self.bn0 = nn.BatchNorm1d(64)\n",
    "\n",
    "        self.resnet = _ResNetWav1d(_ResnetBasicBlockWav1d, [2, 3, 4, 6, 4, 3, 2])\n",
    "\n",
    "        self.fc1 = nn.Linear(2048, 2048, bias=True)\n",
    "        self.fc_audioset = nn.Linear(2048, classes_num, bias=True)\n",
    "         \n",
    "        self.init_weight()\n",
    "\n",
    "    def init_weight(self):\n",
    "        init_layer(self.conv0)\n",
    "        init_bn(self.bn0)\n",
    "        init_layer(self.fc1)\n",
    "        init_layer(self.fc_audioset)\n",
    " \n",
    "    def forward(self, input, mixup_lambda=None):\n",
    "        \"\"\"\n",
    "        Input: (batch_size, data_length)\"\"\"\n",
    "\n",
    "        x = input[:, None, :]\n",
    "\n",
    "        # Mixup on spectrogram\n",
    "        if self.training and mixup_lambda is not None:\n",
    "            x = do_mixup(x, mixup_lambda)\n",
    "\n",
    "        x = self.bn0(self.conv0(x))\n",
    "        x = self.resnet(x)\n",
    "\n",
    "        (x1, _) = torch.max(x, dim=2)\n",
    "        x2 = torch.mean(x, dim=2)\n",
    "        x = x1 + x2\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = F.relu_(self.fc1(x))\n",
    "        embedding = F.dropout(x, p=0.5, training=self.training)\n",
    "        clipwise_output = torch.sigmoid(self.fc_audioset(x))\n",
    "        \n",
    "        output_dict = {'clipwise_output': clipwise_output, 'embedding': embedding}\n",
    "\n",
    "        return output_dict\n",
    "\n",
    "\n",
    "class ConvPreWavBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        \n",
    "        super(ConvPreWavBlock, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv1d(in_channels=in_channels, \n",
    "                              out_channels=out_channels,\n",
    "                              kernel_size=3, stride=1,\n",
    "                              padding=1, bias=False)\n",
    "                              \n",
    "        self.conv2 = nn.Conv1d(in_channels=out_channels, \n",
    "                              out_channels=out_channels,\n",
    "                              kernel_size=3, stride=1, dilation=2, \n",
    "                              padding=2, bias=False)\n",
    "                              \n",
    "        self.bn1 = nn.BatchNorm1d(out_channels)\n",
    "        self.bn2 = nn.BatchNorm1d(out_channels)\n",
    "\n",
    "        self.init_weight()\n",
    "        \n",
    "    def init_weight(self):\n",
    "        init_layer(self.conv1)\n",
    "        init_layer(self.conv2)\n",
    "        init_bn(self.bn1)\n",
    "        init_bn(self.bn2)\n",
    "\n",
    "        \n",
    "    def forward(self, input, pool_size):\n",
    "        \n",
    "        x = input\n",
    "        x = F.relu_(self.bn1(self.conv1(x)))\n",
    "        x = F.relu_(self.bn2(self.conv2(x)))\n",
    "        x = F.max_pool1d(x, kernel_size=pool_size)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "class Wavegram_Cnn14(nn.Module):\n",
    "    def __init__(self, sample_rate, window_size, hop_size, mel_bins, fmin, \n",
    "        fmax, classes_num):\n",
    "        \n",
    "        super(Wavegram_Cnn14, self).__init__()\n",
    "\n",
    "        window = 'hann'\n",
    "        center = True\n",
    "        pad_mode = 'reflect'\n",
    "        ref = 1.0\n",
    "        amin = 1e-10\n",
    "        top_db = None\n",
    "\n",
    "        self.pre_conv0 = nn.Conv1d(in_channels=1, out_channels=64, kernel_size=11, stride=5, padding=5, bias=False)\n",
    "        self.pre_bn0 = nn.BatchNorm1d(64)\n",
    "        self.pre_block1 = ConvPreWavBlock(64, 64)\n",
    "        self.pre_block2 = ConvPreWavBlock(64, 128)\n",
    "        self.pre_block3 = ConvPreWavBlock(128, 128)\n",
    "        self.pre_block4 = ConvBlock(in_channels=4, out_channels=64)\n",
    "\n",
    "        # Spec augmenter\n",
    "        self.spec_augmenter = SpecAugmentation(time_drop_width=64, time_stripes_num=2, \n",
    "            freq_drop_width=8, freq_stripes_num=2)\n",
    "\n",
    "        self.bn0 = nn.BatchNorm2d(64)\n",
    "\n",
    "        self.conv_block1 = ConvBlock(in_channels=1, out_channels=64)\n",
    "        self.conv_block2 = ConvBlock(in_channels=64, out_channels=128)\n",
    "        self.conv_block3 = ConvBlock(in_channels=128, out_channels=256)\n",
    "        self.conv_block4 = ConvBlock(in_channels=256, out_channels=512)\n",
    "        self.conv_block5 = ConvBlock(in_channels=512, out_channels=1024)\n",
    "        self.conv_block6 = ConvBlock(in_channels=1024, out_channels=2048)\n",
    "\n",
    "        self.fc1 = nn.Linear(2048, 2048, bias=True)\n",
    "        self.fc_audioset = nn.Linear(2048, classes_num, bias=True)\n",
    "        \n",
    "        self.init_weight()\n",
    "\n",
    "    def init_weight(self):\n",
    "        init_layer(self.pre_conv0)\n",
    "        init_bn(self.pre_bn0)\n",
    "        init_bn(self.bn0)\n",
    "        init_layer(self.fc1)\n",
    "        init_layer(self.fc_audioset)\n",
    " \n",
    "    def forward(self, input, mixup_lambda=None):\n",
    "        \"\"\"\n",
    "        Input: (batch_size, data_length)\"\"\"\n",
    "\n",
    "        # Wavegram\n",
    "        a1 = F.relu_(self.pre_bn0(self.pre_conv0(input[:, None, :])))\n",
    "        a1 = self.pre_block1(a1, pool_size=4)\n",
    "        a1 = self.pre_block2(a1, pool_size=4)\n",
    "        a1 = self.pre_block3(a1, pool_size=4)\n",
    "        a1 = a1.reshape((a1.shape[0], -1, 32, a1.shape[-1])).transpose(2, 3)\n",
    "        a1 = self.pre_block4(a1, pool_size=(2, 1))\n",
    "\n",
    "        # Mixup on spectrogram\n",
    "        if self.training and mixup_lambda is not None:\n",
    "            a1 = do_mixup(a1, mixup_lambda)\n",
    "        \n",
    "        x = a1\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = self.conv_block2(x, pool_size=(2, 2), pool_type='avg')\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = self.conv_block3(x, pool_size=(2, 2), pool_type='avg')\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = self.conv_block4(x, pool_size=(2, 2), pool_type='avg')\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = self.conv_block5(x, pool_size=(2, 2), pool_type='avg')\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = self.conv_block6(x, pool_size=(1, 1), pool_type='avg')\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = torch.mean(x, dim=3)\n",
    "        \n",
    "        (x1, _) = torch.max(x, dim=2)\n",
    "        x2 = torch.mean(x, dim=2)\n",
    "        x = x1 + x2\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = F.relu_(self.fc1(x))\n",
    "        embedding = F.dropout(x, p=0.5, training=self.training)\n",
    "        clipwise_output = torch.sigmoid(self.fc_audioset(x))\n",
    "        \n",
    "        output_dict = {'clipwise_output': clipwise_output, 'embedding': embedding}\n",
    "\n",
    "        return output_dict\n",
    "\n",
    "\n",
    "class Wavegram_Logmel_Cnn14(nn.Module):\n",
    "    def __init__(self, sample_rate, window_size, hop_size, mel_bins, fmin, \n",
    "        fmax, classes_num):\n",
    "        \n",
    "        super(Wavegram_Logmel_Cnn14, self).__init__()\n",
    "\n",
    "        window = 'hann'\n",
    "        center = True\n",
    "        pad_mode = 'reflect'\n",
    "        ref = 1.0\n",
    "        amin = 1e-10\n",
    "        top_db = None\n",
    "\n",
    "        self.pre_conv0 = nn.Conv1d(in_channels=1, out_channels=64, kernel_size=11, stride=5, padding=5, bias=False)\n",
    "        self.pre_bn0 = nn.BatchNorm1d(64)\n",
    "        self.pre_block1 = ConvPreWavBlock(64, 64)\n",
    "        self.pre_block2 = ConvPreWavBlock(64, 128)\n",
    "        self.pre_block3 = ConvPreWavBlock(128, 128)\n",
    "        self.pre_block4 = ConvBlock(in_channels=4, out_channels=64)\n",
    "\n",
    "        # Spectrogram extractor\n",
    "        self.spectrogram_extractor = Spectrogram(n_fft=window_size, hop_length=hop_size, \n",
    "            win_length=window_size, window=window, center=center, pad_mode=pad_mode, \n",
    "            freeze_parameters=True)\n",
    "\n",
    "        # Logmel feature extractor\n",
    "        self.logmel_extractor = LogmelFilterBank(sr=sample_rate, n_fft=window_size, \n",
    "            n_mels=mel_bins, fmin=fmin, fmax=fmax, ref=ref, amin=amin, top_db=top_db, \n",
    "            freeze_parameters=True)\n",
    "\n",
    "        # Spec augmenter\n",
    "        self.spec_augmenter = SpecAugmentation(time_drop_width=64, time_stripes_num=2, \n",
    "            freq_drop_width=8, freq_stripes_num=2)\n",
    "\n",
    "        self.bn0 = nn.BatchNorm2d(64)\n",
    "\n",
    "        self.conv_block1 = ConvBlock(in_channels=1, out_channels=64)\n",
    "        self.conv_block2 = ConvBlock(in_channels=128, out_channels=128)\n",
    "        self.conv_block3 = ConvBlock(in_channels=128, out_channels=256)\n",
    "        self.conv_block4 = ConvBlock(in_channels=256, out_channels=512)\n",
    "        self.conv_block5 = ConvBlock(in_channels=512, out_channels=1024)\n",
    "        self.conv_block6 = ConvBlock(in_channels=1024, out_channels=2048)\n",
    "\n",
    "        self.fc1 = nn.Linear(2048, 2048, bias=True)\n",
    "        self.fc_audioset = nn.Linear(2048, classes_num, bias=True)\n",
    "        \n",
    "        self.init_weight()\n",
    "\n",
    "    def init_weight(self):\n",
    "        init_layer(self.pre_conv0)\n",
    "        init_bn(self.pre_bn0)\n",
    "        init_bn(self.bn0)\n",
    "        init_layer(self.fc1)\n",
    "        init_layer(self.fc_audioset)\n",
    " \n",
    "    def forward(self, input, mixup_lambda=None):\n",
    "        \"\"\"\n",
    "        Input: (batch_size, data_length)\"\"\"\n",
    "\n",
    "        # Wavegram\n",
    "        a1 = F.relu_(self.pre_bn0(self.pre_conv0(input[:, None, :])))\n",
    "        a1 = self.pre_block1(a1, pool_size=4)\n",
    "        a1 = self.pre_block2(a1, pool_size=4)\n",
    "        a1 = self.pre_block3(a1, pool_size=4)\n",
    "        a1 = a1.reshape((a1.shape[0], -1, 32, a1.shape[-1])).transpose(2, 3)\n",
    "        a1 = self.pre_block4(a1, pool_size=(2, 1))\n",
    "\n",
    "        # Log mel spectrogram\n",
    "        x = self.spectrogram_extractor(input)   # (batch_size, 1, time_steps, freq_bins)\n",
    "        x = self.logmel_extractor(x)    # (batch_size, 1, time_steps, mel_bins)\n",
    "        \n",
    "        x = x.transpose(1, 3)\n",
    "        x = self.bn0(x)\n",
    "        x = x.transpose(1, 3)\n",
    "\n",
    "        if self.training:\n",
    "            x = self.spec_augmenter(x)\n",
    "\n",
    "        # Mixup on spectrogram\n",
    "        if self.training and mixup_lambda is not None:\n",
    "            x = do_mixup(x, mixup_lambda)\n",
    "            a1 = do_mixup(a1, mixup_lambda)\n",
    "        \n",
    "        x = self.conv_block1(x, pool_size=(2, 2), pool_type='avg')\n",
    "\n",
    "        # Concatenate Wavegram and Log mel spectrogram along the channel dimension\n",
    "        a1 = F.interpolate(a1, size=(x.size()[2], x.size()[3]), mode='nearest')\n",
    "        x = torch.cat((x, a1), dim=1)\n",
    "\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = self.conv_block2(x, pool_size=(2, 2), pool_type='avg')\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = self.conv_block3(x, pool_size=(2, 2), pool_type='avg')\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = self.conv_block4(x, pool_size=(2, 2), pool_type='avg')\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = self.conv_block5(x, pool_size=(2, 2), pool_type='avg')\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = self.conv_block6(x, pool_size=(1, 1), pool_type='avg')\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = torch.mean(x, dim=3)\n",
    "        \n",
    "        (x1, _) = torch.max(x, dim=2)\n",
    "        x2 = torch.mean(x, dim=2)\n",
    "        x = x1 + x2\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = F.relu_(self.fc1(x))\n",
    "        embedding = F.dropout(x, p=0.5, training=self.training)\n",
    "        clipwise_output = torch.sigmoid(self.fc_audioset(x))\n",
    "        \n",
    "        output_dict = {'clipwise_output': clipwise_output, 'embedding': embedding}\n",
    "\n",
    "        return output_dict\n",
    "\n",
    "\n",
    "class Wavegram_Logmel128_Cnn14(nn.Module):\n",
    "    def __init__(self, sample_rate, window_size, hop_size, mel_bins, fmin, \n",
    "        fmax, classes_num):\n",
    "        \n",
    "        super(Wavegram_Logmel128_Cnn14, self).__init__()\n",
    "\n",
    "        window = 'hann'\n",
    "        center = True\n",
    "        pad_mode = 'reflect'\n",
    "        ref = 1.0\n",
    "        amin = 1e-10\n",
    "        top_db = None\n",
    "\n",
    "        self.pre_conv0 = nn.Conv1d(in_channels=1, out_channels=64, kernel_size=11, stride=5, padding=5, bias=False)\n",
    "        self.pre_bn0 = nn.BatchNorm1d(64)\n",
    "        self.pre_block1 = ConvPreWavBlock(64, 64)\n",
    "        self.pre_block2 = ConvPreWavBlock(64, 128)\n",
    "        self.pre_block3 = ConvPreWavBlock(128, 256)\n",
    "        self.pre_block4 = ConvBlock(in_channels=4, out_channels=64)\n",
    "\n",
    "        # Spectrogram extractor\n",
    "        self.spectrogram_extractor = Spectrogram(n_fft=window_size, hop_length=hop_size, \n",
    "            win_length=window_size, window=window, center=center, pad_mode=pad_mode, \n",
    "            freeze_parameters=True)\n",
    "\n",
    "        # Logmel feature extractor\n",
    "        self.logmel_extractor = LogmelFilterBank(sr=sample_rate, n_fft=window_size, \n",
    "            n_mels=mel_bins, fmin=fmin, fmax=fmax, ref=ref, amin=amin, top_db=top_db, \n",
    "            freeze_parameters=True)\n",
    "\n",
    "        # Spec augmenter\n",
    "        self.spec_augmenter = SpecAugmentation(time_drop_width=64, time_stripes_num=2, \n",
    "            freq_drop_width=16, freq_stripes_num=2)\n",
    "\n",
    "        self.bn0 = nn.BatchNorm2d(128)\n",
    "\n",
    "        self.conv_block1 = ConvBlock(in_channels=1, out_channels=64)\n",
    "        self.conv_block2 = ConvBlock(in_channels=128, out_channels=128)\n",
    "        self.conv_block3 = ConvBlock(in_channels=128, out_channels=256)\n",
    "        self.conv_block4 = ConvBlock(in_channels=256, out_channels=512)\n",
    "        self.conv_block5 = ConvBlock(in_channels=512, out_channels=1024)\n",
    "        self.conv_block6 = ConvBlock(in_channels=1024, out_channels=2048)\n",
    "\n",
    "        self.fc1 = nn.Linear(2048, 2048, bias=True)\n",
    "        self.fc_audioset = nn.Linear(2048, classes_num, bias=True)\n",
    "        \n",
    "        self.init_weight()\n",
    "\n",
    "    def init_weight(self):\n",
    "        init_layer(self.pre_conv0)\n",
    "        init_bn(self.pre_bn0)\n",
    "        init_bn(self.bn0)\n",
    "        init_layer(self.fc1)\n",
    "        init_layer(self.fc_audioset)\n",
    " \n",
    "    def forward(self, input, mixup_lambda=None):\n",
    "        \"\"\"\n",
    "        Input: (batch_size, data_length)\"\"\"\n",
    "\n",
    "        # Wavegram\n",
    "        a1 = F.relu_(self.pre_bn0(self.pre_conv0(input[:, None, :])))\n",
    "        a1 = self.pre_block1(a1, pool_size=4)\n",
    "        a1 = self.pre_block2(a1, pool_size=4)\n",
    "        a1 = self.pre_block3(a1, pool_size=4)\n",
    "        a1 = a1.reshape((a1.shape[0], -1, 64, a1.shape[-1])).transpose(2, 3)\n",
    "        a1 = self.pre_block4(a1, pool_size=(2, 1))\n",
    "\n",
    "        # Log mel spectrogram\n",
    "        x = self.spectrogram_extractor(input)   # (batch_size, 1, time_steps, freq_bins)\n",
    "        x = self.logmel_extractor(x)    # (batch_size, 1, time_steps, mel_bins)\n",
    "        \n",
    "        x = x.transpose(1, 3)\n",
    "        x = self.bn0(x)\n",
    "        x = x.transpose(1, 3)\n",
    "\n",
    "        if self.training:\n",
    "            x = self.spec_augmenter(x)\n",
    "\n",
    "        # Mixup on spectrogram\n",
    "        if self.training and mixup_lambda is not None:\n",
    "            x = do_mixup(x, mixup_lambda)\n",
    "            a1 = do_mixup(a1, mixup_lambda)\n",
    "        \n",
    "        x = self.conv_block1(x, pool_size=(2, 2), pool_type='avg')\n",
    "\n",
    "        # Concatenate Wavegram and Log mel spectrogram along the channel dimension\n",
    "        a1 = F.interpolate(a1, size=(x.size()[2], x.size()[3]), mode='nearest')\n",
    "        x = torch.cat((x, a1), dim=1)\n",
    "        \n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = self.conv_block2(x, pool_size=(2, 2), pool_type='avg')\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = self.conv_block3(x, pool_size=(2, 2), pool_type='avg')\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = self.conv_block4(x, pool_size=(2, 2), pool_type='avg')\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = self.conv_block5(x, pool_size=(2, 2), pool_type='avg')\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = self.conv_block6(x, pool_size=(1, 1), pool_type='avg')\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = torch.mean(x, dim=3)\n",
    "        \n",
    "        (x1, _) = torch.max(x, dim=2)\n",
    "        x2 = torch.mean(x, dim=2)\n",
    "        x = x1 + x2\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = F.relu_(self.fc1(x))\n",
    "        embedding = F.dropout(x, p=0.5, training=self.training)\n",
    "        clipwise_output = torch.sigmoid(self.fc_audioset(x))\n",
    "        \n",
    "        output_dict = {'clipwise_output': clipwise_output, 'embedding': embedding}\n",
    "\n",
    "        return output_dict\n",
    "\n",
    "\n",
    "class Cnn14_16k(nn.Module):\n",
    "    def __init__(self, sample_rate, window_size, hop_size, mel_bins, fmin, fmax, classes_num):\n",
    "        \n",
    "        super(Cnn14_16k, self).__init__() \n",
    "\n",
    "        assert sample_rate == 16000\n",
    "        assert window_size == 512\n",
    "        assert hop_size == 160\n",
    "        assert mel_bins == 64\n",
    "        assert fmin == 50\n",
    "        assert fmax == 8000\n",
    "\n",
    "        window = 'hann'\n",
    "        center = True\n",
    "        pad_mode = 'reflect'\n",
    "        ref = 1.0\n",
    "        amin = 1e-10\n",
    "        top_db = None\n",
    "\n",
    "        # Spectrogram extractor\n",
    "        self.spectrogram_extractor = Spectrogram(n_fft=window_size, hop_length=hop_size, \n",
    "            win_length=window_size, window=window, center=center, pad_mode=pad_mode, \n",
    "            freeze_parameters=True)\n",
    "\n",
    "        # Logmel feature extractor\n",
    "        self.logmel_extractor = LogmelFilterBank(sr=sample_rate, n_fft=window_size, \n",
    "            n_mels=mel_bins, fmin=fmin, fmax=fmax, ref=ref, amin=amin, top_db=top_db, \n",
    "            freeze_parameters=True)\n",
    "\n",
    "        # Spec augmenter\n",
    "        self.spec_augmenter = SpecAugmentation(time_drop_width=64, time_stripes_num=2, \n",
    "            freq_drop_width=8, freq_stripes_num=2)\n",
    "\n",
    "        self.bn0 = nn.BatchNorm2d(64)\n",
    "\n",
    "        self.conv_block1 = ConvBlock(in_channels=1, out_channels=64)\n",
    "        self.conv_block2 = ConvBlock(in_channels=64, out_channels=128)\n",
    "        self.conv_block3 = ConvBlock(in_channels=128, out_channels=256)\n",
    "        self.conv_block4 = ConvBlock(in_channels=256, out_channels=512)\n",
    "        self.conv_block5 = ConvBlock(in_channels=512, out_channels=1024)\n",
    "        self.conv_block6 = ConvBlock(in_channels=1024, out_channels=2048)\n",
    "\n",
    "        self.fc1 = nn.Linear(2048, 2048, bias=True)\n",
    "        self.fc_audioset = nn.Linear(2048, classes_num, bias=True)\n",
    "        \n",
    "        self.init_weight()\n",
    "\n",
    "    def init_weight(self):\n",
    "        init_bn(self.bn0)\n",
    "        init_layer(self.fc1)\n",
    "        init_layer(self.fc_audioset)\n",
    " \n",
    "    def forward(self, input, mixup_lambda=None):\n",
    "        \"\"\"\n",
    "        Input: (batch_size, data_length)\"\"\"\n",
    "\n",
    "        x = self.spectrogram_extractor(input)   # (batch_size, 1, time_steps, freq_bins)\n",
    "        x = self.logmel_extractor(x)    # (batch_size, 1, time_steps, mel_bins)\n",
    "\n",
    "        x = x.transpose(1, 3)\n",
    "        x = self.bn0(x)\n",
    "        x = x.transpose(1, 3)\n",
    "        \n",
    "        if self.training:\n",
    "            x = self.spec_augmenter(x)\n",
    "\n",
    "        # Mixup on spectrogram\n",
    "        if self.training and mixup_lambda is not None:\n",
    "            x = do_mixup(x, mixup_lambda)\n",
    "        \n",
    "        x = self.conv_block1(x, pool_size=(2, 2), pool_type='avg')\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = self.conv_block2(x, pool_size=(2, 2), pool_type='avg')\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = self.conv_block3(x, pool_size=(2, 2), pool_type='avg')\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = self.conv_block4(x, pool_size=(2, 2), pool_type='avg')\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = self.conv_block5(x, pool_size=(2, 2), pool_type='avg')\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = self.conv_block6(x, pool_size=(1, 1), pool_type='avg')\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = torch.mean(x, dim=3)\n",
    "        \n",
    "        (x1, _) = torch.max(x, dim=2)\n",
    "        x2 = torch.mean(x, dim=2)\n",
    "        x = x1 + x2\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = F.relu_(self.fc1(x))\n",
    "        embedding = F.dropout(x, p=0.5, training=self.training)\n",
    "        clipwise_output = torch.sigmoid(self.fc_audioset(x))\n",
    "        \n",
    "        output_dict = {'clipwise_output': clipwise_output, 'embedding': embedding}\n",
    "\n",
    "        return output_dict\n",
    "\n",
    "\n",
    "class Cnn14_8k(nn.Module):\n",
    "    def __init__(self, sample_rate, window_size, hop_size, mel_bins, fmin, fmax, classes_num):\n",
    "        \n",
    "        super(Cnn14_8k, self).__init__() \n",
    "\n",
    "        assert sample_rate == 8000\n",
    "        assert window_size == 256\n",
    "        assert hop_size == 80\n",
    "        assert mel_bins == 64\n",
    "        assert fmin == 50\n",
    "        assert fmax == 4000\n",
    "\n",
    "        window = 'hann'\n",
    "        center = True\n",
    "        pad_mode = 'reflect'\n",
    "        ref = 1.0\n",
    "        amin = 1e-10\n",
    "        top_db = None\n",
    "\n",
    "        # Spectrogram extractor\n",
    "        self.spectrogram_extractor = Spectrogram(n_fft=window_size, hop_length=hop_size, \n",
    "            win_length=window_size, window=window, center=center, pad_mode=pad_mode, \n",
    "            freeze_parameters=True)\n",
    "\n",
    "        # Logmel feature extractor\n",
    "        self.logmel_extractor = LogmelFilterBank(sr=sample_rate, n_fft=window_size, \n",
    "            n_mels=mel_bins, fmin=fmin, fmax=fmax, ref=ref, amin=amin, top_db=top_db, \n",
    "            freeze_parameters=True)\n",
    "\n",
    "        # Spec augmenter\n",
    "        self.spec_augmenter = SpecAugmentation(time_drop_width=64, time_stripes_num=2, \n",
    "            freq_drop_width=8, freq_stripes_num=2)\n",
    "\n",
    "        self.bn0 = nn.BatchNorm2d(64)\n",
    "\n",
    "        self.conv_block1 = ConvBlock(in_channels=1, out_channels=64)\n",
    "        self.conv_block2 = ConvBlock(in_channels=64, out_channels=128)\n",
    "        self.conv_block3 = ConvBlock(in_channels=128, out_channels=256)\n",
    "        self.conv_block4 = ConvBlock(in_channels=256, out_channels=512)\n",
    "        self.conv_block5 = ConvBlock(in_channels=512, out_channels=1024)\n",
    "        self.conv_block6 = ConvBlock(in_channels=1024, out_channels=2048)\n",
    "\n",
    "        self.fc1 = nn.Linear(2048, 2048, bias=True)\n",
    "        self.fc_audioset = nn.Linear(2048, classes_num, bias=True)\n",
    "        \n",
    "        self.init_weight()\n",
    "\n",
    "    def init_weight(self):\n",
    "        init_bn(self.bn0)\n",
    "        init_layer(self.fc1)\n",
    "        init_layer(self.fc_audioset)\n",
    " \n",
    "    def forward(self, input, mixup_lambda=None):\n",
    "        \"\"\"\n",
    "        Input: (batch_size, data_length)\"\"\"\n",
    "\n",
    "        x = self.spectrogram_extractor(input)   # (batch_size, 1, time_steps, freq_bins)\n",
    "        x = self.logmel_extractor(x)    # (batch_size, 1, time_steps, mel_bins)\n",
    "\n",
    "        x = x.transpose(1, 3)\n",
    "        x = self.bn0(x)\n",
    "        x = x.transpose(1, 3)\n",
    "        \n",
    "        if self.training:\n",
    "            x = self.spec_augmenter(x)\n",
    "\n",
    "        # Mixup on spectrogram\n",
    "        if self.training and mixup_lambda is not None:\n",
    "            x = do_mixup(x, mixup_lambda)\n",
    "        \n",
    "        x = self.conv_block1(x, pool_size=(2, 2), pool_type='avg')\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = self.conv_block2(x, pool_size=(2, 2), pool_type='avg')\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = self.conv_block3(x, pool_size=(2, 2), pool_type='avg')\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = self.conv_block4(x, pool_size=(2, 2), pool_type='avg')\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = self.conv_block5(x, pool_size=(2, 2), pool_type='avg')\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = self.conv_block6(x, pool_size=(1, 1), pool_type='avg')\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = torch.mean(x, dim=3)\n",
    "        \n",
    "        (x1, _) = torch.max(x, dim=2)\n",
    "        x2 = torch.mean(x, dim=2)\n",
    "        x = x1 + x2\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = F.relu_(self.fc1(x))\n",
    "        embedding = F.dropout(x, p=0.5, training=self.training)\n",
    "        clipwise_output = torch.sigmoid(self.fc_audioset(x))\n",
    "        \n",
    "        output_dict = {'clipwise_output': clipwise_output, 'embedding': embedding}\n",
    "\n",
    "        return output_dict\n",
    "\n",
    "\n",
    "class Cnn14_mixup_time_domain(nn.Module):\n",
    "    def __init__(self, sample_rate, window_size, hop_size, mel_bins, fmin, \n",
    "        fmax, classes_num):\n",
    "        \n",
    "        super(Cnn14_mixup_time_domain, self).__init__()\n",
    "\n",
    "        window = 'hann'\n",
    "        center = True\n",
    "        pad_mode = 'reflect'\n",
    "        ref = 1.0\n",
    "        amin = 1e-10\n",
    "        top_db = None\n",
    "\n",
    "        # Spectrogram extractor\n",
    "        self.spectrogram_extractor = Spectrogram(n_fft=window_size, hop_length=hop_size, \n",
    "            win_length=window_size, window=window, center=center, pad_mode=pad_mode, \n",
    "            freeze_parameters=True)\n",
    "\n",
    "        # Logmel feature extractor\n",
    "        self.logmel_extractor = LogmelFilterBank(sr=sample_rate, n_fft=window_size, \n",
    "            n_mels=mel_bins, fmin=fmin, fmax=fmax, ref=ref, amin=amin, top_db=top_db, \n",
    "            freeze_parameters=True)\n",
    "\n",
    "        # Spec augmenter\n",
    "        self.spec_augmenter = SpecAugmentation(time_drop_width=64, time_stripes_num=2, \n",
    "            freq_drop_width=8, freq_stripes_num=2)\n",
    "\n",
    "        self.bn0 = nn.BatchNorm2d(64)\n",
    "\n",
    "        self.conv_block1 = ConvBlock(in_channels=1, out_channels=64)\n",
    "        self.conv_block2 = ConvBlock(in_channels=64, out_channels=128)\n",
    "        self.conv_block3 = ConvBlock(in_channels=128, out_channels=256)\n",
    "        self.conv_block4 = ConvBlock(in_channels=256, out_channels=512)\n",
    "        self.conv_block5 = ConvBlock(in_channels=512, out_channels=1024)\n",
    "        self.conv_block6 = ConvBlock(in_channels=1024, out_channels=2048)\n",
    "\n",
    "        self.fc1 = nn.Linear(2048, 2048, bias=True)\n",
    "        self.fc_audioset = nn.Linear(2048, classes_num, bias=True)\n",
    "        \n",
    "        self.init_weight()\n",
    "\n",
    "    def init_weight(self):\n",
    "        init_bn(self.bn0)\n",
    "        init_layer(self.fc1)\n",
    "        init_layer(self.fc_audioset)\n",
    " \n",
    "    def forward(self, input, mixup_lambda=None):\n",
    "        \"\"\"\n",
    "        Input: (batch_size, data_length)\"\"\"\n",
    "\n",
    "        x = input\n",
    "\n",
    "        # Mixup in time domain\n",
    "        if self.training and mixup_lambda is not None:\n",
    "            x = do_mixup(x, mixup_lambda)\n",
    "\n",
    "        x = self.spectrogram_extractor(x)   # (batch_size, 1, time_steps, freq_bins)\n",
    "        x = self.logmel_extractor(x)    # (batch_size, 1, time_steps, mel_bins)\n",
    "\n",
    "        x = x.transpose(1, 3)\n",
    "        x = self.bn0(x)\n",
    "        x = x.transpose(1, 3)\n",
    "\n",
    "        if self.training:\n",
    "            x = self.spec_augmenter(x)\n",
    "\n",
    "        x = self.conv_block1(x, pool_size=(2, 2), pool_type='avg')\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = self.conv_block2(x, pool_size=(2, 2), pool_type='avg')\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = self.conv_block3(x, pool_size=(2, 2), pool_type='avg')\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = self.conv_block4(x, pool_size=(2, 2), pool_type='avg')\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = self.conv_block5(x, pool_size=(2, 2), pool_type='avg')\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = self.conv_block6(x, pool_size=(1, 1), pool_type='avg')\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = torch.mean(x, dim=3)\n",
    "        \n",
    "        (x1, _) = torch.max(x, dim=2)\n",
    "        x2 = torch.mean(x, dim=2)\n",
    "        x = x1 + x2\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = F.relu_(self.fc1(x))\n",
    "        embedding = F.dropout(x, p=0.5, training=self.training)\n",
    "        clipwise_output = torch.sigmoid(self.fc_audioset(x))\n",
    "        \n",
    "        output_dict = {'clipwise_output': clipwise_output, 'embedding': embedding}\n",
    "\n",
    "        return output_dict\n",
    "\n",
    "\n",
    "class Cnn14_mel32(nn.Module):\n",
    "    def __init__(self, sample_rate, window_size, hop_size, mel_bins, fmin, \n",
    "        fmax, classes_num):\n",
    "        \n",
    "        super(Cnn14_mel32, self).__init__()\n",
    "\n",
    "        window = 'hann'\n",
    "        center = True\n",
    "        pad_mode = 'reflect'\n",
    "        ref = 1.0\n",
    "        amin = 1e-10\n",
    "        top_db = None\n",
    "\n",
    "        # Spectrogram extractor\n",
    "        self.spectrogram_extractor = Spectrogram(n_fft=window_size, hop_length=hop_size, \n",
    "            win_length=window_size, window=window, center=center, pad_mode=pad_mode, \n",
    "            freeze_parameters=True)\n",
    "\n",
    "        # Logmel feature extractor\n",
    "        self.logmel_extractor = LogmelFilterBank(sr=sample_rate, n_fft=window_size, \n",
    "            n_mels=mel_bins, fmin=fmin, fmax=fmax, ref=ref, amin=amin, top_db=top_db, \n",
    "            freeze_parameters=True)\n",
    "\n",
    "        # Spec augmenter\n",
    "        self.spec_augmenter = SpecAugmentation(time_drop_width=64, time_stripes_num=2, \n",
    "            freq_drop_width=4, freq_stripes_num=2)\n",
    "\n",
    "        self.bn0 = nn.BatchNorm2d(32)\n",
    "\n",
    "        self.conv_block1 = ConvBlock(in_channels=1, out_channels=64)\n",
    "        self.conv_block2 = ConvBlock(in_channels=64, out_channels=128)\n",
    "        self.conv_block3 = ConvBlock(in_channels=128, out_channels=256)\n",
    "        self.conv_block4 = ConvBlock(in_channels=256, out_channels=512)\n",
    "        self.conv_block5 = ConvBlock(in_channels=512, out_channels=1024)\n",
    "        self.conv_block6 = ConvBlock(in_channels=1024, out_channels=2048)\n",
    "\n",
    "        self.fc1 = nn.Linear(2048, 2048, bias=True)\n",
    "        self.fc_audioset = nn.Linear(2048, classes_num, bias=True)\n",
    "        \n",
    "        self.init_weight()\n",
    "\n",
    "    def init_weight(self):\n",
    "        init_bn(self.bn0)\n",
    "        init_layer(self.fc1)\n",
    "        init_layer(self.fc_audioset)\n",
    " \n",
    "    def forward(self, input, mixup_lambda=None):\n",
    "        \"\"\"\n",
    "        Input: (batch_size, data_length)\"\"\"\n",
    "\n",
    "        x = self.spectrogram_extractor(input)   # (batch_size, 1, time_steps, freq_bins)\n",
    "        x = self.logmel_extractor(x)    # (batch_size, 1, time_steps, mel_bins)\n",
    "        \n",
    "        x = x.transpose(1, 3)\n",
    "        x = self.bn0(x)\n",
    "        x = x.transpose(1, 3)\n",
    "        \n",
    "        if self.training:\n",
    "            x = self.spec_augmenter(x)\n",
    "\n",
    "        # Mixup on spectrogram\n",
    "        if self.training and mixup_lambda is not None:\n",
    "            x = do_mixup(x, mixup_lambda)\n",
    "\n",
    "        x = self.conv_block1(x, pool_size=(2, 2), pool_type='avg')\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = self.conv_block2(x, pool_size=(2, 2), pool_type='avg')\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = self.conv_block3(x, pool_size=(2, 2), pool_type='avg')\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = self.conv_block4(x, pool_size=(2, 2), pool_type='avg')\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = self.conv_block5(x, pool_size=(2, 2), pool_type='avg')\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = self.conv_block6(x, pool_size=(1, 1), pool_type='avg')\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = torch.mean(x, dim=3)\n",
    "        \n",
    "        (x1, _) = torch.max(x, dim=2)\n",
    "        x2 = torch.mean(x, dim=2)\n",
    "        x = x1 + x2\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = F.relu_(self.fc1(x))\n",
    "        embedding = F.dropout(x, p=0.5, training=self.training)\n",
    "        clipwise_output = torch.sigmoid(self.fc_audioset(x))\n",
    "        \n",
    "        output_dict = {'clipwise_output': clipwise_output, 'embedding': embedding}\n",
    "\n",
    "        return output_dict\n",
    "\n",
    "\n",
    "class Cnn14_mel128(nn.Module):\n",
    "    def __init__(self, sample_rate, window_size, hop_size, mel_bins, fmin, \n",
    "        fmax, classes_num):\n",
    "        \n",
    "        super(Cnn14_mel128, self).__init__()\n",
    "\n",
    "        window = 'hann'\n",
    "        center = True\n",
    "        pad_mode = 'reflect'\n",
    "        ref = 1.0\n",
    "        amin = 1e-10\n",
    "        top_db = None\n",
    "\n",
    "        # Spectrogram extractor\n",
    "        self.spectrogram_extractor = Spectrogram(n_fft=window_size, hop_length=hop_size, \n",
    "            win_length=window_size, window=window, center=center, pad_mode=pad_mode, \n",
    "            freeze_parameters=True)\n",
    "\n",
    "        # Logmel feature extractor\n",
    "        self.logmel_extractor = LogmelFilterBank(sr=sample_rate, n_fft=window_size, \n",
    "            n_mels=mel_bins, fmin=fmin, fmax=fmax, ref=ref, amin=amin, top_db=top_db, \n",
    "            freeze_parameters=True)\n",
    "\n",
    "        # Spec augmenter\n",
    "        self.spec_augmenter = SpecAugmentation(time_drop_width=64, time_stripes_num=2, \n",
    "            freq_drop_width=16, freq_stripes_num=2)\n",
    "\n",
    "        self.bn0 = nn.BatchNorm2d(128)\n",
    "\n",
    "        self.conv_block1 = ConvBlock(in_channels=1, out_channels=64)\n",
    "        self.conv_block2 = ConvBlock(in_channels=64, out_channels=128)\n",
    "        self.conv_block3 = ConvBlock(in_channels=128, out_channels=256)\n",
    "        self.conv_block4 = ConvBlock(in_channels=256, out_channels=512)\n",
    "        self.conv_block5 = ConvBlock(in_channels=512, out_channels=1024)\n",
    "        self.conv_block6 = ConvBlock(in_channels=1024, out_channels=2048)\n",
    "\n",
    "        self.fc1 = nn.Linear(2048, 2048, bias=True)\n",
    "        self.fc_audioset = nn.Linear(2048, classes_num, bias=True)\n",
    "        \n",
    "        self.init_weight()\n",
    "\n",
    "    def init_weight(self):\n",
    "        init_bn(self.bn0)\n",
    "        init_layer(self.fc1)\n",
    "        init_layer(self.fc_audioset)\n",
    " \n",
    "    def forward(self, input, mixup_lambda=None):\n",
    "        \"\"\"\n",
    "        Input: (batch_size, data_length)\"\"\"\n",
    "\n",
    "        x = self.spectrogram_extractor(input)   # (batch_size, 1, time_steps, freq_bins)\n",
    "        x = self.logmel_extractor(x)    # (batch_size, 1, time_steps, mel_bins)\n",
    "        \n",
    "        x = x.transpose(1, 3)\n",
    "        x = self.bn0(x)\n",
    "        x = x.transpose(1, 3)\n",
    "        \n",
    "        if self.training:\n",
    "            x = self.spec_augmenter(x)\n",
    "\n",
    "        # Mixup on spectrogram\n",
    "        if self.training and mixup_lambda is not None:\n",
    "            x = do_mixup(x, mixup_lambda)\n",
    "\n",
    "        x = self.conv_block1(x, pool_size=(2, 2), pool_type='avg')\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = self.conv_block2(x, pool_size=(2, 2), pool_type='avg')\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = self.conv_block3(x, pool_size=(2, 2), pool_type='avg')\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = self.conv_block4(x, pool_size=(2, 2), pool_type='avg')\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = self.conv_block5(x, pool_size=(2, 2), pool_type='avg')\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = self.conv_block6(x, pool_size=(1, 1), pool_type='avg')\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = torch.mean(x, dim=3)\n",
    "        \n",
    "        (x1, _) = torch.max(x, dim=2)\n",
    "        x2 = torch.mean(x, dim=2)\n",
    "        x = x1 + x2\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = F.relu_(self.fc1(x))\n",
    "        embedding = F.dropout(x, p=0.5, training=self.training)\n",
    "        clipwise_output = torch.sigmoid(self.fc_audioset(x))\n",
    "        \n",
    "        output_dict = {'clipwise_output': clipwise_output, 'embedding': embedding}\n",
    "\n",
    "        return output_dict\n",
    "\n",
    "\n",
    "############\n",
    "class Cnn14_DecisionLevelMax(nn.Module):\n",
    "    def __init__(self, sample_rate, window_size, hop_size, mel_bins, fmin, \n",
    "        fmax, classes_num):\n",
    "        \n",
    "        super(Cnn14_DecisionLevelMax, self).__init__()\n",
    "\n",
    "        window = 'hann'\n",
    "        center = True\n",
    "        pad_mode = 'reflect'\n",
    "        ref = 1.0\n",
    "        amin = 1e-10\n",
    "        top_db = None\n",
    "        self.interpolate_ratio = 32     # Downsampled ratio\n",
    "\n",
    "        # Spectrogram extractor\n",
    "        self.spectrogram_extractor = Spectrogram(n_fft=window_size, hop_length=hop_size, \n",
    "            win_length=window_size, window=window, center=center, pad_mode=pad_mode, \n",
    "            freeze_parameters=True)\n",
    "\n",
    "        # Logmel feature extractor\n",
    "        self.logmel_extractor = LogmelFilterBank(sr=sample_rate, n_fft=window_size, \n",
    "            n_mels=mel_bins, fmin=fmin, fmax=fmax, ref=ref, amin=amin, top_db=top_db, \n",
    "            freeze_parameters=True)\n",
    "\n",
    "        # Spec augmenter\n",
    "        self.spec_augmenter = SpecAugmentation(time_drop_width=64, time_stripes_num=2, \n",
    "            freq_drop_width=8, freq_stripes_num=2)\n",
    "\n",
    "        self.bn0 = nn.BatchNorm2d(64)\n",
    "\n",
    "        self.conv_block1 = ConvBlock(in_channels=1, out_channels=64)\n",
    "        self.conv_block2 = ConvBlock(in_channels=64, out_channels=128)\n",
    "        self.conv_block3 = ConvBlock(in_channels=128, out_channels=256)\n",
    "        self.conv_block4 = ConvBlock(in_channels=256, out_channels=512)\n",
    "        self.conv_block5 = ConvBlock(in_channels=512, out_channels=1024)\n",
    "        self.conv_block6 = ConvBlock(in_channels=1024, out_channels=2048)\n",
    "\n",
    "        self.fc1 = nn.Linear(2048, 2048, bias=True)\n",
    "        self.fc_audioset = nn.Linear(2048, classes_num, bias=True)\n",
    "        \n",
    "        self.init_weight()\n",
    "\n",
    "    def init_weight(self):\n",
    "        init_bn(self.bn0)\n",
    "        init_layer(self.fc1)\n",
    "        init_layer(self.fc_audioset)\n",
    " \n",
    "    def forward(self, input, mixup_lambda=None):\n",
    "        \"\"\"\n",
    "        Input: (batch_size, data_length)\"\"\"\n",
    "\n",
    "        x = self.spectrogram_extractor(input)   # (batch_size, 1, time_steps, freq_bins)\n",
    "        x = self.logmel_extractor(x)    # (batch_size, 1, time_steps, mel_bins)\n",
    "\n",
    "        frames_num = x.shape[2]\n",
    "        \n",
    "        x = x.transpose(1, 3)\n",
    "        x = self.bn0(x)\n",
    "        x = x.transpose(1, 3)\n",
    "        \n",
    "        if self.training:\n",
    "            x = self.spec_augmenter(x)\n",
    "\n",
    "        # Mixup on spectrogram\n",
    "        if self.training and mixup_lambda is not None:\n",
    "            x = do_mixup(x, mixup_lambda)\n",
    "\n",
    "        x = self.conv_block1(x, pool_size=(2, 2), pool_type='avg')\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = self.conv_block2(x, pool_size=(2, 2), pool_type='avg')\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = self.conv_block3(x, pool_size=(2, 2), pool_type='avg')\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = self.conv_block4(x, pool_size=(2, 2), pool_type='avg')\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = self.conv_block5(x, pool_size=(2, 2), pool_type='avg')\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = self.conv_block6(x, pool_size=(1, 1), pool_type='avg')\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = torch.mean(x, dim=3)\n",
    "        \n",
    "        x1 = F.max_pool1d(x, kernel_size=3, stride=1, padding=1)\n",
    "        x2 = F.avg_pool1d(x, kernel_size=3, stride=1, padding=1)\n",
    "        x = x1 + x2\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = x.transpose(1, 2)\n",
    "        x = F.relu_(self.fc1(x))\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        segmentwise_output = torch.sigmoid(self.fc_audioset(x))\n",
    "        (clipwise_output, _) = torch.max(segmentwise_output, dim=1)\n",
    "\n",
    "        # Get framewise output\n",
    "        framewise_output = interpolate(segmentwise_output, self.interpolate_ratio)\n",
    "        framewise_output = pad_framewise_output(framewise_output, frames_num)\n",
    "\n",
    "        output_dict = {'framewise_output': framewise_output, \n",
    "            'clipwise_output': clipwise_output}\n",
    "\n",
    "        return output_dict\n",
    "\n",
    "\n",
    "class Cnn14_DecisionLevelAvg(nn.Module):\n",
    "    def __init__(self, sample_rate, window_size, hop_size, mel_bins, fmin, \n",
    "        fmax, classes_num):\n",
    "        \n",
    "        super(Cnn14_DecisionLevelAvg, self).__init__()\n",
    "\n",
    "        window = 'hann'\n",
    "        center = True\n",
    "        pad_mode = 'reflect'\n",
    "        ref = 1.0\n",
    "        amin = 1e-10\n",
    "        top_db = None\n",
    "        self.interpolate_ratio = 32     # Downsampled ratio\n",
    "\n",
    "        # Spectrogram extractor\n",
    "        self.spectrogram_extractor = Spectrogram(n_fft=window_size, hop_length=hop_size, \n",
    "            win_length=window_size, window=window, center=center, pad_mode=pad_mode, \n",
    "            freeze_parameters=True)\n",
    "\n",
    "        # Logmel feature extractor\n",
    "        self.logmel_extractor = LogmelFilterBank(sr=sample_rate, n_fft=window_size, \n",
    "            n_mels=mel_bins, fmin=fmin, fmax=fmax, ref=ref, amin=amin, top_db=top_db, \n",
    "            freeze_parameters=True)\n",
    "\n",
    "        # Spec augmenter\n",
    "        self.spec_augmenter = SpecAugmentation(time_drop_width=64, time_stripes_num=2, \n",
    "            freq_drop_width=8, freq_stripes_num=2)\n",
    "\n",
    "        self.bn0 = nn.BatchNorm2d(64)\n",
    "\n",
    "        self.conv_block1 = ConvBlock(in_channels=1, out_channels=64)\n",
    "        self.conv_block2 = ConvBlock(in_channels=64, out_channels=128)\n",
    "        self.conv_block3 = ConvBlock(in_channels=128, out_channels=256)\n",
    "        self.conv_block4 = ConvBlock(in_channels=256, out_channels=512)\n",
    "        self.conv_block5 = ConvBlock(in_channels=512, out_channels=1024)\n",
    "        self.conv_block6 = ConvBlock(in_channels=1024, out_channels=2048)\n",
    "\n",
    "        self.fc1 = nn.Linear(2048, 2048, bias=True)\n",
    "        self.fc_audioset = nn.Linear(2048, classes_num, bias=True)\n",
    "        \n",
    "        self.init_weight()\n",
    "\n",
    "    def init_weight(self):\n",
    "        init_bn(self.bn0)\n",
    "        init_layer(self.fc1)\n",
    "        init_layer(self.fc_audioset)\n",
    " \n",
    "    def forward(self, input, mixup_lambda=None):\n",
    "        \"\"\"\n",
    "        Input: (batch_size, data_length)\"\"\"\n",
    "\n",
    "        x = self.spectrogram_extractor(input)   # (batch_size, 1, time_steps, freq_bins)\n",
    "        x = self.logmel_extractor(x)    # (batch_size, 1, time_steps, mel_bins)\n",
    "\n",
    "        frames_num = x.shape[2]\n",
    "        \n",
    "        x = x.transpose(1, 3)\n",
    "        x = self.bn0(x)\n",
    "        x = x.transpose(1, 3)\n",
    "        \n",
    "        if self.training:\n",
    "            x = self.spec_augmenter(x)\n",
    "\n",
    "        # Mixup on spectrogram\n",
    "        if self.training and mixup_lambda is not None:\n",
    "            x = do_mixup(x, mixup_lambda)\n",
    "        \n",
    "        x = self.conv_block1(x, pool_size=(2, 2), pool_type='avg')\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = self.conv_block2(x, pool_size=(2, 2), pool_type='avg')\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = self.conv_block3(x, pool_size=(2, 2), pool_type='avg')\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = self.conv_block4(x, pool_size=(2, 2), pool_type='avg')\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = self.conv_block5(x, pool_size=(2, 2), pool_type='avg')\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = self.conv_block6(x, pool_size=(1, 1), pool_type='avg')\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = torch.mean(x, dim=3)\n",
    "        \n",
    "        x1 = F.max_pool1d(x, kernel_size=3, stride=1, padding=1)\n",
    "        x2 = F.avg_pool1d(x, kernel_size=3, stride=1, padding=1)\n",
    "        x = x1 + x2\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = x.transpose(1, 2)\n",
    "        x = F.relu_(self.fc1(x))\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        segmentwise_output = torch.sigmoid(self.fc_audioset(x))\n",
    "        clipwise_output = torch.mean(segmentwise_output, dim=1)\n",
    "\n",
    "        # Get framewise output\n",
    "        framewise_output = interpolate(segmentwise_output, self.interpolate_ratio)\n",
    "        framewise_output = pad_framewise_output(framewise_output, frames_num)\n",
    "\n",
    "        # Get framewise output\n",
    "        framewise_output = interpolate(segmentwise_output, self.interpolate_ratio)\n",
    "        framewise_output = pad_framewise_output(framewise_output, frames_num)\n",
    "\n",
    "        output_dict = {'framewise_output': framewise_output, \n",
    "            'clipwise_output': clipwise_output}\n",
    "\n",
    "        return output_dict\n",
    "\n",
    "\n",
    "class Cnn14_DecisionLevelAtt(nn.Module):\n",
    "    def __init__(self, sample_rate, window_size, hop_size, mel_bins, fmin, \n",
    "        fmax, classes_num):\n",
    "        \n",
    "        super(Cnn14_DecisionLevelAtt, self).__init__()\n",
    "\n",
    "        window = 'hann'\n",
    "        center = True\n",
    "        pad_mode = 'reflect'\n",
    "        ref = 1.0\n",
    "        amin = 1e-10\n",
    "        top_db = None\n",
    "        self.interpolate_ratio = 32     # Downsampled ratio\n",
    "\n",
    "        # Spectrogram extractor\n",
    "        self.spectrogram_extractor = Spectrogram(n_fft=window_size, hop_length=hop_size, \n",
    "            win_length=window_size, window=window, center=center, pad_mode=pad_mode, \n",
    "            freeze_parameters=True)\n",
    "\n",
    "        # Logmel feature extractor\n",
    "        self.logmel_extractor = LogmelFilterBank(sr=sample_rate, n_fft=window_size, \n",
    "            n_mels=mel_bins, fmin=fmin, fmax=fmax, ref=ref, amin=amin, top_db=top_db, \n",
    "            freeze_parameters=True)\n",
    "\n",
    "        # Spec augmenter\n",
    "        self.spec_augmenter = SpecAugmentation(time_drop_width=64, time_stripes_num=2, \n",
    "            freq_drop_width=8, freq_stripes_num=2)\n",
    "\n",
    "        self.bn0 = nn.BatchNorm2d(64)\n",
    "\n",
    "        self.conv_block1 = ConvBlock(in_channels=1, out_channels=64)\n",
    "        self.conv_block2 = ConvBlock(in_channels=64, out_channels=128)\n",
    "        self.conv_block3 = ConvBlock(in_channels=128, out_channels=256)\n",
    "        self.conv_block4 = ConvBlock(in_channels=256, out_channels=512)\n",
    "        self.conv_block5 = ConvBlock(in_channels=512, out_channels=1024)\n",
    "        self.conv_block6 = ConvBlock(in_channels=1024, out_channels=2048)\n",
    "\n",
    "        self.fc1 = nn.Linear(2048, 2048, bias=True)\n",
    "        self.att_block = AttBlock(2048, classes_num, activation='sigmoid')\n",
    "        \n",
    "        self.init_weight()\n",
    "\n",
    "    def init_weight(self):\n",
    "        init_bn(self.bn0)\n",
    "        init_layer(self.fc1)\n",
    " \n",
    "    def forward(self, input, mixup_lambda=None):\n",
    "        \"\"\"\n",
    "        Input: (batch_size, data_length)\"\"\"\n",
    "\n",
    "        x = self.spectrogram_extractor(input)   # (batch_size, 1, time_steps, freq_bins)\n",
    "        x = self.logmel_extractor(x)    # (batch_size, 1, time_steps, mel_bins)\n",
    "\n",
    "        frames_num = x.shape[2]\n",
    "        \n",
    "        x = x.transpose(1, 3)\n",
    "        x = self.bn0(x)\n",
    "        x = x.transpose(1, 3)\n",
    "        \n",
    "        if self.training:\n",
    "            x = self.spec_augmenter(x)\n",
    "\n",
    "        # Mixup on spectrogram\n",
    "        if self.training and mixup_lambda is not None:\n",
    "            x = do_mixup(x, mixup_lambda)\n",
    "        \n",
    "        x = self.conv_block1(x, pool_size=(2, 2), pool_type='avg')\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = self.conv_block2(x, pool_size=(2, 2), pool_type='avg')\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = self.conv_block3(x, pool_size=(2, 2), pool_type='avg')\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = self.conv_block4(x, pool_size=(2, 2), pool_type='avg')\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = self.conv_block5(x, pool_size=(2, 2), pool_type='avg')\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = self.conv_block6(x, pool_size=(1, 1), pool_type='avg')\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = torch.mean(x, dim=3)\n",
    "        \n",
    "        x1 = F.max_pool1d(x, kernel_size=3, stride=1, padding=1)\n",
    "        x2 = F.avg_pool1d(x, kernel_size=3, stride=1, padding=1)\n",
    "        x = x1 + x2\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = x.transpose(1, 2)\n",
    "        x = F.relu_(self.fc1(x))\n",
    "        x = x.transpose(1, 2)\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        (clipwise_output, _, segmentwise_output) = self.att_block(x)\n",
    "        segmentwise_output = segmentwise_output.transpose(1, 2)\n",
    "\n",
    "        # Get framewise output\n",
    "        framewise_output = interpolate(segmentwise_output, self.interpolate_ratio)\n",
    "        framewise_output = pad_framewise_output(framewise_output, frames_num)\n",
    "\n",
    "        output_dict = {'framewise_output': framewise_output, \n",
    "            'clipwise_output': clipwise_output}\n",
    "\n",
    "        return output_dict\n",
    "\n",
    "class Transfer_Cnn14(nn.Module):\n",
    "    def __init__(self, sample_rate, window_size, hop_size, mel_bins, fmin, \n",
    "        fmax, classes_num, freeze_base):\n",
    "        \"\"\"Classifier for a new task using pretrained Cnn14 as a sub module.\n",
    "        \"\"\"\n",
    "        super(Transfer_Cnn14, self).__init__()\n",
    "        audioset_classes_num = 527\n",
    "        \n",
    "        self.base = Cnn14(sample_rate, window_size, hop_size, mel_bins, fmin, \n",
    "            fmax, audioset_classes_num)\n",
    "\n",
    "        # Transfer to another task layer\n",
    "        self.fc_transfer = nn.Linear(2048, classes_num, bias=True)\n",
    "\n",
    "        if freeze_base:\n",
    "            # Freeze AudioSet pretrained layers\n",
    "            for param in self.base.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        init_layer(self.fc_transfer)\n",
    "\n",
    "    def load_from_pretrain(self, pretrained_checkpoint_path):\n",
    "        checkpoint = torch.load(pretrained_checkpoint_path)\n",
    "        self.base.load_state_dict(checkpoint['model'])\n",
    "\n",
    "    def forward(self, input, mixup_lambda=None):\n",
    "        \"\"\"Input: (batch_size, data_length)\n",
    "        \"\"\"\n",
    "        output_dict = self.base(input, mixup_lambda)\n",
    "        embedding = output_dict['embedding']\n",
    "\n",
    "        # if nll loss\n",
    "        # clipwise_output =  torch.log_softmax(self.fc_transfer(embedding), dim=-1)\n",
    "        clipwise_output =  torch.sigmoid(self.fc_transfer(embedding))\n",
    "        output_dict['clipwise_output'] = clipwise_output\n",
    " \n",
    "        return output_dict\n",
    "\n",
    "class Transfer_Cnn14_16k(nn.Module):\n",
    "    def __init__(self, sample_rate, window_size, hop_size, mel_bins, fmin, \n",
    "        fmax, classes_num, freeze_base):\n",
    "        \"\"\"Classifier for a new task using pretrained Cnn14 as a sub module.\n",
    "        \"\"\"\n",
    "        super(Transfer_Cnn14_16k, self).__init__()\n",
    "        audioset_classes_num = 527\n",
    "        \n",
    "        self.base = Cnn14_16k(sample_rate, window_size, hop_size, mel_bins, fmin, \n",
    "            fmax, audioset_classes_num)\n",
    "\n",
    "        # Transfer to another task layer\n",
    "        self.fc_transfer = nn.Linear(2048, classes_num, bias=True)\n",
    "\n",
    "        if freeze_base:\n",
    "            # Freeze AudioSet pretrained layers\n",
    "            for param in self.base.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        init_layer(self.fc_transfer)\n",
    "\n",
    "    def load_from_pretrain(self, pretrained_checkpoint_path):\n",
    "        checkpoint = torch.load(pretrained_checkpoint_path)\n",
    "        self.base.load_state_dict(checkpoint['model'])\n",
    "\n",
    "    def forward(self, input, mixup_lambda=None):\n",
    "        \"\"\"Input: (batch_size, data_length)\n",
    "        \"\"\"\n",
    "        output_dict = self.base(input, mixup_lambda)\n",
    "        embedding = output_dict['embedding']\n",
    "\n",
    "        # if nll loss\n",
    "        # clipwise_output =  torch.log_softmax(self.fc_transfer(embedding), dim=-1)\n",
    "        clipwise_output =  torch.sigmoid(self.fc_transfer(embedding))\n",
    "        output_dict['clipwise_output'] = clipwise_output\n",
    " \n",
    "        return output_dict\n",
    "\n",
    "class Transfer_Wavegram_Logmel_Cnn14(nn.Module):\n",
    "    def __init__(self, sample_rate, window_size, hop_size, mel_bins, fmin, \n",
    "        fmax, classes_num, freeze_base):\n",
    "        \"\"\"Classifier for a new task using pretrained Wavegram_Logmel_Cnn14 as a sub module.\n",
    "        \"\"\"\n",
    "        super(Transfer_Wavegram_Logmel_Cnn14, self).__init__()\n",
    "        audioset_classes_num = 527\n",
    "        \n",
    "        self.base = Wavegram_Logmel_Cnn14(sample_rate, window_size, hop_size, mel_bins, fmin, \n",
    "            fmax, audioset_classes_num)\n",
    "\n",
    "        # Transfer to another task layer\n",
    "        self.fc_transfer = nn.Linear(2048, classes_num, bias=True)\n",
    "\n",
    "        if freeze_base:\n",
    "            # Freeze AudioSet pretrained layers\n",
    "            for param in self.base.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        init_layer(self.fc_transfer)\n",
    "\n",
    "    def load_from_pretrain(self, pretrained_checkpoint_path):\n",
    "        checkpoint = torch.load(pretrained_checkpoint_path)\n",
    "        self.base.load_state_dict(checkpoint['model'])\n",
    "\n",
    "    def forward(self, input, mixup_lambda=None):\n",
    "        \"\"\"Input: (batch_size, data_length)\n",
    "        \"\"\"\n",
    "        output_dict = self.base(input, mixup_lambda)\n",
    "        embedding = output_dict['embedding']\n",
    "\n",
    "        # if nll loss\n",
    "        # clipwise_output =  torch.log_softmax(self.fc_transfer(embedding), dim=-1)\n",
    "        clipwise_output =  torch.sigmoid(self.fc_transfer(embedding))\n",
    "        output_dict['clipwise_output'] = clipwise_output\n",
    " \n",
    "        return output_dict\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "ea4d6ae3-a073-47ea-9be5-ed770da8e3b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import argparse\n",
    "import time\n",
    "import logging\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb98955-a224-404b-8fbb-9fa95e8e41df",
   "metadata": {},
   "source": [
    "# Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "1848f077-606f-4904-a441-9ff2bdf59a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import librosa\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import pickle\n",
    "import numpy as np\n",
    "import csv\n",
    "import h5py\n",
    "\n",
    "\n",
    "def create_folder(fd):\n",
    "    if not os.path.exists(fd):\n",
    "        os.makedirs(fd)\n",
    "\n",
    "\n",
    "def get_filename(path):\n",
    "    path = os.path.realpath(path)\n",
    "    name_ext = path.split('/')[-1]\n",
    "    name = os.path.splitext(name_ext)[0]\n",
    "    return name\n",
    "\n",
    "\n",
    "def traverse_folder(fd):\n",
    "    paths = []\n",
    "    names = []\n",
    "\n",
    "    for root, dirs, files in os.walk(fd):\n",
    "        for name in files:\n",
    "            filepath = os.path.join(root, name)\n",
    "            names.append(name)\n",
    "            paths.append(filepath)\n",
    "\n",
    "    return names, paths\n",
    "\n",
    "\n",
    "def create_logging(log_dir, filemode):\n",
    "    create_folder(log_dir)\n",
    "    i1 = 0\n",
    "\n",
    "    while os.path.isfile(os.path.join(log_dir, '{:04d}.log'.format(i1))):\n",
    "        i1 += 1\n",
    "        \n",
    "    log_path = os.path.join(log_dir, '{:04d}.log'.format(i1))\n",
    "    logging.basicConfig(\n",
    "        level=logging.DEBUG,\n",
    "        format='%(asctime)s %(filename)s[line:%(lineno)d] %(levelname)s %(message)s',\n",
    "        datefmt='%a, %d %b %Y %H:%M:%S',\n",
    "        filename=log_path,\n",
    "        filemode=filemode)\n",
    "\n",
    "    # Print to console\n",
    "    console = logging.StreamHandler()\n",
    "    console.setLevel(logging.INFO)\n",
    "    formatter = logging.Formatter('%(name)-12s: %(levelname)-8s %(message)s')\n",
    "    console.setFormatter(formatter)\n",
    "    logging.getLogger('').addHandler(console)\n",
    "    \n",
    "    return logging\n",
    "\n",
    "\n",
    "def get_metadata(audio_names, audio_paths):\n",
    "    meta_dict = {\n",
    "        'audio_name': audio_names, \n",
    "        'audio_path': audio_paths, \n",
    "        'fold': np.arange(len(audio_names))}\n",
    "\n",
    "    return meta_dict\n",
    "\n",
    "\n",
    "def float32_to_int16(x):\n",
    "    # assert np.max(np.abs(x)) <= 1.\n",
    "    if np.max(np.abs(x)) > 1.:\n",
    "        x /= np.max(np.abs(x))\n",
    "    return (x * 32767.).astype(np.int16)\n",
    "\n",
    "def int16_to_float32(x):\n",
    "    return (x / 32767.).astype(np.float32)\n",
    "\n",
    "\n",
    "class StatisticsContainer(object):\n",
    "    def __init__(self, statistics_path):\n",
    "        \"\"\"Contain statistics of different training iterations.\n",
    "        \"\"\"\n",
    "        self.statistics_path = statistics_path\n",
    "\n",
    "        self.backup_statistics_path = '{}_{}.pkl'.format(\n",
    "            os.path.splitext(self.statistics_path)[0], \n",
    "            datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S'))\n",
    "\n",
    "        self.statistics_dict = {'validate': []}\n",
    "\n",
    "    def append(self, iteration, statistics, data_type):\n",
    "        statistics['iteration'] = iteration\n",
    "        self.statistics_dict[data_type].append(statistics)\n",
    "        \n",
    "    def dump(self):\n",
    "        pickle.dump(self.statistics_dict, open(self.statistics_path, 'wb'))\n",
    "        pickle.dump(self.statistics_dict, open(self.backup_statistics_path, 'wb'))\n",
    "        logging.info('    Dump statistics to {}'.format(self.statistics_path))\n",
    "        logging.info('    Dump statistics to {}'.format(self.backup_statistics_path))\n",
    "        \n",
    "    def load_state_dict(self, resume_iteration):\n",
    "        self.statistics_dict = pickle.load(open(self.statistics_path, 'rb'))\n",
    "\n",
    "        resume_statistics_dict = {'validate': []}\n",
    "        \n",
    "        for key in self.statistics_dict.keys():\n",
    "            for statistics in self.statistics_dict[key]:\n",
    "                if statistics['iteration'] <= resume_iteration:\n",
    "                    resume_statistics_dict[key].append(statistics)\n",
    "                \n",
    "        self.statistics_dict = resume_statistics_dict\n",
    "\n",
    "\n",
    "class Mixup(object):\n",
    "    def __init__(self, mixup_alpha, random_seed=1234):\n",
    "        \"\"\"Mixup coefficient generator.\n",
    "        \"\"\"\n",
    "        self.mixup_alpha = mixup_alpha\n",
    "        self.random_state = np.random.RandomState(random_seed)\n",
    "\n",
    "    def get_lambda(self, batch_size):\n",
    "        \"\"\"Get mixup random coefficients.\n",
    "        Args:\n",
    "          batch_size: int\n",
    "        Returns:\n",
    "          mixup_lambdas: (batch_size,)\n",
    "        \"\"\"\n",
    "        mixup_lambdas = []\n",
    "        for n in range(0, batch_size, 2):\n",
    "            lam = self.random_state.beta(self.mixup_alpha, self.mixup_alpha, 1)[0]\n",
    "            mixup_lambdas.append(lam)\n",
    "            mixup_lambdas.append(1. - lam)\n",
    "\n",
    "        return np.array(mixup_lambdas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e6b73d-d956-466b-8a3f-cb8b6074b85d",
   "metadata": {},
   "source": [
    "# Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "f4399fdc-13c4-4f1d-bf0e-5ee0b11e17a5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import logging\n",
    "from sklearn import metrics\n",
    "\n",
    "# from pytorch_utils import forward\n",
    "# from utilities import get_filename\n",
    "# from config import idx_to_lb\n",
    "\n",
    "def probability_to_binary(y_pred):\n",
    "    max_index = np.argmax(y_pred, axis=1)\n",
    "\n",
    "    # Create a new array of zeros with the same shape as y_pred\n",
    "    binary_y_pred = np.zeros_like(y_pred)\n",
    "\n",
    "    # Set the maximum value in each row to 1\n",
    "    # We use fancy indexing to set the values to 1\n",
    "    binary_y_pred[np.arange(len(y_pred)), max_index] = 1\n",
    "    return binary_y_pred\n",
    "\n",
    "class Evaluator(object):\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "\n",
    "    def evaluate(self, data_loader):\n",
    "\n",
    "        # Forward\n",
    "        output_dict = forward(\n",
    "            model=self.model, \n",
    "            generator=data_loader, \n",
    "            return_target=True)\n",
    "\n",
    "        clipwise_output = output_dict['clipwise_output']    # (audios_num, classes_num)\n",
    "        target = output_dict['target']    # (audios_num, classes_num)\n",
    "\n",
    "        statistics = {}\n",
    "\n",
    "        cm = metrics.multilabel_confusion_matrix(target, probability_to_binary(clipwise_output))\n",
    "        accuracy = np.array([])\n",
    "        balanced_accuracy = np.array([])\n",
    "        precision = np.array([])\n",
    "        recall = np.array([])\n",
    "        f1 = np.array([])\n",
    "        jaccard = np.array([])\n",
    "        for idx, current_cm in enumerate(cm):\n",
    "            tn, fp, fn, tp = current_cm.ravel()\n",
    "            try:\n",
    "                current_accuracy = (tp + tn) / (tn + fp + fn + tp)\n",
    "            except:\n",
    "                current_accuracy = 0\n",
    "            try:\n",
    "                current_balanced_accuracy = ((tp / (tp + fn)) + ( tn / (tn + fp))) / 2\n",
    "            except:\n",
    "                current_balanced_accuracy = 0\n",
    "            try:\n",
    "                current_precision = tp / (tp + fp)\n",
    "            except:\n",
    "                current_precision = 0\n",
    "            try:\n",
    "                current_recall = tp / (tp + fn)\n",
    "            except:\n",
    "                current_recall = 0\n",
    "            try:\n",
    "                current_f1 = (1 + (1 ** 2) ) * (tp / (tp + fp)) * (tp / (tp + fn)) / (((1 ** 2) * (tp / (tp + fp))) + (tp / (tp + fn)))\n",
    "            except:\n",
    "                current_f1 = 0\n",
    "            try:\n",
    "                current_jaccard = tp / (tp + fp + fn)\n",
    "            except:\n",
    "                current_jaccard = 0\n",
    "                \n",
    "            statistics.update({\n",
    "                'cm_'+idx_to_lb[idx]: current_cm,\n",
    "                'tn_'+idx_to_lb[idx]: tn,\n",
    "                'fp_'+idx_to_lb[idx]: fp,\n",
    "                'fn_'+idx_to_lb[idx]: fn,\n",
    "                'tp_'+idx_to_lb[idx]: tp,\n",
    "                'accuracy_'+idx_to_lb[idx]: current_accuracy,\n",
    "                'balanced_accuracy_'+idx_to_lb[idx]: current_balanced_accuracy,\n",
    "                'precision_'+idx_to_lb[idx]: current_precision,\n",
    "                'recall_'+idx_to_lb[idx]: current_recall,\n",
    "                'f1_'+idx_to_lb[idx]: current_f1,\n",
    "                'jaccard_'+idx_to_lb[idx]: current_jaccard,\n",
    "            })\n",
    "            accuracy = np.append(accuracy,current_accuracy)\n",
    "            balanced_accuracy = np.append(balanced_accuracy,current_balanced_accuracy)\n",
    "            precision = np.append(precision,current_precision)\n",
    "            recall = np.append(recall,current_recall)\n",
    "            f1 = np.append(f1,current_f1)\n",
    "            jaccard = np.append(jaccard,current_jaccard)\n",
    "\n",
    "        statistics.update({\n",
    "            'accuracy': np.mean(accuracy),\n",
    "            'balanced_accuracy': np.mean(balanced_accuracy),\n",
    "            'precision': np.mean(precision),\n",
    "            'recall': np.mean(recall),\n",
    "            'f1': np.mean(f1),\n",
    "            'jaccard': np.mean(jaccard),\n",
    "        })\n",
    "\n",
    "        return statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26270d98-11e1-4836-bfa7-c47f461f033c",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "65d25752-5e63-4b0f-82a1-8aeb527cc6d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import argparse\n",
    "import h5py\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import csv\n",
    "import math\n",
    "import re\n",
    "import random\n",
    "\n",
    "# import config\n",
    "# from utilities import create_folder, traverse_folder, float32_to_int16\n",
    "\n",
    "\n",
    "def to_one_hot(k, classes_num):\n",
    "    target = np.zeros(classes_num)\n",
    "    target[k] = 1\n",
    "    return target\n",
    "\n",
    "def get_middle_part(arr, x):\n",
    "    # Convert the input list to a NumPy array\n",
    "    arr = np.array(arr)\n",
    "    \n",
    "    # Calculate the start index of the middle part\n",
    "    start_index = (len(arr) - x) // 2\n",
    "    \n",
    "    # Calculate the end index of the middle part\n",
    "    end_index = start_index + x\n",
    "    \n",
    "    # Extract the middle part from the array\n",
    "    middle_part = arr[start_index:end_index]\n",
    "    \n",
    "    return middle_part\n",
    "    \n",
    "def pad_truncate_sequence(x, max_len, center=False):\n",
    "    if len(x) < max_len:\n",
    "        return np.concatenate((x, np.zeros(max_len - len(x))))\n",
    "    elif len(x) > max_len and center:\n",
    "        return get_middle_part(x, max_len)\n",
    "    else:\n",
    "        return x[0 : max_len]\n",
    "\n",
    "\n",
    "def pack_audio_files_to_hdf5(args, config):\n",
    "\n",
    "    # Arguments & parameters\n",
    "    dataset_dir = args.dataset_dir\n",
    "    workspace = args.workspace\n",
    "    mini_data = args.mini_data\n",
    "\n",
    "    sample_rate = config.sample_rate\n",
    "    clip_samples = config.clip_samples\n",
    "    center = config.center\n",
    "    classes_num = config.classes_num\n",
    "    lb_to_idx = config.lb_to_idx\n",
    "\n",
    "    # Paths\n",
    "    audios_dir = os.path.join(dataset_dir)\n",
    "\n",
    "    if mini_data:\n",
    "        packed_hdf5_path = os.path.join(workspace, 'features', 'minidata_waveform.h5')\n",
    "    else:\n",
    "        packed_hdf5_path = os.path.join(workspace, 'features', 'waveform.h5')\n",
    "    create_folder(os.path.dirname(packed_hdf5_path))\n",
    "\n",
    "    (audio_names, audio_paths) = traverse_folder(audios_dir)\n",
    "    \n",
    "    audio_names = sorted(audio_names)\n",
    "    audio_paths = sorted(audio_paths)\n",
    "\n",
    "    meta_dict = {\n",
    "        'audio_name': np.array(audio_names), \n",
    "        'audio_path': np.array(audio_paths), \n",
    "        # 'target': np.array([lb_to_idx[audio_name.split('.')[0]] for audio_name in audio_names]), \n",
    "        'target': np.array([lb_to_idx[audio_path.split('/')[-2]] for audio_path in audio_paths]),\n",
    "        'fold': np.arange(len(audio_names)) % 10 + 1}\n",
    "    \n",
    "    if mini_data:\n",
    "        mini_num = 10\n",
    "        total_num = len(meta_dict['audio_name'])\n",
    "        random_state = np.random.RandomState(1234)\n",
    "        indexes = random_state.choice(total_num, size=mini_num, replace=False)\n",
    "        for key in meta_dict.keys():\n",
    "            meta_dict[key] = meta_dict[key][indexes]\n",
    "\n",
    "    audios_num = len(meta_dict['audio_name'])\n",
    "\n",
    "    feature_time = time.time()\n",
    "    with h5py.File(packed_hdf5_path, 'w') as hf:\n",
    "        hf.create_dataset(\n",
    "            name='audio_name', \n",
    "            shape=(audios_num,), \n",
    "            dtype='S80')\n",
    "\n",
    "        hf.create_dataset(\n",
    "            name='waveform', \n",
    "            shape=(audios_num, clip_samples), \n",
    "            dtype=np.int16)\n",
    "\n",
    "        hf.create_dataset(\n",
    "            name='target', \n",
    "            shape=(audios_num, classes_num), \n",
    "            dtype=np.float32)\n",
    "\n",
    "        hf.create_dataset(\n",
    "            name='fold', \n",
    "            shape=(audios_num,), \n",
    "            dtype=np.int32)\n",
    " \n",
    "        for n in range(audios_num):\n",
    "            print(n)\n",
    "            audio_name = meta_dict['audio_name'][n]\n",
    "            fold = meta_dict['fold'][n]\n",
    "            audio_path = meta_dict['audio_path'][n]\n",
    "            print(audio_path)\n",
    "            (audio, fs) = librosa.core.load(audio_path, sr=sample_rate, mono=True)\n",
    "            \n",
    "            print('Audio length:',len(audio)/sample_rate,'(s)')\n",
    "            audio = pad_truncate_sequence(audio, clip_samples, center)\n",
    "            print('Audio length (after truncating):',len(audio)/sample_rate,'(s)')\n",
    "            \n",
    "            hf['audio_name'][n] = audio_name.encode()\n",
    "            hf['waveform'][n] = float32_to_int16(audio)\n",
    "            hf['target'][n] = to_one_hot(meta_dict['target'][n], classes_num)\n",
    "            hf['fold'][n] = meta_dict['fold'][n]\n",
    "\n",
    "    print('Write hdf5 to {}'.format(packed_hdf5_path))\n",
    "    print('Time: {:.3f} s'.format(time.time() - feature_time))\n",
    "\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "    \n",
    "#     parser = argparse.ArgumentParser(description='')\n",
    "#     subparsers = parser.add_subparsers(dest='mode')\n",
    "\n",
    "#     # Calculate feature for all audio files\n",
    "#     parser_pack_audio = subparsers.add_parser('pack_audio_files_to_hdf5')\n",
    "#     parser_pack_audio.add_argument('--dataset_dir', type=str, required=True, help='Directory of dataset.')\n",
    "#     parser_pack_audio.add_argument('--workspace', type=str, required=True, help='Directory of your workspace.')\n",
    "#     parser_pack_audio.add_argument('--mini_data', action='store_true', default=False, help='Set True for debugging on a small part of data.')\n",
    "    \n",
    "#     # Parse arguments\n",
    "#     args = parser.parse_args()\n",
    "    \n",
    "#     if args.mode == 'pack_audio_files_to_hdf5':\n",
    "#         pack_audio_files_to_hdf5(args)\n",
    "        \n",
    "#     else:\n",
    "#         raise Exception('Incorrect arguments!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "c8345d11-94e6-4bc3-b041-2cdf7cefa49c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "class Dataset:\n",
    "    workspace = \"\"\n",
    "    dataset_dir = \"\"\n",
    "    mini_data = False    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "87208232-d552-42cf-86e0-56e8247e12b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Dataset.workspace = workspace\n",
    "Dataset.dataset_dir = dataset_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "5e303eed-d1b3-4ba5-a075-e289a021a653",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# pack_audio_files_to_hdf5(args=Dataset, config=Config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd258a9-a5d6-4e61-893c-cac8bd995a4d",
   "metadata": {},
   "source": [
    "# Data Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "774fbf68-89fb-4b33-aff4-cb098c90cc19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "import csv\n",
    "import time\n",
    "import logging\n",
    "\n",
    "# from utilities import int16_to_float32\n",
    "\n",
    "class AudioSetDataset(object):\n",
    "    def __init__(self):\n",
    "        \"\"\"This class takes the meta of an audio clip as input, and return \n",
    "        the waveform and target of the audio clip. This class is used by DataLoader. \n",
    "        Args:\n",
    "          clip_samples: int\n",
    "          classes_num: int\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def __getitem__(self, meta):\n",
    "        \"\"\"Load waveform and target of an audio clip.\n",
    "        \n",
    "        Args:\n",
    "          meta: {\n",
    "            'audio_name': str, \n",
    "            'hdf5_path': str, \n",
    "            'index_in_hdf5': int}\n",
    "        Returns: \n",
    "          data_dict: {\n",
    "            'audio_name': str, \n",
    "            'waveform': (clip_samples,), \n",
    "            'target': (classes_num,)}\n",
    "        \"\"\"\n",
    "        hdf5_path = meta['hdf5_path']\n",
    "        index_in_hdf5 = meta['index_in_hdf5']\n",
    "\n",
    "        with h5py.File(hdf5_path, 'r') as hf:\n",
    "            audio_name = hf['audio_name'][index_in_hdf5].decode()\n",
    "            waveform = int16_to_float32(hf['waveform'][index_in_hdf5])\n",
    "            target = hf['target'][index_in_hdf5].astype(np.float32)\n",
    "\n",
    "        data_dict = {\n",
    "            'audio_name': audio_name, 'waveform': waveform, 'target': target}\n",
    "            \n",
    "        return data_dict\n",
    "\n",
    "    def resample(self, waveform):\n",
    "        \"\"\"Resample.\n",
    "\n",
    "        Args:\n",
    "          waveform: (clip_samples,)\n",
    "\n",
    "        Returns:\n",
    "          (resampled_clip_samples,)\n",
    "        \"\"\"\n",
    "        if self.sample_rate == 32000:\n",
    "            return waveform\n",
    "        elif self.sample_rate == 16000:\n",
    "            return waveform[0 :: 2]\n",
    "        elif self.sample_rate == 8000:\n",
    "            return waveform[0 :: 4]\n",
    "        else:\n",
    "            raise Exception('Incorrect sample rate!')\n",
    "\n",
    "\n",
    "class Base(object):\n",
    "    def __init__(self, indexes_hdf5_path, batch_size, random_seed):\n",
    "        \"\"\"Base class of train sampler.\n",
    "        \n",
    "        Args:\n",
    "          indexes_hdf5_path: string\n",
    "          batch_size: int\n",
    "          black_list_csv: string\n",
    "          random_seed: int\n",
    "        \"\"\"\n",
    "        self.batch_size = batch_size\n",
    "        self.random_state = np.random.RandomState(random_seed)\n",
    "\n",
    "        # Load target\n",
    "        load_time = time.time()\n",
    "\n",
    "        with h5py.File(indexes_hdf5_path, 'r') as hf:\n",
    "            self.audio_names = [audio_name.decode() for audio_name in hf['audio_name'][:]]\n",
    "            # self.hdf5_paths = [hdf5_path.decode() for hdf5_path in hf['hdf5_path'][:]]\n",
    "            self.indexes_in_hdf5 = hf['index_in_hdf5'][:]\n",
    "            self.targets = hf['target'][:].astype(np.float32)\n",
    "            self.folds = hf['fold'][:].astype(np.float32)\n",
    "        \n",
    "        (self.audios_num, self.classes_num) = self.targets.shape\n",
    "        logging.info('Training number: {}'.format(self.audios_num))\n",
    "        logging.info('Load target time: {:.3f} s'.format(time.time() - load_time))\n",
    "\n",
    "\n",
    "class TrainSampler(object):\n",
    "    def __init__(self, hdf5_path, holdout_fold, batch_size, random_seed=1234):\n",
    "        \"\"\"Balanced sampler. Generate batch meta for training.\n",
    "        \n",
    "        Args:\n",
    "          indexes_hdf5_path: string\n",
    "          batch_size: int\n",
    "          black_list_csv: string\n",
    "          random_seed: int\n",
    "        \"\"\"\n",
    "        # super(TrainSampler, self).__init__(indexes_hdf5_path, batch_size, \n",
    "            # random_seed)\n",
    "\n",
    "        self.hdf5_path = hdf5_path\n",
    "        self.batch_size = batch_size\n",
    "        self.random_state = np.random.RandomState(random_seed)\n",
    "\n",
    "        with h5py.File(hdf5_path, 'r') as hf:\n",
    "            self.folds = hf['fold'][:].astype(np.float32)\n",
    "\n",
    "        self.indexes = np.where(self.folds != int(holdout_fold))[0]\n",
    "        self.audios_num = len(self.indexes)\n",
    "        # self.validate_audio_indexes = np.where(self.folds == int(holdout_fold))[0]\n",
    "        \n",
    "        # self.indexes = np.arange(self.audios_num)\n",
    "            \n",
    "        # Shuffle indexes\n",
    "        self.random_state.shuffle(self.indexes)\n",
    "        \n",
    "        self.pointer = 0\n",
    "\n",
    "    def __iter__(self):\n",
    "        \"\"\"Generate batch meta for training. \n",
    "        \n",
    "        Returns:\n",
    "          batch_meta: e.g.: [\n",
    "            {'audio_name': 'YfWBzCRl6LUs.wav', \n",
    "             'hdf5_path': 'xx/balanced_train.h5', \n",
    "             'index_in_hdf5': 15734, \n",
    "             'target': [0, 1, 0, 0, ...]}, \n",
    "            ...]\n",
    "        \"\"\"\n",
    "        batch_size = self.batch_size\n",
    "\n",
    "        while True:\n",
    "            batch_meta = []\n",
    "            i = 0\n",
    "            while i < batch_size:\n",
    "                index = self.indexes[self.pointer]\n",
    "                self.pointer += 1\n",
    "\n",
    "                # Shuffle indexes and reset pointer\n",
    "                if self.pointer >= self.audios_num:\n",
    "                    self.pointer = 0\n",
    "                    self.random_state.shuffle(self.indexes)\n",
    "                \n",
    "                batch_meta.append({\n",
    "                    'hdf5_path': self.hdf5_path, \n",
    "                    'index_in_hdf5': self.indexes[self.pointer]})\n",
    "                i += 1\n",
    "\n",
    "            yield batch_meta\n",
    "\n",
    "    def state_dict(self):\n",
    "        state = {\n",
    "            'indexes': self.indexes,\n",
    "            'pointer': self.pointer}\n",
    "        return state\n",
    "            \n",
    "    def load_state_dict(self, state):\n",
    "        self.indexes = state['indexes']\n",
    "        self.pointer = state['pointer']\n",
    "\n",
    "\n",
    "class EvaluateSampler(object):\n",
    "    def __init__(self, hdf5_path, holdout_fold, batch_size, random_seed=1234):\n",
    "        \"\"\"Balanced sampler. Generate batch meta for training.\n",
    "        \n",
    "        Args:\n",
    "          indexes_hdf5_path: string\n",
    "          batch_size: int\n",
    "          black_list_csv: string\n",
    "          random_seed: int\n",
    "        \"\"\"\n",
    "        # super(TrainSampler, self).__init__(indexes_hdf5_path, batch_size, \n",
    "            # random_seed)\n",
    "\n",
    "        self.hdf5_path = hdf5_path\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        with h5py.File(hdf5_path, 'r') as hf:\n",
    "            self.folds = hf['fold'][:].astype(np.float32)\n",
    "\n",
    "        self.indexes = np.where(self.folds == int(holdout_fold))[0]\n",
    "        self.audios_num = len(self.indexes)\n",
    "        \n",
    "    def __iter__(self):\n",
    "        \"\"\"Generate batch meta for training. \n",
    "        \n",
    "        Returns:\n",
    "          batch_meta: e.g.: [\n",
    "            {'audio_name': 'YfWBzCRl6LUs.wav', \n",
    "             'hdf5_path': 'xx/balanced_train.h5', \n",
    "             'index_in_hdf5': 15734, \n",
    "             'target': [0, 1, 0, 0, ...]}, \n",
    "            ...]\n",
    "        \"\"\"\n",
    "        batch_size = self.batch_size\n",
    "        pointer = 0\n",
    "\n",
    "        while pointer < self.audios_num:\n",
    "            batch_indexes = np.arange(pointer, \n",
    "                min(pointer + batch_size, self.audios_num))\n",
    "\n",
    "            batch_meta = []\n",
    "\n",
    "            for i in batch_indexes:\n",
    "                batch_meta.append({\n",
    "                    'hdf5_path': self.hdf5_path, \n",
    "                    'index_in_hdf5': self.indexes[i]})\n",
    "\n",
    "            pointer += batch_size\n",
    "            yield batch_meta\n",
    "\n",
    "\n",
    "def collate_fn(list_data_dict):\n",
    "    \"\"\"Collate data.\n",
    "    Args:\n",
    "      list_data_dict, e.g., [{'audio_name': str, 'waveform': (clip_samples,), ...}, \n",
    "                             {'audio_name': str, 'waveform': (clip_samples,), ...},\n",
    "                             ...]\n",
    "    Returns:\n",
    "      np_data_dict, dict, e.g.,\n",
    "          {'audio_name': (batch_size,), 'waveform': (batch_size, clip_samples), ...}\n",
    "    \"\"\"\n",
    "    np_data_dict = {}\n",
    "    \n",
    "    for key in list_data_dict[0].keys():\n",
    "        np_data_dict[key] = np.array([data_dict[key] for data_dict in list_data_dict])\n",
    "    \n",
    "    return np_data_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4454e7a1-d26d-409e-b114-856a92dfd845",
   "metadata": {},
   "source": [
    "# Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "c3ce0127-9c15-444c-bc13-5de46e46cd38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import sklearn\n",
    "\n",
    "\n",
    "def clip_bce(output_dict, target_dict):\n",
    "    \"\"\"Binary crossentropy loss.\n",
    "    \"\"\"\n",
    "    return F.binary_cross_entropy_with_logits(\n",
    "        output_dict['clipwise_output'], target_dict['target'])\n",
    "\n",
    "def clip_balanced_bce(output_dict, target_dict):\n",
    "    \"\"\"Binary crossentropy loss.\n",
    "    \"\"\"\n",
    "    class_weights = class_weight.compute_class_weight('balanced',np.unique(target_dict['target']),target_dict['target'].numpy())\n",
    "    class_weights = torch.tensor(class_weights, dtype=torch.float)\n",
    "    return F.binary_cross_entropy_with_logits(\n",
    "        output_dict['clipwise_output'], target_dict['target'], weight=class_weights)\n",
    "\n",
    "def clip_nll(output_dict, target_dict):\n",
    "    loss = - torch.mean(target_dict['target'] * output_dict['clipwise_output'])\n",
    "    return loss\n",
    "\n",
    "def get_loss_func(loss_type):\n",
    "    if loss_type == 'clip_bce':\n",
    "        return clip_bce\n",
    "    if loss_type == 'clip_balanced_bce':\n",
    "        return clip_bce\n",
    "    if loss_type == 'clip_nll':\n",
    "        return clip_nll"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd44b4b-cb5b-4b84-ac78-b727a5fccbaa",
   "metadata": {},
   "source": [
    "# Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "58610b52-ea75-4e1f-a728-aa543d503895",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import sys\n",
    "# sys.path.insert(1, os.path.join(sys.path[0], '../utils'))\n",
    "# import numpy as np\n",
    "# import argparse\n",
    "# import time\n",
    "# import logging\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "# import torch.optim as optim\n",
    " \n",
    "# from config import (sample_rate, classes_num, mel_bins, fmin, fmax, window_size, \n",
    "#     hop_size, center, labels)\n",
    "# from losses import get_loss_func\n",
    "# from pytorch_utils import move_data_to_device, do_mixup\n",
    "# from utilities import (create_folder, get_filename, create_logging, StatisticsContainer, Mixup)\n",
    "# from data_generator import DatasetGenerator, TrainSampler, EvaluateSampler, collate_fn\n",
    "# from models import *\n",
    "# from evaluate import Evaluator\n",
    "\n",
    "\n",
    "def train(args, config):\n",
    "    \"\"\"Train AudioSet tagging model. \n",
    "    \n",
    "    Args:\n",
    "      workspace: str\n",
    "      dataset_dir: str\n",
    "      model_type: str\n",
    "      pretrained_checkpoint_path: str\n",
    "      freeze_base: str\n",
    "      loss_type: 'clip_bce'\n",
    "      holdout_fold: int\n",
    "      augmentation: 'none' | 'mixup'\n",
    "      batch_size: int\n",
    "      learning_rate: float\n",
    "      resume_iteration: int\n",
    "      stop_iteration: int\n",
    "      cuda: bool\n",
    "      filename: str\n",
    "      num_workers: int\n",
    "      sample_rate: int\n",
    "      mel_bins: int\n",
    "      hop_size: int\n",
    "      fmin: int\n",
    "      fmax: int\n",
    "      window_size: int      \n",
    "    \"\"\"\n",
    "        \n",
    "    # Arguments & parameters\n",
    "    workspace = args.workspace\n",
    "    dataset_dir = args.dataset_dir\n",
    "    \n",
    "    model_type = args.model_type\n",
    "    pretrained_checkpoint_path = args.pretrained_checkpoint_path\n",
    "    freeze_base = args.freeze_base\n",
    "    loss_type = args.loss_type\n",
    "    \n",
    "    holdout_fold = args.holdout_fold\n",
    "    augmentation = args.augmentation\n",
    "    batch_size = args.batch_size\n",
    "    learning_rate = args.learning_rate\n",
    "    resume_iteration = args.resume_iteration\n",
    "    stop_iteration = args.stop_iteration\n",
    "    torch_available = torch.cuda.is_available()\n",
    "    device = torch.device('cuda') if args.cuda and torch_available else torch.device('cpu')\n",
    "    filename = args.filename\n",
    "    num_workers = args.num_workers\n",
    "    \n",
    "    clip_samples = config.clip_samples\n",
    "    classes_num = config.classes_num\n",
    "    pretrain = True if pretrained_checkpoint_path else False\n",
    "    loss_func = get_loss_func(loss_type)\n",
    "\n",
    "    sample_rate = config.sample_rate\n",
    "    mel_bins = args.mel_bins\n",
    "    fmin = args.fmin\n",
    "    fmax = args.fmax\n",
    "    window_size = args.window_size\n",
    "    hop_size = args.hop_size\n",
    "    center = args.center\n",
    "    \n",
    "    # Paths\n",
    "    hdf5_path = os.path.join(workspace, 'features', 'waveform.h5')\n",
    "\n",
    "    checkpoints_dir = os.path.join(workspace, 'checkpoints', filename, \n",
    "        'holdout_fold={}'.format(holdout_fold), model_type, 'pretrain={}'.format(pretrain), \n",
    "        'loss_type={}'.format(loss_type), 'augmentation={}'.format(augmentation),\n",
    "         'batch_size={}'.format(batch_size), 'freeze_base={}'.format(freeze_base))\n",
    "    create_folder(checkpoints_dir)\n",
    "\n",
    "    statistics_path = os.path.join(workspace, 'statistics', filename, \n",
    "        'holdout_fold={}'.format(holdout_fold), model_type, 'pretrain={}'.format(pretrain), \n",
    "        'loss_type={}'.format(loss_type), 'augmentation={}'.format(augmentation), \n",
    "        'batch_size={}'.format(batch_size), 'freeze_base={}'.format(freeze_base), \n",
    "        'statistics.pickle')\n",
    "    create_folder(os.path.dirname(statistics_path))\n",
    "    \n",
    "    logs_dir = os.path.join(workspace, 'logs', filename, \n",
    "        'holdout_fold={}'.format(holdout_fold), model_type, 'pretrain={}'.format(pretrain), \n",
    "        'loss_type={}'.format(loss_type), 'augmentation={}'.format(augmentation), \n",
    "        'batch_size={}'.format(batch_size), 'freeze_base={}'.format(freeze_base))\n",
    "    create_logging(logs_dir, 'w')\n",
    "    logging.info(args)\n",
    "    \n",
    "    if 'cuda' in str(device):\n",
    "        logging.info('Using GPU.')\n",
    "        device = 'cuda'\n",
    "    else:\n",
    "        logging.info('Using CPU. Set --cuda flag to use GPU.')\n",
    "        device = 'cpu'\n",
    "    \n",
    "    # Model\n",
    "    Model = eval(model_type)\n",
    "    if pretrain:\n",
    "        model = Model(sample_rate=sample_rate, window_size=window_size, \n",
    "        hop_size=hop_size, mel_bins=mel_bins, fmin=fmin, fmax=fmax, \n",
    "        classes_num=classes_num, freeze_base=freeze_base)\n",
    "    else:   \n",
    "        model = Model(sample_rate=sample_rate, window_size=window_size, \n",
    "        hop_size=hop_size, mel_bins=mel_bins, fmin=fmin, fmax=fmax, \n",
    "        classes_num=classes_num)\n",
    "     \n",
    "    params_num = count_parameters(model)\n",
    "    # flops_num = count_flops(model, clip_samples)\n",
    "    logging.info('Parameters num: {}'.format(params_num))\n",
    "    # logging.info('Flops num: {:.3f} G'.format(flops_num / 1e9))\n",
    "\n",
    "    # Statistics\n",
    "    statistics_container = StatisticsContainer(statistics_path)\n",
    "\n",
    "    if pretrain:\n",
    "        logging.info('Load pretrained model from {}'.format(pretrained_checkpoint_path))\n",
    "        model.load_from_pretrain(pretrained_checkpoint_path)\n",
    "    \n",
    "    if resume_iteration:\n",
    "        resume_checkpoint_path = os.path.join(checkpoints_dir, '{}_iterations.pth'.format(resume_iteration))\n",
    "        logging.info('Load resume model from {}'.format(resume_checkpoint_path))\n",
    "        resume_checkpoint = torch.load(resume_checkpoint_path)\n",
    "        model.load_state_dict(resume_checkpoint['model'])\n",
    "        statistics_container.load_state_dict(resume_iteration)\n",
    "        iteration = resume_checkpoint['iteration']\n",
    "    else:\n",
    "        iteration = 0\n",
    "\n",
    "    # Parallel\n",
    "    print('GPU number: {}'.format(torch.cuda.device_count()))\n",
    "    model = torch.nn.DataParallel(model)\n",
    "    \n",
    "    # Dataset will be used by DataLoader later. Dataset takes a meta as input \n",
    "    # and return a waveform and a target.\n",
    "    dataset = AudioSetDataset()\n",
    "    \n",
    "    # Data generator\n",
    "    train_sampler = TrainSampler(\n",
    "        hdf5_path=hdf5_path, \n",
    "        holdout_fold=holdout_fold, \n",
    "        batch_size=batch_size * 2 if 'mixup' in augmentation else batch_size)\n",
    "\n",
    "    validate_sampler = EvaluateSampler(\n",
    "        hdf5_path=hdf5_path, \n",
    "        holdout_fold=holdout_fold, \n",
    "        batch_size=batch_size)\n",
    "\n",
    "    # Data loader\n",
    "    train_loader = torch.utils.data.DataLoader(dataset=dataset, \n",
    "        batch_sampler=train_sampler, collate_fn=collate_fn, \n",
    "        num_workers=num_workers, pin_memory=True)\n",
    "\n",
    "    validate_loader = torch.utils.data.DataLoader(dataset=dataset, \n",
    "        batch_sampler=validate_sampler, collate_fn=collate_fn, \n",
    "        num_workers=num_workers, pin_memory=True)\n",
    "    \n",
    "    if 'cuda' in device:\n",
    "        model.to(device)\n",
    "\n",
    "    # Optimizer\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, betas=(0.9, 0.999),\n",
    "        eps=1e-08, weight_decay=0., amsgrad=True)\n",
    "\n",
    "    \n",
    "    if 'mixup' in augmentation:\n",
    "        mixup_augmenter = Mixup(mixup_alpha=1.)\n",
    "     \n",
    "    # Evaluator\n",
    "    evaluator = Evaluator(model=model)\n",
    "    \n",
    "    train_bgn_time = time.time()\n",
    "    time1 = time.time()\n",
    "    \n",
    "    # Train on mini batches\n",
    "    for batch_data_dict in train_loader:\n",
    "\n",
    "        # Evaluate\n",
    "        if iteration % 100 == 0 and iteration > 0:\n",
    "            if resume_iteration > 0 and iteration == resume_iteration:\n",
    "                pass\n",
    "            else:\n",
    "                logging.info('------------------------------------')\n",
    "                logging.info('Iteration: {}'.format(iteration))\n",
    "\n",
    "                train_fin_time = time.time()\n",
    "\n",
    "                statistics = evaluator.evaluate(validate_loader)\n",
    "                logging.info(f\"Validate accuracy: {statistics['accuracy']}\")\n",
    "                logging.info(f\"Validate balanced accuracy: {statistics['balanced_accuracy']}\")\n",
    "                logging.info(f\"Validate precision: {statistics['precision']}\")\n",
    "                logging.info(f\"Validate recall: {statistics['recall']}\")\n",
    "                logging.info(f\"Validate f1: {statistics['f1']}\")\n",
    "                logging.info(f\"Validate jaccard: {statistics['jaccard']}\")\n",
    "                for label in labels:\n",
    "                    logging.info(f\"Validate {label} cm: {statistics['cm_'+label]}\")\n",
    "                    logging.info(f\"Validate {label} tn: {statistics['tn_'+label]}\")\n",
    "                    logging.info(f\"Validate {label} fp: {statistics['fp_'+label]}\")\n",
    "                    logging.info(f\"Validate {label} fn: {statistics['fn_'+label]}\")\n",
    "                    logging.info(f\"Validate {label} tp: {statistics['tp_'+label]}\")\n",
    "                    logging.info(f\"Validate {label} accuracy: {statistics['accuracy_'+label]}\")\n",
    "                    logging.info(f\"Validate {label} balanced accuracy: {statistics['balanced_accuracy_'+label]}\")\n",
    "                    logging.info(f\"Validate {label} precision: {statistics['precision_'+label]}\")\n",
    "                    logging.info(f\"Validate {label} recall: {statistics['recall_'+label]}\")\n",
    "                    logging.info(f\"Validate {label} f1: {statistics['f1_'+label]}\")\n",
    "                    logging.info(f\"Validate {label} jaccard: {statistics['jaccard_'+label]}\")\n",
    "\n",
    "                \n",
    "                statistics_container.append(iteration, statistics, 'validate')\n",
    "                statistics_container.dump()\n",
    "\n",
    "                train_time = train_fin_time - train_bgn_time\n",
    "                validate_time = time.time() - train_fin_time\n",
    "\n",
    "                logging.info(\n",
    "                    'Train time: {:.3f} s, validate time: {:.3f} s'\n",
    "                    ''.format(train_time, validate_time))\n",
    "\n",
    "                train_bgn_time = time.time()\n",
    "\n",
    "        # Save model \n",
    "        if iteration % 2000 == 0 and iteration > 0:\n",
    "            checkpoint = {\n",
    "                'iteration': iteration, \n",
    "                'model': model.module.state_dict()}\n",
    "\n",
    "            checkpoint_path = os.path.join(\n",
    "                checkpoints_dir, '{}_iterations.pth'.format(iteration))\n",
    "                \n",
    "            torch.save(checkpoint, checkpoint_path)\n",
    "            logging.info('Model saved to {}'.format(checkpoint_path))\n",
    "        \n",
    "        if 'mixup' in augmentation:\n",
    "            batch_data_dict['mixup_lambda'] = mixup_augmenter.get_lambda(len(batch_data_dict['waveform']))\n",
    "        \n",
    "        # Move data to GPU\n",
    "        for key in batch_data_dict.keys():\n",
    "            batch_data_dict[key] = move_data_to_device(batch_data_dict[key], device)\n",
    "        \n",
    "        # Train\n",
    "        model.train()\n",
    "\n",
    "        if 'mixup' in augmentation:\n",
    "            batch_output_dict = model(batch_data_dict['waveform'], \n",
    "                batch_data_dict['mixup_lambda'])\n",
    "            \"\"\"{'clipwise_output': (batch_size, classes_num), ...}\"\"\"\n",
    "\n",
    "            batch_target_dict = {'target': do_mixup(batch_data_dict['target'], \n",
    "                batch_data_dict['mixup_lambda'])}\n",
    "            \"\"\"{'target': (batch_size, classes_num)}\"\"\"\n",
    "        else:\n",
    "            batch_output_dict = model(batch_data_dict['waveform'], None)\n",
    "            \"\"\"{'clipwise_output': (batch_size, classes_num), ...}\"\"\"\n",
    "\n",
    "            batch_target_dict = {'target': batch_data_dict['target']}\n",
    "            \"\"\"{'target': (batch_size, classes_num)}\"\"\"\n",
    "\n",
    "        # sanity check\n",
    "        # print(\"\\ntarget_dict['target']:\",batch_target_dict['target'])\n",
    "        # print(\"output_dict['clipwise_output']\",batch_output_dict['clipwise_output'],'\\n')\n",
    "\n",
    "        # loss\n",
    "        loss = loss_func(batch_output_dict, batch_target_dict)\n",
    "        print(iteration, loss)\n",
    "\n",
    "        # Backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Stop learning\n",
    "        if iteration == stop_iteration:\n",
    "            break \n",
    "\n",
    "        iteration += 1\n",
    "        \n",
    "\n",
    "# if __name__ == '__main__':\n",
    "\n",
    "#     parser = argparse.ArgumentParser(description='Example of parser. ')\n",
    "#     subparsers = parser.add_subparsers(dest='mode')\n",
    "\n",
    "#     parser_train = subparsers.add_parser('train') \n",
    "#     parser_train.add_argument('--workspace', type=str, required=True)\n",
    "#     parser_train.add_argument('--data_type', type=str, default='full_train', choices=['balanced_train', 'full_train'])\n",
    "#     parser_train.add_argument('--sample_rate', type=int, default=32000)\n",
    "#     parser_train.add_argument('--window_size', type=int, default=1024)\n",
    "#     parser_train.add_argument('--hop_size', type=int, default=320)\n",
    "#     parser_train.add_argument('--mel_bins', type=int, default=64)\n",
    "#     parser_train.add_argument('--fmin', type=int, default=50)\n",
    "#     parser_train.add_argument('--fmax', type=int, default=14000) \n",
    "#     parser_train.add_argument('--model_type', type=str, required=True)\n",
    "#     parser_train.add_argument('--loss_type', type=str, default='clip_bce', choices=['clip_bce'])\n",
    "#     parser_train.add_argument('--balanced', type=str, default='balanced', choices=['none', 'balanced', 'alternate'])\n",
    "#     parser_train.add_argument('--augmentation', type=str, default='mixup', choices=['none', 'mixup'])\n",
    "#     parser_train.add_argument('--batch_size', type=int, default=32)\n",
    "#     parser_train.add_argument('--learning_rate', type=float, default=1e-3)\n",
    "#     parser_train.add_argument('--resume_iteration', type=int, default=0)\n",
    "#     parser_train.add_argument('--stop_iteration', type=int, default=1000000)\n",
    "#     parser_train.add_argument('--cuda', action='store_true', default=False)\n",
    "    \n",
    "#     args = parser.parse_args()\n",
    "#     args.filename = get_filename(__file__)\n",
    "\n",
    "#     if args.mode == 'train':\n",
    "#         train(args)\n",
    "\n",
    "#     else:\n",
    "#         raise Exception('Error argument!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "3ade2022-570a-4340-9f1f-715c5bfd91d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "class Train:\n",
    "    workspace = \"\"\n",
    "    dataset_dir = \"\"\n",
    "    holdout_fold = 2    \n",
    "    model_type = \"Wavegram_Logmel_Cnn14\"\n",
    "    pretrained_checkpoint_path = \"\"\n",
    "    freeze_base = False\n",
    "    loss_type = \"clip_balanced_bce\"\n",
    "    augmentation = \"\"\n",
    "    batch_size = 32\n",
    "    learning_rate = 1e-4\n",
    "    resume_iteration = 0\n",
    "    stop_iteration = 10000\n",
    "    cuda = False\n",
    "    filename = \"main\"\n",
    "    num_workers = 8\n",
    "    mel_bins = 64\n",
    "    fmin = 50\n",
    "    fmax = 8000\n",
    "    window_size = 512\n",
    "    hop_size = 160"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "ce2885c4-4020-465c-a0ee-3805d23b4a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "Train.workspace = workspace\n",
    "Train.dataset_dir = dataset_dir\n",
    "Train.holdout_fold = 2\n",
    "Train.model_type = \"Transfer_Wavegram_Logmel_Cnn14\"\n",
    "Train.pretrained_checkpoint_path = os.path.join(workspace,'pretrained_model','Wavegram_Logmel_Cnn14_mAP=0.439.pth')\n",
    "Train.freeze_base = True\n",
    "Train.loss_type = \"clip_balanced_bce\"\n",
    "Train.augmentation = \"mixup\"\n",
    "Train.batch_size = 32\n",
    "Train.learning_rate = 1e-4\n",
    "Train.resume_iteration = 0\n",
    "Train.early_stop = 10000\n",
    "Train.cuda = True\n",
    "Train.filename = \"main\"\n",
    "Train.num_workers = 8\n",
    "Train.mel_bins = 64\n",
    "Train.fmin = 50\n",
    "Train.fmax = 14000\n",
    "Train.window_size = 1024\n",
    "Train.hop_size = 320"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b875777-69dd-42ff-bd02-e3edd21a03b0",
   "metadata": {},
   "source": [
    "Cnn6 0.208\n",
    "Cnn10 0.204\n",
    "ResNet22 0.170\n",
    "ResNet38 0.220\n",
    "ResNet54 0.179\n",
    "Cnn14_emb512 0.156\n",
    "Cnn14_emb128 0.191\n",
    "Cnn14_emb32 0.153\n",
    "MobileNetV1 0.188\n",
    "MobileNetV2 0.187\n",
    "LeeNet24 0.176\n",
    "DaiNet19 0.164\n",
    "Res1dNet51 0.079\n",
    "Wavegram_Logmel_Cnn14 0.150\n",
    "Cnn14_16k\n",
    "Cnn14_mixup_time_domain 0.126\n",
    "Cnn14_DecisionLevelMax 0.227\n",
    "Cnn14_DecisionLevelAtt 0.191"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "6860ee68-8673-4c03-933b-73f4bfdb4dd5",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# train(args=Train, config=Config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b01f272-a7c0-43ef-9b82-74d6cdf914c6",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "b9968f9c-e5de-4915-ac33-7d386c2fb2c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import argparse\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "# from utilities import create_folder, get_filename\n",
    "# from models import *\n",
    "# from pytorch_utils import move_data_to_device\n",
    "# import config\n",
    "\n",
    "\n",
    "def audio_tagging(args, config):\n",
    "    \"\"\"Inference audio tagging result of an audio clip.\n",
    "    \"\"\"\n",
    "\n",
    "    # Arugments & parameters\n",
    "    sample_rate = args.sample_rate\n",
    "    window_size = args.window_size\n",
    "    hop_size = args.hop_size\n",
    "    mel_bins = args.mel_bins\n",
    "    fmin = args.fmin\n",
    "    fmax = args.fmax\n",
    "    model_type = args.model_type\n",
    "    checkpoint_path = args.checkpoint_path\n",
    "    audio_path = args.audio_path\n",
    "    torch_available = torch.cuda.is_available()\n",
    "    device = torch.device('cuda') if args.cuda and torch_available else torch.device('cpu')\n",
    "    freeze_base = args.freeze_base\n",
    "    \n",
    "    classes_num = config.classes_num\n",
    "    labels = config.labels\n",
    "\n",
    "    # Model\n",
    "    Model = eval(model_type)\n",
    "    if 'Transfer' in model_type:\n",
    "        model = Model(sample_rate=sample_rate, window_size=window_size, \n",
    "            hop_size=hop_size, mel_bins=mel_bins, fmin=fmin, fmax=fmax, \n",
    "            classes_num=classes_num, freeze_base=freeze_base)\n",
    "    else:\n",
    "        model = Model(sample_rate=sample_rate, window_size=window_size, \n",
    "        hop_size=hop_size, mel_bins=mel_bins, fmin=fmin, fmax=fmax, \n",
    "        classes_num=classes_num)\n",
    "    \n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "    model.load_state_dict(checkpoint['model'])\n",
    "\n",
    "    # Parallel\n",
    "    if 'cuda' in str(device):\n",
    "        model.to(device)\n",
    "        print('GPU number: {}'.format(torch.cuda.device_count()))\n",
    "        model = torch.nn.DataParallel(model)\n",
    "    else:\n",
    "        print('Using CPU.')\n",
    "    \n",
    "    # Load audio\n",
    "    (waveform, _) = librosa.core.load(audio_path, sr=sample_rate, mono=True)\n",
    "\n",
    "    waveform = waveform[None, :]    # (1, audio_length)\n",
    "    waveform = move_data_to_device(waveform, device)\n",
    "\n",
    "    # Forward\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        batch_output_dict = model(waveform, None)\n",
    "\n",
    "    clipwise_output = batch_output_dict['clipwise_output'].data.cpu().numpy()[0]\n",
    "    \"\"\"(classes_num,)\"\"\"\n",
    "    # activate if using log_softmax output\n",
    "    # sum_values = sum(clipwise_output)\n",
    "    # clipwise_output = np.array([num /sum_values for num in clipwise_output])\n",
    "\n",
    "    sorted_indexes = np.argsort(clipwise_output)[::-1]\n",
    "\n",
    "    # Print audio tagging top probabilities\n",
    "    for k in range(classes_num):\n",
    "        print('{}: {:.3f}'.format(np.array(labels)[sorted_indexes[k]], \n",
    "            clipwise_output[sorted_indexes[k]]))\n",
    "\n",
    "    # Print embedding\n",
    "    if 'embedding' in batch_output_dict.keys():\n",
    "        embedding = batch_output_dict['embedding'].data.cpu().numpy()[0]\n",
    "        print('embedding: {}'.format(embedding.shape))\n",
    "\n",
    "    return clipwise_output, labels\n",
    "\n",
    "\n",
    "def sound_event_detection(args, config):\n",
    "    \"\"\"Inference sound event detection result of an audio clip.\n",
    "    \"\"\"\n",
    "\n",
    "    # Arugments & parameters\n",
    "    sample_rate = args.sample_rate\n",
    "    window_size = args.window_size\n",
    "    hop_size = args.hop_size\n",
    "    mel_bins = args.mel_bins\n",
    "    fmin = args.fmin\n",
    "    fmax = args.fmax\n",
    "    model_type = args.model_type\n",
    "    checkpoint_path = args.checkpoint_path\n",
    "    audio_path = args.audio_path\n",
    "    torch_available = torch.cuda.is_available()\n",
    "    device = torch.device('cuda') if args.cuda and torch_available else torch.device('cpu')\n",
    "    freeze_base = args.freeze_base\n",
    "\n",
    "    classes_num = config.classes_num\n",
    "    labels = config.labels\n",
    "    frames_per_second = sample_rate // hop_size\n",
    "\n",
    "    # Paths\n",
    "    fig_path = os.path.join('results', '{}.png'.format(get_filename(audio_path)))\n",
    "    create_folder(os.path.dirname(fig_path))\n",
    "\n",
    "    # Model\n",
    "    Model = eval(model_type)    \n",
    "    if 'Transfer' in model_type:\n",
    "        model = Model(sample_rate=sample_rate, window_size=window_size, \n",
    "            hop_size=hop_size, mel_bins=mel_bins, fmin=fmin, fmax=fmax, \n",
    "            classes_num=classes_num, freeze_base=freeze_base)\n",
    "    else:\n",
    "        model = Model(sample_rate=sample_rate, window_size=window_size, \n",
    "        hop_size=hop_size, mel_bins=mel_bins, fmin=fmin, fmax=fmax, \n",
    "        classes_num=classes_num)\n",
    "    \n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "    model.load_state_dict(checkpoint['model'])\n",
    "\n",
    "    # Parallel\n",
    "    print('GPU number: {}'.format(torch.cuda.device_count()))\n",
    "    model = torch.nn.DataParallel(model)\n",
    "\n",
    "    if 'cuda' in str(device):\n",
    "        model.to(device)\n",
    "    \n",
    "    # Load audio\n",
    "    (waveform, _) = librosa.core.load(audio_path, sr=sample_rate, mono=True)\n",
    "\n",
    "    waveform = waveform[None, :]    # (1, audio_length)\n",
    "    waveform = move_data_to_device(waveform, device)\n",
    "\n",
    "    # Forward\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        batch_output_dict = model(waveform, None)\n",
    "\n",
    "    framewise_output = batch_output_dict['framewise_output'].data.cpu().numpy()[0]\n",
    "    \"\"\"(time_steps, classes_num)\"\"\"\n",
    "    # activate if using log_softmax output\n",
    "    # sum_values = sum(framewise_output)\n",
    "    # framewise_output = np.array([num /sum_values for num in framewise_output])\n",
    "\n",
    "    print('Sound event detection result (time_steps x classes_num): {}'.format(\n",
    "        framewise_output.shape))\n",
    "\n",
    "    sorted_indexes = np.argsort(np.max(framewise_output, axis=0))[::-1]\n",
    "\n",
    "    top_k = 10  # Show top results\n",
    "    top_result_mat = framewise_output[:, sorted_indexes[0 : top_k]]    \n",
    "    \"\"\"(time_steps, top_k)\"\"\"\n",
    "\n",
    "    # Plot result    \n",
    "    stft = librosa.core.stft(y=waveform[0].data.cpu().numpy(), n_fft=window_size, \n",
    "        hop_length=hop_size, window='hann', center=True)\n",
    "    frames_num = stft.shape[-1]\n",
    "\n",
    "    fig, axs = plt.subplots(2, 1, sharex=True, figsize=(10, 4))\n",
    "    axs[0].matshow(np.log(np.abs(stft)), origin='lower', aspect='auto', cmap='jet')\n",
    "    axs[0].set_ylabel('Frequency bins')\n",
    "    axs[0].set_title('Log spectrogram')\n",
    "    axs[1].matshow(top_result_mat.T, origin='upper', aspect='auto', cmap='jet', vmin=0, vmax=1)\n",
    "    axs[1].xaxis.set_ticks(np.arange(0, frames_num, frames_per_second))\n",
    "    axs[1].xaxis.set_ticklabels(np.arange(0, frames_num / frames_per_second))\n",
    "    axs[1].yaxis.set_ticks(np.arange(0, top_k))\n",
    "    axs[1].yaxis.set_ticklabels(np.array(labels)[sorted_indexes[0 : top_k]])\n",
    "    axs[1].yaxis.grid(color='k', linestyle='solid', linewidth=0.3, alpha=0.3)\n",
    "    axs[1].set_xlabel('Seconds')\n",
    "    axs[1].xaxis.set_ticks_position('bottom')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(fig_path)\n",
    "    print('Save sound event detection visualization to {}'.format(fig_path))\n",
    "\n",
    "    return framewise_output, labels\n",
    "\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "\n",
    "#     parser = argparse.ArgumentParser(description='Example of parser. ')\n",
    "#     subparsers = parser.add_subparsers(dest='mode')\n",
    "\n",
    "#     parser_at = subparsers.add_parser('audio_tagging')\n",
    "#     parser_at.add_argument('--sample_rate', type=int, default=32000)\n",
    "#     parser_at.add_argument('--window_size', type=int, default=1024)\n",
    "#     parser_at.add_argument('--hop_size', type=int, default=320)\n",
    "#     parser_at.add_argument('--mel_bins', type=int, default=64)\n",
    "#     parser_at.add_argument('--fmin', type=int, default=50)\n",
    "#     parser_at.add_argument('--fmax', type=int, default=14000) \n",
    "#     parser_at.add_argument('--model_type', type=str, required=True)\n",
    "#     parser_at.add_argument('--checkpoint_path', type=str, required=True)\n",
    "#     parser_at.add_argument('--audio_path', type=str, required=True)\n",
    "#     parser_at.add_argument('--cuda', action='store_true', default=False)\n",
    "#     parser_at.add_argument('--freeze_base', action='store_true', default=False)\n",
    "\n",
    "#     parser_sed = subparsers.add_parser('sound_event_detection')\n",
    "#     parser_sed.add_argument('--sample_rate', type=int, default=32000)\n",
    "#     parser_sed.add_argument('--window_size', type=int, default=1024)\n",
    "#     parser_sed.add_argument('--hop_size', type=int, default=320)\n",
    "#     parser_sed.add_argument('--mel_bins', type=int, default=64)\n",
    "#     parser_sed.add_argument('--fmin', type=int, default=50)\n",
    "#     parser_sed.add_argument('--fmax', type=int, default=14000) \n",
    "#     parser_sed.add_argument('--model_type', type=str, required=True)\n",
    "#     parser_sed.add_argument('--checkpoint_path', type=str, required=True)\n",
    "#     parser_sed.add_argument('--audio_path', type=str, required=True)\n",
    "#     parser_sed.add_argument('--cuda', action='store_true', default=False)\n",
    "#     parser_sed.add_argument('--freeze_base', action='store_true', default=False)\n",
    "    \n",
    "#     args = parser.parse_args()\n",
    "\n",
    "#     if args.mode == 'audio_tagging':\n",
    "#         audio_tagging(args)\n",
    "\n",
    "#     elif args.mode == 'sound_event_detection':\n",
    "#         sound_event_detection(args)\n",
    "\n",
    "#     else:\n",
    "#         raise Exception('Error argument!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "a6d71f37-d6bc-4125-8bc6-34dce38dfbdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioTagging:\n",
    "    sample_rate = 32000\n",
    "    window_size = 512\n",
    "    hop_size = 160\n",
    "    mel_bins = 64\n",
    "    fmin = 50\n",
    "    fmax = 8000\n",
    "    model_type = \"Cnn14_16k\"\n",
    "    checkpoint_path = \"\"\n",
    "    audio_path = \"\"\n",
    "    cuda = False\n",
    "    freeze_base = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "98fbaff7-d122-4976-adf2-15a619947786",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "AudioTagging.sample_rate = Config.sample_rate\n",
    "AudioTagging.window_size = 1024\n",
    "AudioTagging.hop_size = 320\n",
    "AudioTagging.mel_bins = 64\n",
    "AudioTagging.fmin = 50\n",
    "AudioTagging.fmax = 14000\n",
    "AudioTagging.model_type = \"Transfer_Wavegram_Logmel_Cnn14\"\n",
    "AudioTagging.checkpoint_path = workspace+\"/checkpoints/main/holdout_fold=2/Transfer_Wavegram_Logmel_Cnn14/pretrain=True/loss_type=clip_balanced_bce/augmentation=mixup/batch_size=32/freeze_base=True/2000_iterations.pth\"\n",
    "AudioTagging.audio_path = dataset_dir+'/blues/blues.00000.wav'\n",
    "AudioTagging.cuda = False\n",
    "AudioTagging.freeze_base = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "0f608388-e37f-4706-9ccb-ff7bfa6f0cce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CPU.\n",
      "blues: 0.027\n",
      "country: 0.023\n",
      "reggae: 0.018\n",
      "rock: 0.015\n",
      "classical: 0.014\n",
      "hiphop: 0.012\n",
      "metal: 0.012\n",
      "disco: 0.009\n",
      "jazz: 0.009\n",
      "pop: 0.009\n",
      "embedding: (2048,)\n"
     ]
    }
   ],
   "source": [
    "inference_bgn_time = time.time()\n",
    "audio_tagging(args=AudioTagging, config=Config)\n",
    "inference_end_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "f7a3d534-0257-4ccb-a7af-c93e74dbf312",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Inference time: 2.851 s ---\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "print('--- Inference time: {:.3f} s ---'\\\n",
    "                .format(inference_end_time - inference_bgn_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1697e1fe-f2c4-4bdd-970c-e1c98cc333f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9622f12b-3aee-412e-a02b-73a8f411bccc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
